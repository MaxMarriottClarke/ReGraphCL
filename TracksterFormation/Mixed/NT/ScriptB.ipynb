{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "574cb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0: imports\n",
    "\n",
    "import uproot \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from dataAnalyse import CCV1\n",
    "from torch_geometric.data import DataLoader \n",
    "from model import Net\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from Imports import Aggloremative, calculate_reco_to_sim_score, calculate_sim_to_reco_score, calculate_all_event_scores\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8db626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:36<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 500!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#1: Load Data + Model\n",
    "#1.1: Load Data Through the dataloader - used for predictions\n",
    "testpath = \"/vols/cms/mm1221/Data/100k/5e/test/\"  \n",
    "data_test = CCV1(testpath, max_events=500, inp = 'test')\n",
    "test_loader = DataLoader(data_test, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test[0].assoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "141b363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load explicitely, used for analysis and plots\n",
    "data_path = '/vols/cms/mm1221/Data/100k/5e/test/raw/test.root'\n",
    "data_file = uproot.open(data_path)\n",
    "\n",
    "ass = data_file['associations']['tsCLUE3D_recoToSim_CP'].array()\n",
    "\n",
    "Track_ind = data_file['tracksters;1']['vertices_indexes'].array()\n",
    "GT_ind = data_file['simtrackstersCP;3']['vertices_indexes'].array()\n",
    "GT_mult = data_file['simtrackstersCP;3']['vertices_multiplicity'].array()\n",
    "GT_bc = data_file['simtrackstersCP;3']['barycenter_x'].array()\n",
    "energies = data_file['clusters;3']['energy'].array()\n",
    "LC_x = data_file['clusters;3']['position_x'].array()\n",
    "LC_y = data_file['clusters;3']['position_y'].array()\n",
    "LC_z = data_file['clusters;3']['position_z'].array()\n",
    "LC_eta = data_file['clusters;3']['position_eta'].array()\n",
    "#MT_ind = data_file['trackstersMerged;2']['vertices_indexes'].array()\n",
    "\n",
    "#1.3 Filter so get rid of events with 0 calo particles\n",
    "skim_mask = []\n",
    "for e in GT_bc:\n",
    "    if 1 <= len(e) <= 5:\n",
    "        skim_mask.append(True)\n",
    "    else:\n",
    "        skim_mask.append(False)\n",
    "\n",
    "Track_ind = Track_ind[skim_mask]\n",
    "GT_ind = GT_ind[skim_mask]\n",
    "GT_mult = GT_mult[skim_mask]\n",
    "energies = energies[skim_mask]\n",
    "LC_x = LC_x[skim_mask]\n",
    "LC_y = LC_y[skim_mask]\n",
    "LC_z = LC_z[skim_mask]\n",
    "LC_eta = LC_eta[skim_mask]\n",
    "#MT_ind = MT_ind[skim_mask]\n",
    "ass = ass[skim_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b8fe683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "def filter_repeated_indexes(GT_ind, GT_mult):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "       - GT_ind: an awkward array (or list of lists) of indexes for one event.\n",
    "       - GT_mult: an awkward array (or list of lists) of multiplicity values (same shape as GT_ind).\n",
    "    \n",
    "    For any index that appears in more than one sub-array, keep only the occurrence with the\n",
    "    smallest multiplicity, and set that multiplicity to 1.0. All other occurrences are removed.\n",
    "    \n",
    "    Returns:\n",
    "       new_GT_ind, new_GT_mult  \n",
    "         Both are returned as <class 'awkward.highlevel.Array'>.\n",
    "    \"\"\"\n",
    "    # 1. Record all occurrences of each index.\n",
    "    occurrences = {}\n",
    "    for sub_i, (sub_ind, sub_mult) in enumerate(zip(GT_ind, GT_mult)):\n",
    "        for pos, (val, mult) in enumerate(zip(sub_ind, sub_mult)):\n",
    "            occurrences.setdefault(val, []).append((sub_i, pos, mult))\n",
    "    \n",
    "    # 2. Mark occurrences to remove and those to update.\n",
    "    removals = set()\n",
    "    update_to_one = set()\n",
    "    \n",
    "    for index_val, occ_list in occurrences.items():\n",
    "        if len(occ_list) > 1:\n",
    "            occ_list_sorted = sorted(occ_list, key=lambda x: x[2])  # Sort by multiplicity\n",
    "            kept_occ = occ_list_sorted[0]  # Keep lowest multiplicity\n",
    "            update_to_one.add((kept_occ[0], kept_occ[1]))\n",
    "            for occ in occ_list_sorted[1:]:\n",
    "                removals.add((occ[0], occ[1]))\n",
    "    \n",
    "    # 3. Reconstruct new GT_ind and GT_mult by filtering out the removals.\n",
    "    new_GT_ind = []\n",
    "    new_GT_mult = []\n",
    "    for sub_i, (sub_ind, sub_mult) in enumerate(zip(GT_ind, GT_mult)):\n",
    "        new_sub_ind = []\n",
    "        new_sub_mult = []\n",
    "        for pos, (val, mult) in enumerate(zip(sub_ind, sub_mult)):\n",
    "            if (sub_i, pos) in removals:\n",
    "                continue\n",
    "            new_sub_ind.append(val)\n",
    "            new_sub_mult.append(1.0 if (sub_i, pos) in update_to_one else mult)\n",
    "        new_GT_ind.append(new_sub_ind)\n",
    "        new_GT_mult.append(new_sub_mult)\n",
    "    \n",
    "    # Convert lists to awkward arrays\n",
    "    return ak.Array(new_GT_ind), ak.Array(new_GT_mult)\n",
    "\n",
    "def filter_repeated_indexes_for_events(all_GT_ind, all_GT_mult):\n",
    "    \"\"\"\n",
    "    Given a list of events, each with its GT_ind and GT_mult (lists of sub-arrays),\n",
    "    apply filter_repeated_indexes to each event.\n",
    "    \n",
    "    Args:\n",
    "        all_GT_ind: List of events. Each event is an awkward array (or list of sub-arrays) of indexes.\n",
    "        all_GT_mult: List of events. Each event is an awkward array (or list of sub-arrays) of multiplicity values.\n",
    "    \n",
    "    Returns:\n",
    "        new_all_GT_ind, new_all_GT_mult: Awkward arrays (one per event) of filtered GT_ind and GT_mult.\n",
    "    \"\"\"\n",
    "    new_all_GT_ind = []\n",
    "    new_all_GT_mult = []\n",
    "    \n",
    "    # Loop over each event\n",
    "    for event_ind, event_mult in zip(all_GT_ind, all_GT_mult):\n",
    "        new_event_ind, new_event_mult = filter_repeated_indexes(event_ind, event_mult)\n",
    "        new_all_GT_ind.append(new_event_ind)\n",
    "        new_all_GT_mult.append(new_event_mult)\n",
    "    \n",
    "    # Convert to awkward arrays\n",
    "    return ak.Array(new_all_GT_ind), ak.Array(new_all_GT_mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776b8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_ind, GT_mult = filter_repeated_indexes_for_events(GT_ind, GT_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87c81244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lc_encode): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=12)\n",
       "    (1): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=48)\n",
       "    (2): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=12)\n",
       "    (3): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=48)\n",
       "    (4): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=12)\n",
       "    (5): DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.3, inplace=False)\n",
       "    ), k=48)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model \n",
    "model = Net(\n",
    "    hidden_dim=128, num_layers=6, dropout=0.3, contrastive_dim=128, k=48)\n",
    "checkpoint= torch.load('/vols/cms/mm1221/hgcal/elec5New/LC/NegativeMining/runs/hd128nl6cd128k48_12T0.3/epoch-20.pt',  map_location=torch.device('cpu'))\n",
    "#checkpoint= torch.load('/vols/cms/mm1221/hgcal/elec5New/Track/NegativeMining/resultsSECNeg/best_model.pt',  map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])  \n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884a203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "# Create new lists to store the filtered results\n",
    "# This makes sure GT_ind, MT_ind, Recon_ind have the same indices\n",
    "filtered_GT_ind = []\n",
    "filtered_GT_mult = []\n",
    "#filtered_MT_ind = []\n",
    "\n",
    "\n",
    "for event_idx, track_indices in enumerate(Track_ind):\n",
    "    # Flatten the current event's track indices and convert to a set\n",
    "    track_flat = set(ak.flatten(track_indices).tolist())  # Ensure it contains only integers\n",
    "    \n",
    "    # Filter GT_ind and GT_mult for the current event, preserving structure\n",
    "    event_GT_ind = GT_ind[event_idx]\n",
    "    event_GT_mult = GT_mult[event_idx]\n",
    "    filtered_event_GT_ind = []\n",
    "    filtered_event_GT_mult = []\n",
    "    for sublist_ind, sublist_mult in zip(event_GT_ind, event_GT_mult):\n",
    "        filtered_sublist_ind = [idx for idx in sublist_ind if idx in track_flat]\n",
    "        filtered_sublist_mult = [mult for idx, mult in zip(sublist_ind, sublist_mult) if idx in track_flat]\n",
    "        filtered_event_GT_ind.append(filtered_sublist_ind)\n",
    "        filtered_event_GT_mult.append(filtered_sublist_mult)\n",
    "\n",
    "\n",
    "    # Append filtered results\n",
    "    filtered_GT_ind.append(filtered_event_GT_ind)\n",
    "    filtered_GT_mult.append(filtered_event_GT_mult)\n",
    "    #filtered_MT_ind.append(filtered_event_MT_ind)\n",
    "\n",
    "# Convert the filtered results back to awkward arrays\n",
    "GT_ind_filt = ak.Array(filtered_GT_ind)\n",
    "GT_mult_filt = ak.Array(filtered_GT_mult)\n",
    "#MT_ind_filt = ak.Array(filtered_MT_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75946030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.2922e-01, -2.5636e+00, -2.8389e+00, -1.8112e+00,  2.2780e+00,\n",
      "         2.7643e+00,  1.1790e+00, -3.0299e+00,  1.8241e+00,  2.5544e+00,\n",
      "         3.5008e-01, -8.6304e-01,  5.0269e-01, -4.8065e-01, -9.3046e-01,\n",
      "         7.6438e-01,  3.1475e-02, -1.2814e+00,  9.9809e-01, -1.5561e+00,\n",
      "        -9.0422e-02, -1.9421e+00, -1.1850e+00, -1.9169e+00, -7.2745e-01,\n",
      "         2.8215e+00, -8.9555e-01,  2.7550e-01,  5.0290e-02, -2.6579e+00,\n",
      "        -1.4362e-02,  1.6636e+00, -1.1132e-01, -3.2221e-01,  7.6270e-01,\n",
      "         1.3786e+00,  1.4202e+00,  1.6339e-01,  8.8372e-01, -1.2332e+00,\n",
      "        -2.5314e+00,  1.6493e+00,  6.4323e-03,  4.6291e-01,  1.6455e+00,\n",
      "         1.8882e+00,  5.7491e-01,  3.7835e-01, -5.7195e-01,  2.8347e-01,\n",
      "         9.4825e-01,  1.9054e-01, -1.9693e-01,  2.0538e+00, -9.5456e-01,\n",
      "        -9.8681e-01, -1.0563e+00,  6.3979e-01, -1.4328e+00, -9.4713e-01,\n",
      "         7.9943e-02,  1.1674e-02,  2.8796e+00,  2.9591e-02,  1.6330e+00,\n",
      "         2.4767e-02, -5.7850e-01,  1.0237e+00,  1.8104e+00,  1.2104e+00,\n",
      "        -1.7462e+00, -1.1107e+00, -3.3872e+00, -1.2627e-01,  1.9626e+00,\n",
      "         7.0401e-01,  9.9264e-01, -1.6613e+00, -3.8595e-01,  2.0857e+00,\n",
      "         1.3316e+00, -1.7148e+00, -1.7529e+00,  2.8742e+00, -9.4424e-01,\n",
      "        -9.5928e-01, -2.0811e+00, -9.4657e-01,  9.5057e-01,  3.6705e+00,\n",
      "        -1.2406e+00,  9.5505e-01, -9.6515e-01,  1.1804e+00,  1.4776e+00,\n",
      "        -8.8116e-02,  6.0282e-01,  3.0588e+00,  3.6183e-01, -4.5549e-01,\n",
      "        -3.7297e+00,  2.1913e+00, -1.1001e+00,  1.1764e+00, -1.3588e+00,\n",
      "         3.0201e-01,  4.1372e-01,  7.5835e-01, -9.8131e-01,  2.7080e-03,\n",
      "        -7.2841e-01, -6.6849e-01, -4.2514e-02,  2.7178e+00,  6.1952e-01,\n",
      "        -1.9960e-01,  1.3250e+00, -2.4079e+00,  2.9797e-01, -3.4227e+00,\n",
      "         9.9262e-01, -1.5284e+00,  2.2929e-01, -9.7156e-01, -2.0978e+00,\n",
      "         1.1168e+00,  1.0432e+00, -6.1223e-01], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d8b3e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import hdbscan  # Make sure you have hdbscan installed: pip install hdbscan\n",
    "\n",
    "#3: Make Predictions + Cluster -> Calculate the inference time\n",
    "#3.1: Make Predictions\n",
    "\n",
    "all_predictions = []  \n",
    "start_time = time.time()\n",
    "\n",
    "for i, data in enumerate(data_test):\n",
    "    #edge_index = knn_graph(data.x, k=16)  \n",
    "    if i > 100:\n",
    "        break\n",
    "    print(i)\n",
    "    predictions = model(data.x, data.batch)\n",
    "    all_predictions.append(predictions[0].detach().cpu().numpy())  \n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444145ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 1 3 1 0 0 0 1 1 0 0\n",
      " 0 0 0 1 1 1 0 1 0 0 0 2 0 1 1 0 3 0 1 0 2 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0\n",
      " 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 2 2 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1\n",
      " 1 1 1 1 0 0 2 0 0 0 0 1 1 0 1 1 0 1 1 1 1 0 2 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
      " 3 0 0 1 1 1 1 1 1 1 0 0 0 1 1 2 2 0 0 1 2 1 0 0 0 0 1 1 1 1 1 1 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 0 3 0 3 0 0 3 3 3 3 0 0 0 0 3 2 3 3 2 3\n",
      " 3 3 1 3 3 3 3 3 3 3 3 0 0 3 0 3 0 3 3 3 0 3 3 1 3 2 3 3 3 0 0 3 0 3 0 2 1\n",
      " 3 0 3 0 0 3 2 0 0 0 0 0 0 0 3 3 3 3 2 3 3 0 2 3 0 3 0 0 2 3 0 3 3 0 0 0 3\n",
      " 3 0 0 3 3 3 3 2 3 0 3 3 3 2 3 0 3 3 3 0 3 3 1 0 0 3 2 0 0 0 3 0 3 0 0 3 2\n",
      " 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 1 0 3 1 3 3 3 2 3 3 3 3 0 3 3 3 3 3 3 3 0\n",
      " 3 3 3 3 3 0 0 3 0 3 3 3 0 0 0 3 0 0 3 2 3 3 3 0 3 3 0 3 0 3 3 3 3 0 3 3 3\n",
      " 0 2 3 1 0 3 0 3 1 0 0 3 2 0 1 3 3 2 3 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 3 0 0\n",
      " 0 3 0 0 0 1 0 1 0 0 0 1 0 3 0 1 0 1 1 0 1 0 1 1 1 1 0 3 1 1 1 1 1 1 1 0 1\n",
      " 2 0 3 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 2 1 1 0 1 0 0 1 1 2 0 1 0 3 1 1 1 1 2\n",
      " 1 3 0 1 0 1 2 1 1 1 0 1 2 1 1 1 0 1 1 2 3 1 0 1 1 1 1 1 1 1 1 1 3 3 3 3 3\n",
      " 3 3 3 0 0 2 0 0 3 0 0 0 3 0 2 2 0 3 0 0 0 0 3 3 3 3 2 0 0 0 0 3 3 3 3 2 0\n",
      " 2 0 0 3 3 0 0 3 3 1 0 3 0 0 3 2 3 3 2 3 2 0 3 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(all_cluster_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a4a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(data_test[3].assoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "003b5aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|████████████████████████| 10/10 [00:33<00:00,  3.30s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to compute cosine similarities from a set of edge indices.\n",
    "def compute_cosine_similarities(embeddings, edge_indices, skip_self=True):\n",
    "    \"\"\"\n",
    "    Given embeddings (tensor of shape [N, D]) and edge_indices (tensor of shape [num_edges, 2]),\n",
    "    compute the cosine similarity for each edge.\n",
    "    If skip_self is True, edges where source==target are skipped.\n",
    "    Returns a NumPy array of similarities.\n",
    "    \"\"\"\n",
    "    sims = []\n",
    "    for edge in edge_indices:\n",
    "        src, tgt = edge.tolist()\n",
    "        if skip_self and src == tgt:\n",
    "            continue\n",
    "        # Cosine similarity for unit-norm embeddings is just their dot product.\n",
    "        sim = F.cosine_similarity(embeddings[src].unsqueeze(0), embeddings[tgt].unsqueeze(0))\n",
    "        sims.append(sim.item())\n",
    "    return np.array(sims)\n",
    "\n",
    "# Assuming:\n",
    "# - all_predictions is a list (length >= num_events) of NumPy arrays,\n",
    "#   where each element is an array of embeddings for one event.\n",
    "# - data_test is a list (length >= num_events) of data objects with attribute 'assoc'\n",
    "#   which is an array of group labels (e.g., [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2]).\n",
    "# For each event, positive pairs are those nodes with the same label,\n",
    "# and negative pairs are those nodes with different labels.\n",
    "\n",
    "num_events = 10\n",
    "all_pos_sims = []\n",
    "all_neg_sims = []\n",
    "\n",
    "# Loop over events.\n",
    "for i in tqdm(range(num_events), desc=\"Processing events\"):\n",
    "    # Convert predictions for event i to a tensor and normalize them.\n",
    "    pred_tensor = torch.tensor(all_predictions[i], dtype=torch.float32)\n",
    "    pred_norm = F.normalize(pred_tensor, p=2, dim=1)\n",
    "    \n",
    "    # Get the association labels for each node.\n",
    "    # Here, assoc should be a list or array of integers.\n",
    "    assoc = data_test[i].assoc  \n",
    "    num_nodes = len(assoc)\n",
    "\n",
    "    \n",
    "    # Build edge lists based on the association groups.\n",
    "    pos_edges = []\n",
    "    neg_edges = []\n",
    "    for src in range(num_nodes):\n",
    "        for tgt in range(src + 1, num_nodes):  # only consider each pair once\n",
    "            if assoc[src] == assoc[tgt]:\n",
    "                pos_edges.append([src, tgt])\n",
    "            else:\n",
    "                neg_edges.append([src, tgt])\n",
    "\n",
    "    # Convert edge lists to tensors.\n",
    "    pos_edges_tensor = torch.tensor(pos_edges, dtype=torch.long)\n",
    "    neg_edges_tensor = torch.tensor(neg_edges, dtype=torch.long)\n",
    "    \n",
    "    # Compute cosine similarities for positive and negative edges.\n",
    "    pos_sims = compute_cosine_similarities(pred_norm, pos_edges_tensor, skip_self=True)\n",
    "    neg_sims = compute_cosine_similarities(pred_norm, neg_edges_tensor, skip_self=True)\n",
    "    all_pos_sims.extend(pos_sims.tolist())\n",
    "    all_neg_sims.extend(neg_sims.tolist())\n",
    "\n",
    "all_pos_sims = np.array(all_pos_sims)\n",
    "all_neg_sims = np.array(all_neg_sims)\n",
    "\n",
    "# Plot histograms for positive and negative cosine similarities.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_pos_sims, bins=50, alpha=0.7, label=\"Positive Pairs\")\n",
    "plt.hist(all_neg_sims, bins=50, alpha=0.7, label=\"Negative Pairs\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Cosine Similarity Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d0629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86239a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(GT_ind[0][0]))\n",
    "print(type(GT_ind2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f8fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def calculate_sim_to_reco_score(CaloParticle, energies_indices, ReconstructedTrackster, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the sim-to-reco score for a given CaloParticle and ReconstructedTrackster.\n",
    "    \n",
    "    Parameters:\n",
    "    - CaloParticle: array of Layer Clusters in the CaloParticle.\n",
    "    - Multiplicity: array of Multiplicity for layer clusters in CP\n",
    "    - energies_indices: array of energies associated with all LC (indexed by LC).\n",
    "    - ReconstructedTrackster: array of LC in the reconstructed Trackster.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_to_reco_score: the calculated sim-to-reco score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    if total_energy_caloparticle == 0:\n",
    "        return 1.0  # No energy in the CaloParticle implies perfect mismatch\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    i = 0\n",
    "    # Iterate over all DetIds in the CaloParticle\n",
    "    for det_id in CaloParticle:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in CaloParticle\n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1 if det_id in ReconstructedTrackster else 0.0\n",
    "        # Fraction of energy in the CaloParticle (fr_k^SC)\n",
    "        fr_sc_k = 1 / Multi[i]\n",
    "\n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_sc_k ** 2                # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_sc_k ** 2) * (energy_k ** 2)\n",
    "        i+=1\n",
    "\n",
    "    # Calculate score\n",
    "    sim_to_reco_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return sim_to_reco_score\n",
    "\n",
    "def calculate_reco_to_sim_score(ReconstructedTrackster, energies_indices, CaloParticle, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the reco-to-sim score for a given ReconstructedTrackster and CaloParticle.\n",
    "\n",
    "    Parameters:\n",
    "    - ReconstructedTrackster: array of DetIds in the ReconstructedTrackster.\n",
    "    - energies_indices: array of energies associated with all DetIds (indexed by DetId).\n",
    "    - CaloParticle: array of DetIds in the CaloParticle.\n",
    "\n",
    "    Returns:\n",
    "    - reco_to_sim_score: the calculated reco-to-sim score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    if total_energy_trackster == 0:\n",
    "        return 1.0  # No energy in the Trackster implies perfect mismatch\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    # Iterate over all DetIds in the ReconstructedTrackster\n",
    "    for det_id in ReconstructedTrackster:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in the Trackster\n",
    "        \n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1\n",
    "\n",
    "        #fr_sc_k = 1 if det_id in CaloParticle else 0.0\n",
    "        if det_id in CaloParticle:\n",
    "            index = np.where(CaloParticle == det_id)[0][0]  # Find the index\n",
    "            Multiplicity = Multi[index]\n",
    "            fr_sc_k = 1\n",
    "        else:\n",
    "            fr_sc_k = 0\n",
    "            \n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_tst_k ** 2               # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_tst_k ** 2) * (energy_k ** 2)\n",
    "\n",
    "    # Calculate score\n",
    "    reco_to_sim_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return reco_to_sim_score\n",
    "def calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, multi, num_events = 100):\n",
    "    \"\"\"\n",
    "    Calculate sim-to-reco and reco-to-sim scores for all CaloParticle and ReconstructedTrackster combinations across all events.\n",
    "\n",
    "    Parameters:\n",
    "    - GT_ind: List of CaloParticle indices for all events.\n",
    "    - energies: List of energy arrays for all events.\n",
    "    - recon_ind: List of ReconstructedTrackster indices for all events.\n",
    "    - LC_x, LC_y, LC_z, LC_eta: Lists of x, y, z positions and eta values for all DetIds across events.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing scores and additional features for each CaloParticle-Trackster combination across all events.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Loop over all events with a progress bar\n",
    "    for event_index in tqdm(range(num_events)):\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_multi = multi[event_index]\n",
    "\n",
    "        # Extract layer cluster positions and eta for this event\n",
    "        event_x = np.array(LC_x[event_index])\n",
    "        event_y = np.array(LC_y[event_index])\n",
    "        event_z = np.array(LC_z[event_index])\n",
    "        event_eta = np.array(LC_eta[event_index])\n",
    "\n",
    "        # Compute barycenter for each CaloParticle\n",
    "        cp_barycenters = []\n",
    "        cp_avg_etas = []\n",
    "        for caloparticle in caloparticles:\n",
    "            # Compute barycenter (x, y, z)\n",
    "            \n",
    "            barycenter_x = np.mean([event_x[det_id] for det_id in caloparticle])\n",
    "            barycenter_y = np.mean([event_y[det_id] for det_id in caloparticle])\n",
    "            barycenter_z = np.mean([event_z[det_id] for det_id in caloparticle])\n",
    "            cp_barycenters.append(np.array([barycenter_x, barycenter_y, barycenter_z]))\n",
    "            \n",
    "            # Compute average eta\n",
    "            avg_eta = np.mean([event_eta[det_id] for det_id in caloparticle])\n",
    "            cp_avg_etas.append(avg_eta)\n",
    "\n",
    "        # Compute separation between two CaloParticles if at least two exist\n",
    "        if len(cp_barycenters) >= 2:\n",
    "            cp_separation = np.linalg.norm(cp_barycenters[0] - cp_barycenters[1])\n",
    "        else:\n",
    "            cp_separation = 0.0\n",
    "            \n",
    "        trackster_det_id_sets = [set(trackster) for trackster in tracksters]\n",
    "\n",
    "        # Loop over all CaloParticles\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            Calo_multi = event_multi[calo_idx]\n",
    "            calo_det_ids = set(calo_id for calo_id in caloparticle)\n",
    "            # Loop over all Tracksters\n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                # Calculate sim-to-reco score\n",
    "                trackster_det_ids = trackster_det_id_sets[trackster_idx]\n",
    "                shared_det_ids = calo_det_ids.intersection(trackster_det_ids)\n",
    "                \n",
    "                # Calculate shared_energy by summing energies of shared det_ids\n",
    "                shared_energy = np.sum(event_energies[list(shared_det_ids)]) if shared_det_ids else 0.0\n",
    "                \n",
    "                \n",
    "                sim_to_reco_score = calculate_sim_to_reco_score(caloparticle, event_energies, trackster, Calo_multi)\n",
    "                # Calculate reco-to-sim score\n",
    "                reco_to_sim_score = calculate_reco_to_sim_score(trackster, event_energies, caloparticle, Calo_multi)\n",
    "\n",
    "                # Calculate total energy for CaloParticle and Trackster\n",
    "                cp_energy_lc2 = event_energies[caloparticle] / Calo_multi\n",
    "                cp_energy = np.sum(cp_energy_lc2)\n",
    "                \n",
    "                trackster_energy = np.sum([event_energies[det_id] for det_id in trackster])\n",
    "\n",
    "                # Calculate energy difference ratio\n",
    "                energy_diff_ratio = (trackster_energy / cp_energy if cp_energy != 0 else None)\n",
    "\n",
    "                # Append results\n",
    "                all_results.append({\n",
    "                    \"event_index\": event_index,\n",
    "                    \"cp_id\": calo_idx,\n",
    "                    \"trackster_id\": trackster_idx,\n",
    "                    \"sim_to_reco_score\": sim_to_reco_score,\n",
    "                    \"reco_to_sim_score\": reco_to_sim_score,\n",
    "                    \"cp_energy\": cp_energy,\n",
    "                    \"trackster_energy\": trackster_energy,\n",
    "                    \"cp_avg_eta\": cp_avg_etas[calo_idx],\n",
    "                    \"cp_separation\": cp_separation,\n",
    "                    \"energy_ratio\": energy_diff_ratio,\n",
    "                    \"shared_energy\": shared_energy  # New column\n",
    "                })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71d8695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "print(len(all_cluster_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "003f1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[75, 115, 92, 50, 149, 128, 35, 160, 178, ... 258, 169, 21, 254, 286, 45, 170, 211]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd10f50b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in ListOffsetArray64 attempting to get 4, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34288/3854272439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcluster_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevent_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mevent_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mevent_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrack_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mrecon_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jaxtracers_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in ListOffsetArray64 attempting to get 4, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)"
     ]
    }
   ],
   "source": [
    "#4: Calculate Scores and create DF for our model and TICL\n",
    "\n",
    "#4.1: Turn the cluster labels into our reconstructed tracksters\n",
    "\n",
    "recon_ind = []\n",
    "\n",
    "for event_idx, labels in enumerate(all_cluster_labels):\n",
    "\n",
    "    event_clusters = {} \n",
    "    \n",
    "    for cluster_idx, cluster_label in enumerate(labels):\n",
    "        print()\n",
    "        if cluster_label not in event_clusters:\n",
    "            event_clusters[cluster_label] = []\n",
    "        event_clusters[cluster_label].extend(Track_ind[event_idx][cluster_idx])\n",
    "    \n",
    "    recon_ind.append([event_clusters[label] for label in sorted(event_clusters.keys())])\n",
    "\n",
    "#4.2 Make DF from our model and CERN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b778727",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_CL = calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 1000)\n",
    "df_TICL = calculate_all_event_scores(GT_ind, energies, MT_ind_filt, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e490e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL2 = pd.read_csv('df_scores_noRepeats.csv')\n",
    "df_TICL2 = pd.read_csv('df_CERN_noRepeats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac63997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Print metrics\n",
    "\n",
    "def calculate_metrics(df, model_name):\n",
    "    # ----- Efficiency Calculation -----\n",
    "    # Step 1: Filter out rows where 'cp_id' is NaN\n",
    "    cp_valid = df.dropna(subset=['cp_id']).copy()\n",
    "\n",
    "    # Step 2: Group by 'event_index' and 'cp_id' to proess each CaloParticle individually\n",
    "    cp_grouped = cp_valid.groupby(['event_index', 'cp_id'])\n",
    "\n",
    "    # Step 3: For each CaloParticle, check if any 'shared_energy' >= 50% of 'cp_energy'\n",
    "    def is_cp_associated(group):\n",
    "        cp_energy = group['cp_energy'].iloc[0]  # Assuming 'cp_energy' is consistent within the group\n",
    "        threshold = 0.5 * cp_energy\n",
    "        return (group['shared_energy'] >= threshold).any()\n",
    "\n",
    "    # Apply the association function to each group\n",
    "    cp_associated = cp_grouped.apply(is_cp_associated)\n",
    "\n",
    "    # Step 4: Calculate the number of associated CaloParticles and total CaloParticles\n",
    "    num_associated_cp = cp_associated.sum()\n",
    "    total_cp = cp_associated.count()\n",
    "    efficiency = num_associated_cp / total_cp if total_cp > 0 else 0\n",
    "\n",
    "    # ----- Purity Calculation -----\n",
    "    tst_valid = df.dropna(subset=['trackster_id']).copy()\n",
    "    tst_grouped = tst_valid.groupby(['event_index', 'trackster_id'])\n",
    "    tst_associated = tst_grouped['reco_to_sim_score'].min() < 0.2\n",
    "    num_associated_tst = tst_associated.sum()\n",
    "    total_tst = tst_associated.count()\n",
    "    purity = num_associated_tst / total_tst if total_tst > 0 else 0\n",
    "    \n",
    "        # ----- Containment Calculation -----\n",
    "    cp_valid_cont = df.dropna(subset=['cp_id']).copy()\n",
    "    cp_grouped_cont = cp_valid_cont.groupby(['event_index', 'cp_id'])\n",
    "    cp_associated_cont = cp_grouped_cont['sim_to_reco_score'].min() < 0.2\n",
    "    num_associated_cp_cont = cp_associated_cont.sum()\n",
    "    total_cp_cont = cp_associated_cont.count()\n",
    "    containment = num_associated_cp_cont / total_cp_cont if total_cp_cont > 0 else 0\n",
    "\n",
    "\n",
    "    # Print results for the model\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Efficiency: {efficiency:.4f} ({num_associated_cp} associated CPs out of {total_cp} total CPs)\")\n",
    "    print(f\"Containment: {containment:.4f} ({num_associated_cp_cont} associated CPs out of {total_cp_cont} total CPs)\")\n",
    "    print(f\"Purity: {purity:.4f} ({num_associated_tst} associated Tracksters out of {total_tst} total Tracksters)\")\n",
    "    print(f\"Num tracksters ratio: {total_tst / total_cp if total_cp > 0 else 0:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'efficiency': efficiency,\n",
    "        'purity': purity\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "our_model_metrics = calculate_metrics(df_CL, \"Our Model\")\n",
    "cern_model_metrics = calculate_metrics(df_CL2, \"CERN Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6efcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_CL and df_TICL are your two DataFrames\n",
    "# Replace the following with your actual data loading mechanism\n",
    "# df_CL = pd.read_csv('df_CL.csv')\n",
    "# df_TICL = pd.read_csv('df_TICL.csv')\n",
    "\n",
    "# ----- Preprocessing -----\n",
    "# Convert relevant columns to numeric.\n",
    "for df in [df_CL, df_TICL]:\n",
    "    df['cp_id'] = pd.to_numeric(df['cp_id'], errors='coerce')\n",
    "    df['shared_energy'] = pd.to_numeric(df['shared_energy'], errors='coerce')  # New Column\n",
    "    df['cp_energy'] = pd.to_numeric(df['cp_energy'], errors='coerce')\n",
    "\n",
    "def prepare_cp_data(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by ['event_index', 'cp_id'] so that each caloparticle is counted once.\n",
    "    For each group:\n",
    "      - Take the first cp_energy (they are identical).\n",
    "      - Take the maximum shared_energy to check if any shared_energy >= 50% of cp_energy.\n",
    "      - Mark the caloparticle as 'reconstructed' if any shared_energy >= 50% of cp_energy.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(['event_index', 'cp_id']).agg({\n",
    "        'cp_energy': 'first',          \n",
    "        'shared_energy': 'max'        # Use max to determine if any shared_energy meets the threshold\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Mark as reconstructed if any shared_energy >= 50% of cp_energy.\n",
    "    grouped['reco'] = (grouped['shared_energy'] >= 0.5 * grouped['cp_energy']).astype(int)\n",
    "    return grouped\n",
    "\n",
    "# Prepare the caloparticle data for both DataFrames.\n",
    "df_CL_cp = prepare_cp_data(df_CL)\n",
    "df_TICL_cp = prepare_cp_data(df_TICL)\n",
    "\n",
    "# ----- Bin Caloparticles by Energy -----\n",
    "# Define energy bins based on the range of cp_energy from df_CL.\n",
    "min_energy = df_CL_cp['cp_energy'].min()\n",
    "max_energy = df_CL_cp['cp_energy'].max()\n",
    "n_bins = 10\n",
    "energy_bins = np.linspace(min_energy, max_energy, n_bins + 1)\n",
    "\n",
    "# Assign each caloparticle to an energy bin.\n",
    "df_CL_cp['energy_bin'] = pd.cut(df_CL_cp['cp_energy'], bins=energy_bins, labels=False, include_lowest=True)\n",
    "df_TICL_cp['energy_bin'] = pd.cut(df_TICL_cp['cp_energy'], bins=energy_bins, labels=False, include_lowest=True)\n",
    "\n",
    "# ----- Calculate Efficiency per Energy Bin -----\n",
    "def aggregate_efficiency(df):\n",
    "    \"\"\"\n",
    "    For each energy bin, calculate:\n",
    "      - The total number of caloparticles in the bin.\n",
    "      - The number of reconstructed caloparticles.\n",
    "      - Efficiency = (number of reconstructed) / (total number).\n",
    "    \"\"\"\n",
    "    agg = df.groupby('energy_bin').agg(\n",
    "        total_cp=('cp_energy', 'count'),\n",
    "        reco_cp=('reco', 'sum')\n",
    "    ).reset_index()\n",
    "    agg['efficiency'] = agg['reco_cp'] / agg['total_cp']\n",
    "    \n",
    "    agg['eff_error'] = np.sqrt(agg['efficiency'] * (1 - agg['efficiency']) / agg['total_cp'])\n",
    "    return agg\n",
    "\n",
    "agg_CL = aggregate_efficiency(df_CL_cp)\n",
    "agg_TICL = aggregate_efficiency(df_TICL_cp)\n",
    "\n",
    "# ----- Plot Efficiency vs Energy with Histogram Overlay -----\n",
    "# Compute bin centers for plotting: average of adjacent bin edges.\n",
    "bin_centers = (energy_bins[:-1] + energy_bins[1:]) / 2\n",
    "bar_width = energy_bins[1] - energy_bins[0]\n",
    "\n",
    "eff_ratio = agg_CL['efficiency'] / agg_TICL['efficiency']\n",
    "eff_ratio_error = eff_ratio * np.sqrt(\n",
    "    (agg_CL['eff_error'] / agg_CL['efficiency'])**2 + \n",
    "    (agg_TICL['eff_error'] / agg_TICL['efficiency'])**2\n",
    ")\n",
    "\n",
    "\n",
    "# Create a figure with two y-axes.\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
    "\n",
    "\n",
    "# Plot Efficiency\n",
    "ax1.errorbar(bin_centers, agg_CL['efficiency'], yerr=agg_CL['eff_error'], marker='o',linestyle ='--',  color='blue', label='Our Model')\n",
    "ax1.errorbar(bin_centers, agg_TICL['efficiency'], yerr=agg_TICL['eff_error'],   marker='x', linestyle ='--', color='green', label='TICL')\n",
    "ax1.set_ylabel('Efficiency', fontsize=12)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# ----- Histogram for Number of CaloParticles -----\n",
    "ax1_hist = ax1.twinx()  # Create secondary y-axis for histogram\n",
    "ax1_hist.bar(bin_centers, agg_CL['total_cp'], width=bar_width, color='lightblue', alpha=0.4)\n",
    "ax1_hist.set_ylabel('Number of CaloParticles', fontsize=12)\n",
    "ax1_hist.set_ylim(0, agg_CL['total_cp'].max() * 1.2)\n",
    "\n",
    "# ----- Ratio Plot -----\n",
    "ax2.errorbar(bin_centers, eff_ratio, yerr=eff_ratio_error, fmt='x', color='blue', markersize=10)\n",
    "ax2.axhline(1.0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('CaloParticle Energy [GeV]', fontsize=12)\n",
    "ax2.set_ylabel('Ratio', fontsize=12)\n",
    "ax2.set_ylim(0.95, 1.05)\n",
    "\n",
    "# Update Title to Reflect New Efficiency Definition\n",
    "plt.title(r'Efficiency vs CaloParticle Energy: 5 Electron', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/efficiency.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae469ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert relevant columns to numeric.\n",
    "for df in [df_CL, df_TICL]:\n",
    "    df['cp_id'] = pd.to_numeric(df['cp_id'], errors='coerce')\n",
    "    df['sim_to_reco_score'] = pd.to_numeric(df['sim_to_reco_score'], errors='coerce')\n",
    "    df['cp_energy'] = pd.to_numeric(df['cp_energy'], errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare Trackster-Level Data for containment\n",
    "# -----------------------------\n",
    "def prepare_cont_data(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by ['event_index', 'trackster_id'] so that each trackster is counted once.\n",
    "    For each group:\n",
    "      - Take the first cp_energy,\n",
    "      - Take the minimum reco_to_sim_score,\n",
    "      - Mark the trackster as 'associated' if the minimum reco_to_sim_score is < 0.2.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(['event_index', 'cp_id']).agg({\n",
    "        'cp_energy': 'first',       # Use the first cp_energy value.\n",
    "        'sim_to_reco_score': 'min'           # Minimum score among the rows for that trackster.\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Mark as associated if reco_to_sim_score is < 0.2.\n",
    "    grouped['assoc'] = (grouped['sim_to_reco_score'] < 0.2).astype(int)\n",
    "    return grouped\n",
    "\n",
    "# Prepare the trackster-level data for both DataFrames.\n",
    "df_CL_cont = prepare_cont_data(df_CL)\n",
    "df_TICL_cont   = prepare_cont_data(df_TICL)\n",
    "\n",
    "# -----------------------------\n",
    "# Bin Tracksters by Energy\n",
    "# -----------------------------\n",
    "# Define energy bins based on the range of cp_energy from df_CL.\n",
    "min_energy_cont = df_CL_cont['cp_energy'].min()\n",
    "max_energy_cont = df_CL_cont['cp_energy'].max()\n",
    "n_bins_cont = 10  # Adjust the number of bins if desired.\n",
    "energy_bins_cont = np.linspace(min_energy_cont, max_energy_cont, n_bins_cont + 1)\n",
    "\n",
    "# Assign each trackster to an energy bin.\n",
    "df_CL_cont['energy_bin'] = pd.cut(df_CL_cont['cp_energy'],\n",
    "                                    bins=energy_bins_cont, labels=False, include_lowest=True)\n",
    "df_TICL_cont['energy_bin']   = pd.cut(df_TICL_cont['cp_energy'],\n",
    "                                    bins=energy_bins_cont, labels=False, include_lowest=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Calculate containment per Energy Bin\n",
    "# -----------------------------\n",
    "def aggregate_containment(df):\n",
    "    \"\"\"\n",
    "    For each energy bin, calculate:\n",
    "      - Total number of tracksters,\n",
    "      - Number of associated tracksters (with reco_to_sim_score < 0.2),\n",
    "      - containment = (number of associated tracksters) / (total number).\n",
    "    \"\"\"\n",
    "    agg = df.groupby('energy_bin').agg(\n",
    "        total_cont = ('cp_energy', 'count'),\n",
    "        assoc_cont = ('assoc', 'sum')\n",
    "    ).reset_index()\n",
    "    agg['containment'] = agg['assoc_cont'] / agg['total_cont']\n",
    "    \n",
    "    agg['containment_error'] = np.sqrt(agg['containment'] * (1 - agg['containment']) / agg['total_cont'])\n",
    "    return agg\n",
    "\n",
    "agg_CL_cont = aggregate_containment(df_CL_cont)\n",
    "agg_TICL_cont   = aggregate_containment(df_TICL_cont)\n",
    "\n",
    "# Reindex both aggregated DataFrames so that they have one row per energy bin (0 to n_bins_cont-1)\n",
    "agg_CL_cont = agg_CL_cont.set_index('energy_bin').reindex(range(n_bins_cont), fill_value=np.nan).reset_index()\n",
    "agg_TICL_cont   = agg_TICL_cont.set_index('energy_bin').reindex(range(n_bins_cont), fill_value=np.nan).reset_index()\n",
    "\n",
    "# -----------------------------\n",
    "# Plot containment vs Trackster Energy with Histogram Overlay\n",
    "# -----------------------------\n",
    "# Compute bin centers for plotting: average of adjacent bin edges.\n",
    "bin_centers_cont = (energy_bins_cont[:-1] + energy_bins_cont[1:]) / 2\n",
    "bar_width_cont = energy_bins_cont[1] - energy_bins_cont[0]\n",
    "\n",
    "containment_ratio = agg_CL_cont['containment'] / agg_TICL_cont['containment']\n",
    "containment_ratio_error = containment_ratio * np.sqrt(\n",
    "    (agg_CL_cont['containment_error'] / agg_CL_cont['containment'])**2 +\n",
    "    (agg_TICL_cont['containment_error'] / agg_TICL_cont['containment'])**2\n",
    ")\n",
    "\n",
    "# Create a figure with two y-axes.\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.1)  # Set spacing to zero\n",
    "\n",
    "# Plot containment curves on the primary y-axis.\n",
    "ax1.errorbar(bin_centers_cont, agg_CL_cont['containment'], yerr=agg_CL_cont['containment_error'], marker='o', linestyle='--', color = 'blue', label='Our Model')\n",
    "ax1.errorbar(bin_centers_cont, agg_TICL_cont['containment'], yerr = agg_TICL_cont['containment_error'], marker='x', linestyle='--', color = 'green', label='TICL')\n",
    "ax1.set_ylabel('containment')\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot histogram (trackster count per energy bin) on the secondary y-axis.\n",
    "ax1_hist = ax1.twinx()  # Create secondary y-axis for histogram\n",
    "ax1_hist.bar(bin_centers_cont, agg_CL_cont['total_cont'], width=bar_width_cont, color='lightblue', alpha=0.4)\n",
    "ax1_hist.set_ylabel('Number of Merged Tracksters', fontsize=12)\n",
    "ax1_hist.set_ylim(0, agg_CL_cont['total_cont'].max() * 1.2)\n",
    "\n",
    "ax2.errorbar(bin_centers_cont, containment_ratio, yerr=containment_ratio_error, fmt='x', color='blue', markersize=7)\n",
    "ax2.axhline(1.0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('CaloParticle [GeV]', fontsize=12)\n",
    "ax2.set_ylabel('Ratio', fontsize=12)\n",
    "ax2.set_ylim(0.6, 1.05)\n",
    "\n",
    "plt.title(r'containment vs Merged Trackster Energy: 5 Pion', fontsize=14)\n",
    "plt.savefig(\"plots/containment.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cff41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert relevant columns to numeric.\n",
    "for df in [df_CL, df_TICL]:\n",
    "    df['trackster_id'] = pd.to_numeric(df['trackster_id'], errors='coerce')\n",
    "    df['reco_to_sim_score'] = pd.to_numeric(df['reco_to_sim_score'], errors='coerce')\n",
    "    df['trackster_energy'] = pd.to_numeric(df['trackster_energy'], errors='coerce')\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare Trackster-Level Data for Purity\n",
    "# -----------------------------\n",
    "def prepare_trackster_data(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by ['event_index', 'trackster_id'] so that each trackster is counted once.\n",
    "    For each group:\n",
    "      - Take the first trackster_energy,\n",
    "      - Take the minimum reco_to_sim_score,\n",
    "      - Mark the trackster as 'associated' if the minimum reco_to_sim_score is < 0.2.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(['event_index', 'trackster_id']).agg({\n",
    "        'trackster_energy': 'first',       # Use the first trackster_energy value.\n",
    "        'reco_to_sim_score': 'min'           # Minimum score among the rows for that trackster.\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Mark as associated if reco_to_sim_score is < 0.2.\n",
    "    grouped['assoc'] = (grouped['reco_to_sim_score'] < 0.2).astype(int)\n",
    "    return grouped\n",
    "\n",
    "# Prepare the trackster-level data for both DataFrames.\n",
    "df_CL_ts = prepare_trackster_data(df_CL)\n",
    "df_TICL_ts   = prepare_trackster_data(df_TICL)\n",
    "\n",
    "# -----------------------------\n",
    "# Bin Tracksters by Energy\n",
    "# -----------------------------\n",
    "# Define energy bins based on the range of trackster_energy from df_CL.\n",
    "min_energy_ts = df_CL_ts['trackster_energy'].min()\n",
    "max_energy_ts = 300\n",
    "n_bins_ts = 10  # Adjust the number of bins if desired.\n",
    "energy_bins_ts = np.linspace(min_energy_ts, max_energy_ts, n_bins_ts + 1)\n",
    "\n",
    "# Assign each trackster to an energy bin.\n",
    "df_CL_ts['energy_bin'] = pd.cut(df_CL_ts['trackster_energy'],\n",
    "                                    bins=energy_bins_ts, labels=False, include_lowest=True)\n",
    "df_TICL_ts['energy_bin']   = pd.cut(df_TICL_ts['trackster_energy'],\n",
    "                                    bins=energy_bins_ts, labels=False, include_lowest=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Calculate Purity per Energy Bin\n",
    "# -----------------------------\n",
    "def aggregate_purity(df):\n",
    "    \"\"\"\n",
    "    For each energy bin, calculate:\n",
    "      - Total number of tracksters,\n",
    "      - Number of associated tracksters (with reco_to_sim_score < 0.2),\n",
    "      - Purity = (number of associated tracksters) / (total number).\n",
    "    \"\"\"\n",
    "    agg = df.groupby('energy_bin').agg(\n",
    "        total_ts = ('trackster_energy', 'count'),\n",
    "        assoc_ts = ('assoc', 'sum')\n",
    "    ).reset_index()\n",
    "    agg['purity'] = agg['assoc_ts'] / agg['total_ts']\n",
    "    \n",
    "    agg['purity_error'] = np.sqrt(agg['purity'] * (1 - agg['purity']) / agg['total_ts'])\n",
    "    return agg\n",
    "\n",
    "agg_CL_ts = aggregate_purity(df_CL_ts)\n",
    "agg_TICL_ts   = aggregate_purity(df_TICL_ts)\n",
    "\n",
    "# Reindex both aggregated DataFrames so that they have one row per energy bin (0 to n_bins_ts-1)\n",
    "agg_CL_ts = agg_CL_ts.set_index('energy_bin').reindex(range(n_bins_ts), fill_value=np.nan).reset_index()\n",
    "agg_TICL_ts   = agg_TICL_ts.set_index('energy_bin').reindex(range(n_bins_ts), fill_value=np.nan).reset_index()\n",
    "\n",
    "# -----------------------------\n",
    "# Plot Purity vs Trackster Energy with Histogram Overlay\n",
    "# -----------------------------\n",
    "# Compute bin centers for plotting: average of adjacent bin edges.\n",
    "bin_centers_ts = (energy_bins_ts[:-1] + energy_bins_ts[1:]) / 2\n",
    "bar_width_ts = energy_bins_ts[1] - energy_bins_ts[0]\n",
    "\n",
    "purity_ratio = agg_CL_ts['purity'] / agg_TICL_ts['purity']\n",
    "purity_ratio_error = purity_ratio * np.sqrt(\n",
    "    (agg_CL_ts['purity_error'] / agg_CL_ts['purity'])**2 +\n",
    "    (agg_TICL_ts['purity_error'] / agg_TICL_ts['purity'])**2\n",
    ")\n",
    "\n",
    "# Create a figure with two y-axes.\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.1)  # Set spacing to zero\n",
    "\n",
    "# Plot purity curves on the primary y-axis.\n",
    "ax1.errorbar(bin_centers_ts, agg_CL_ts['purity'], yerr = agg_CL_ts['purity_error'],marker='o', linestyle='--', color = 'blue', label='Our Model')\n",
    "ax1.errorbar(bin_centers_ts, agg_TICL_ts['purity'], yerr = agg_TICL_ts['purity_error'], marker='x', linestyle='--', color = 'green', label='TICL')\n",
    "ax1.set_ylabel('Purity')\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot histogram (trackster count per energy bin) on the secondary y-axis.\n",
    "ax1_hist = ax1.twinx()  # Create secondary y-axis for histogram\n",
    "ax1_hist.bar(bin_centers_ts, agg_CL_ts['total_ts'], width=bar_width_ts, color='lightblue', alpha=0.4)\n",
    "ax1_hist.set_ylabel('Number of Merged Tracksters', fontsize=12)\n",
    "ax1_hist.set_ylim(0, agg_CL_ts['total_ts'].max() * 1.2)\n",
    "\n",
    "ax2.errorbar(bin_centers_ts, purity_ratio, yerr=purity_ratio_error, fmt='x', color='blue', markersize=7)\n",
    "ax2.axhline(1.0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Merged Trackster Energy [GeV]', fontsize=12)\n",
    "ax2.set_ylabel('Ratio', fontsize=12)\n",
    "ax2.set_ylim(0.95, 1.05)\n",
    "\n",
    "plt.title(r'Purity vs Merged Trackster Energy: 5 Electron', fontsize=14)\n",
    "plt.savefig(\"plots/Purity.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL_best[df_CL_best['energy_bin']==0].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae892f70",
   "metadata": {},
   "source": [
    "# Response and Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c063f48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================================\n",
    "# 1. Select Best RecoTrackster (Best Associated Trackster) per SimTrackster\n",
    "# ====================================================\n",
    "def select_best_trackster(df):\n",
    "    \"\"\"\n",
    "    For each (event_index, cp_id) combination, select the row with the maximum shared_energy.\n",
    "    This is assumed to give the best associated RecoTrackster for each SimTrackster.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in ['cp_id', 'shared_energy', 'cp_energy', 'trackster_energy']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df_best = (df.groupby(['event_index', 'cp_id'], as_index=False)\n",
    "                 .apply(lambda g: g.loc[g['shared_energy'].idxmax()])\n",
    "                 .reset_index(drop=True))\n",
    "    return df_best\n",
    "\n",
    "# Process both dataframes (df_CL and df_TICL)\n",
    "df_CL_best   = select_best_trackster(df_CL)\n",
    "df_TICL_best = select_best_trackster(df_TICL)\n",
    "\n",
    "# ====================================================\n",
    "# 2. Compute the Response Ratio = (raw energy / regressed energy)\n",
    "# ====================================================\n",
    "# Here we treat trackster_energy as the raw energy and cp_energy as the regressed energy.\n",
    "df_CL_best['ratio']   = df_CL_best['trackster_energy'] / df_CL_best['cp_energy']\n",
    "df_TICL_best['ratio'] = df_TICL_best['trackster_energy'] / df_TICL_best['cp_energy']\n",
    "\n",
    "# ====================================================\n",
    "# 3. Bin by regressed energy using specified bin centers\n",
    "# ====================================================\n",
    "# Define desired energy bin centers (in GeV)\n",
    "centers = np.array([10, 20, 30, 40, 60, 80, 100, 150, 200])\n",
    "n_bins = len(centers)\n",
    "\n",
    "# Compute bin edges: use midpoints for interior edges; extrapolate for the first and last edge.\n",
    "edges = np.empty(n_bins + 1)\n",
    "edges[1:-1] = (centers[:-1] + centers[1:]) / 2.0\n",
    "edges[0]    = centers[0] - (centers[1] - centers[0]) / 2.0\n",
    "edges[-1]   = centers[-1] + (centers[-1] - centers[-2]) / 2.0\n",
    "\n",
    "# Assign each event an energy bin based on its regressed energy (cp_energy)\n",
    "df_CL_best['energy_bin']   = pd.cut(df_CL_best['cp_energy'], bins=edges, labels=False)\n",
    "df_TICL_best['energy_bin'] = pd.cut(df_TICL_best['cp_energy'], bins=edges, labels=False)\n",
    "\n",
    "# ====================================================\n",
    "# 4. Compute Sigma Effective on the Scaled Response for Each Energy Bin\n",
    "# ====================================================\n",
    "def compute_sigma_effective_scaled(df, n_bins):\n",
    "    \"\"\"\n",
    "    For each energy bin:\n",
    "      - Extract the response ratio distribution.\n",
    "      - Remove non-finite values.\n",
    "      - Scale the distribution by dividing by its mean (so the mean becomes 1).\n",
    "      - Compute sigma effective as half the width of the narrowest window that contains 68% of the scaled data.\n",
    "      - Also compute an approximate error (sigma_eff / sqrt(N)).\n",
    "    Returns arrays: sigma_eff_vals, sigma_eff_errs, and event counts per bin.\n",
    "    \"\"\"\n",
    "    sigma_eff_vals = []\n",
    "    sigma_eff_errs = []\n",
    "    counts = []\n",
    "    \n",
    "    for b in range(n_bins):\n",
    "        # Select data in the bin\n",
    "        bin_data = df.loc[df['energy_bin'] == b, 'ratio'].dropna()\n",
    "        # Filter: only finite values (optionally you may further restrict to [0,2])\n",
    "        bin_data = bin_data[np.isfinite(bin_data)]\n",
    "        bin_data = bin_data[(bin_data >= 0) & (bin_data <= 2)]\n",
    "        counts.append(len(bin_data))\n",
    "        \n",
    "        if len(bin_data) == 0:\n",
    "            sigma_eff_vals.append(np.nan)\n",
    "            sigma_eff_errs.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        # Scale the distribution by its mean so that the mean response becomes 1.\n",
    "        mean_val = bin_data.mean()\n",
    "        scaled = bin_data / mean_val\n",
    "        \n",
    "        # Compute sigma effective: half the width of the narrowest window containing 68% of the scaled data.\n",
    "        sorted_scaled = np.sort(scaled)\n",
    "        n_points = len(sorted_scaled)\n",
    "        window_size = int(np.round(0.683 * n_points))\n",
    "        if window_size < 1:\n",
    "            window_size = 1\n",
    "        \n",
    "        min_width = np.inf\n",
    "        best_range = (None, None)\n",
    "        for i in range(n_points - window_size + 1):\n",
    "            width = sorted_scaled[i + window_size - 1] - sorted_scaled[i]\n",
    "            if width < min_width:\n",
    "                min_width = width\n",
    "                best_range = (sorted_scaled[i], sorted_scaled[i + window_size - 1])\n",
    "        \n",
    "        # Sigma effective is half the window width\n",
    "        sigma_eff = (best_range[1] - best_range[0])\n",
    "        sigma_eff_err = sigma_eff / np.sqrt(n_points) if n_points > 1 else np.nan\n",
    "        \n",
    "        sigma_eff_vals.append(sigma_eff)\n",
    "        sigma_eff_errs.append(sigma_eff_err)\n",
    "    \n",
    "    return np.array(sigma_eff_vals), np.array(sigma_eff_errs), np.array(counts)\n",
    "\n",
    "# Compute sigma effective for each energy bin for both data sets.\n",
    "sigma_eff_CL, sigma_eff_err_CL, counts_CL = compute_sigma_effective_scaled(df_CL_best, n_bins)\n",
    "sigma_eff_TICL, sigma_eff_err_TICL, counts_TICL = compute_sigma_effective_scaled(df_TICL_best, n_bins)\n",
    "\n",
    "# ====================================================\n",
    "# 4.5 Debug: Plot a few Histograms for Selected Energy Bins\n",
    "# ====================================================\n",
    "# For debugging, we display the scaled response histograms for a few bins.\n",
    "# Here we choose the first, middle, and last energy bins.\n",
    "debug_bins = [0, n_bins // 2, n_bins - 1]\n",
    "fig, axes = plt.subplots(len(debug_bins), 2, figsize=(12, 4 * len(debug_bins)), sharex=True)\n",
    "for j, b in enumerate(debug_bins):\n",
    "    for dataset, df_best, ax in zip([\"CL\", \"TICL\"], [df_CL_best, df_TICL_best], axes[j]):\n",
    "        # Get the ratio distribution for this energy bin.\n",
    "        bin_data = df_best.loc[df_best['energy_bin'] == b, 'ratio'].dropna()\n",
    "        bin_data = bin_data[np.isfinite(bin_data)]\n",
    "        # (Optional) Restrict the ratio values to a reasonable range, e.g. [0, 2].\n",
    "        bin_data = bin_data[(bin_data >= 0) & (bin_data <= 2)]\n",
    "        if len(bin_data) == 0:\n",
    "            ax.text(0.5, 0.5, \"No data in bin\", transform=ax.transAxes,\n",
    "                    horizontalalignment=\"center\")\n",
    "            continue\n",
    "        # Scale the distribution by its mean.\n",
    "        mean_val = bin_data.mean()\n",
    "        scaled = bin_data / mean_val\n",
    "        \n",
    "        # Compute sigma effective for this bin (for display purposes)\n",
    "        sorted_scaled = np.sort(scaled)\n",
    "        n_points = len(sorted_scaled)\n",
    "        window_size = int(np.round(0.683 * n_points))\n",
    "        if window_size < 1:\n",
    "            window_size = 1\n",
    "        min_width = np.inf\n",
    "        best_range = (None, None)\n",
    "        for i in range(n_points - window_size + 1):\n",
    "            width = sorted_scaled[i + window_size - 1] - sorted_scaled[i]\n",
    "            if width < min_width:\n",
    "                min_width = width\n",
    "                best_range = (sorted_scaled[i], sorted_scaled[i + window_size - 1])\n",
    "        sigma_eff_this = (best_range[1] - best_range[0])\n",
    "        \n",
    "        # Plot the histogram of the scaled response\n",
    "        ax.hist(scaled, bins=30, alpha=0.7, color='C0')\n",
    "        ax.axvline(best_range[0], color='red', linestyle='--', label='68% window boundaries')\n",
    "        ax.axvline(best_range[1], color='red', linestyle='--')\n",
    "        ax.set_title(f\"{dataset} Bin {b} (center ~ {centers[b]} GeV)\\n sigma_eff = {sigma_eff_this:.3f}\")\n",
    "        ax.set_xlabel(\"Scaled Ratio\")\n",
    "        ax.set_ylabel(\"Counts\")\n",
    "        ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ====================================================\n",
    "# 5. Final Plot: Normalized Sigma Effective vs. Regressed Energy\n",
    "# ====================================================\n",
    "# Normalize sigma effective by dividing by the corresponding energy bin center.\n",
    "norm_sigma_eff_CL = sigma_eff_CL / centers\n",
    "norm_sigma_eff_err_CL = sigma_eff_err_CL / centers\n",
    "\n",
    "norm_sigma_eff_TICL = sigma_eff_TICL / centers\n",
    "norm_sigma_eff_err_TICL = sigma_eff_err_TICL / centers\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.errorbar(centers, norm_sigma_eff_CL, yerr=norm_sigma_eff_err_CL, fmt='bo-', capsize=5, label='CL')\n",
    "ax.errorbar(centers, norm_sigma_eff_TICL, yerr=norm_sigma_eff_err_TICL, fmt='gs-', capsize=5, label='TICL')\n",
    "ax.set_xlabel('Regressed Energy (GeV)', fontsize=14)\n",
    "ax.set_ylabel('Normalized Sigma Effective', fontsize=14)\n",
    "ax.set_title('Normalized Response Resolution vs. Regressed Energy', fontsize=16)\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db29132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50acdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80b383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cd145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab190ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting longitudinal energy profiles of both simulated and reconstructed showers along with associated pairs.\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "layer = data_file['clusters;3']['cluster_layer_id'].array()\n",
    "\n",
    "def calculate_all_event_profile_data(GT_ind, energies, recon_ind, layer, multi, num_events=10):\n",
    "    # Define unique colors for caloparticles\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "    # Loop over all events\n",
    "    for event_index in range(num_events):\n",
    "        # Get data for the current event\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_layer_id = layer[event_index]\n",
    "        event_multi = multi[event_index]\n",
    "\n",
    "        # Initialize lists for profiles and associations\n",
    "        simulated_profiles = []\n",
    "        reconstructed_profiles = []\n",
    "        associations = {}  # To store caloparticle-trackster associations\n",
    "\n",
    "        # Simulated particle shower\n",
    "        for cp_idx, caloparticle in enumerate(caloparticles):\n",
    "            cp_lc_layer_id = event_layer_id[caloparticle]\n",
    "            cp_lc_energies = event_energies[caloparticle] / event_multi[cp_idx]\n",
    "            df_cp = pd.DataFrame({'energy': cp_lc_energies, 'layer_id': cp_lc_layer_id})\n",
    "            layer_energy_cp = df_cp.groupby('layer_id')['energy'].sum().reset_index()\n",
    "            simulated_profiles.append(layer_energy_cp)\n",
    "\n",
    "        # Reconstructed particle shower\n",
    "        for trackster in tracksters:\n",
    "            tst_lc_layer_id = event_layer_id[trackster]\n",
    "            tst_lc_energies = event_energies[trackster]\n",
    "            df_tst = pd.DataFrame({'energy': tst_lc_energies, 'layer_id': tst_lc_layer_id})\n",
    "            layer_energy_tst = df_tst.groupby('layer_id')['energy'].sum().reset_index()\n",
    "            reconstructed_profiles.append(layer_energy_tst)\n",
    "\n",
    "        # Calculate sim_to_reco score and determine associations\n",
    "\n",
    "\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            Calo_multi = event_multi[calo_idx]\n",
    "            cp_energy = np.sum(event_energies[caloparticle] / Calo_multi)\n",
    "            \n",
    "            # Loop over all Tracksters\n",
    "            max_shared_energy = 0.0\n",
    "            best_tst_idx = None\n",
    "            \n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                # Calculate sim-to-reco score\n",
    "                \n",
    "                \n",
    "                shared_det_ids = set(calo_det_id for calo_det_id in caloparticle).intersection(set(trackster))\n",
    "                if not shared_det_ids:\n",
    "                    continue  # No shared det_ids\n",
    "\n",
    "                # Calculate shared_energy\n",
    "                shared_energy = np.sum(event_energies[list(shared_det_ids)])\n",
    "\n",
    "                # Check if shared_energy meets the threshold\n",
    "                if shared_energy >= 0.5 * cp_energy and shared_energy > max_shared_energy:\n",
    "                    max_shared_energy = shared_energy\n",
    "                    best_tst_idx = trackster_idx\n",
    "\n",
    "            if best_tst_idx is not None:\n",
    "                associations[calo_idx] = best_tst_idx\n",
    "                    \n",
    "        \n",
    "        \n",
    "        # Determine energy range from reconstructed showers for consistent y-axis\n",
    "        min_energy = min(profile['energy'].min() for profile in reconstructed_profiles)\n",
    "        max_energy = max(profile['energy'].max() for profile in reconstructed_profiles)\n",
    "    \n",
    "    \n",
    "        # Plot simulated profiles\n",
    "        fig, axes = plt.subplots(1, len(simulated_profiles) + 1, figsize=(15, 3), constrained_layout=True)\n",
    "\n",
    "        for idx, profile in enumerate(simulated_profiles):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(profile['layer_id'], profile['energy'], marker='o', linestyle='-', color=colors[idx % len(colors)])\n",
    "            ax.set_ylim(min_energy, max_energy)\n",
    "            ax.set_title(f'Simulated Shower {idx + 1} (Event {event_index + 1})')\n",
    "            ax.set_xlabel('Layer ID')\n",
    "            ax.set_ylabel('Energy')\n",
    "\n",
    "        # Plot all reconstructed profiles in a single plot\n",
    "        ax = axes[-1]\n",
    "        for idx, profile in enumerate(reconstructed_profiles):\n",
    "            # Determine color based on association\n",
    "            associated_cp = [cp_idx for cp_idx, tst_idx in associations.items() if tst_idx == idx]\n",
    "            color = colors[associated_cp[0] % len(colors)] if associated_cp else 'gray'\n",
    "            ax.plot(profile['layer_id'], profile['energy'], marker='x', linestyle='--', color=color)\n",
    "        ax.set_ylim(min_energy, max_energy)\n",
    "        ax.set_title(f'Reconstructed Showers (Event {event_index + 1})')\n",
    "        ax.set_xlabel('Layer ID')\n",
    "        ax.set_ylabel('Energy')\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "events_ours = calculate_all_event_profile_data(GT_ind_filt, energies, recon_ind, layer, GT_mult_filt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd81e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "events_TICL = calculate_all_event_profile_data(GT_ind_filt, energies, MT_ind_filt, layer, GT_mult_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ec2c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting longitudinal energy profiles of both simulated and reconstructed showers along with associated pairs.\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define LC_x, LC_y, LC_z somewhere or import if needed.\n",
    "# For example purposes, assume they are defined globally.\n",
    "\n",
    "def calculate_all_event_profile_data(GT_ind, energies, recon_ind, layer, multi, num_events=5):\n",
    "    # Define unique colors (using Tableau colors)\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "    # Loop over all events\n",
    "    for event_index in range(num_events):\n",
    "        # Get data for the current event\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_layer_id = layer[event_index]\n",
    "        event_multi = multi[event_index]\n",
    "        event_LC_x = LC_x[event_index]\n",
    "        event_LC_y = LC_y[event_index]\n",
    "        event_LC_z = LC_z[event_index]\n",
    "\n",
    "        # Initialize lists for profiles and associations\n",
    "        simulated_profiles = []\n",
    "        reconstructed_profiles = []\n",
    "        associations = {}  # To store caloparticle-trackster associations\n",
    "        cp_lc_pos_all = []\n",
    "        cp_lc_energies_all = []\n",
    "        tst_lc_pos_all = []\n",
    "        tst_lc_energies_all = []\n",
    "        total_cp_energies = []\n",
    "        total_tst_energies = []\n",
    "\n",
    "        # --- Simulated particle shower ---\n",
    "        for cp_idx, caloparticle in enumerate(caloparticles):\n",
    "            cp_lc_layer_id = event_layer_id[caloparticle]\n",
    "            # Divide energy by multi (assume event_multi[cp_idx] corresponds to this caloparticle)\n",
    "            cp_lc_energies = event_energies[caloparticle] / event_multi[cp_idx]\n",
    "            df_cp = pd.DataFrame({'energy': cp_lc_energies, 'layer_id': cp_lc_layer_id})\n",
    "            layer_energy_cp = df_cp.groupby('layer_id')['energy'].sum().reset_index()\n",
    "            simulated_profiles.append(layer_energy_cp)\n",
    "            \n",
    "            cp_lc_pos_all.append([event_LC_x[caloparticle], event_LC_y[caloparticle], event_LC_z[caloparticle]])\n",
    "            cp_lc_energies_all.append(cp_lc_energies)\n",
    "            total_cp_energies.append(layer_energy_cp['energy'].sum())\n",
    "\n",
    "        # Build a dedicated color map for simulated showers\n",
    "        cp_color_map = {cp_idx: colors[cp_idx % len(colors)] for cp_idx in range(len(simulated_profiles))}\n",
    "\n",
    "        # --- Reconstructed particle shower ---\n",
    "        for trackster in tracksters:\n",
    "            tst_lc_layer_id = event_layer_id[trackster]\n",
    "            tst_lc_energies = event_energies[trackster]\n",
    "            df_tst = pd.DataFrame({'energy': tst_lc_energies, 'layer_id': tst_lc_layer_id})\n",
    "            layer_energy_tst = df_tst.groupby('layer_id')['energy'].sum().reset_index()\n",
    "            reconstructed_profiles.append(layer_energy_tst)\n",
    "            \n",
    "            tst_lc_pos_all.append([event_LC_x[trackster], event_LC_y[trackster], event_LC_z[trackster]])\n",
    "            tst_lc_energies_all.append(tst_lc_energies)\n",
    "            total_tst_energy = np.sum(tst_lc_energies)\n",
    "            total_tst_energies.append(layer_energy_tst['energy'].sum())\n",
    "\n",
    "        # --- Calculate sim_to_reco score and determine associations ---\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            # Use corresponding multiplicity for this calo particle\n",
    "            Calo_multi = event_multi[calo_idx]\n",
    "            cp_energy = np.sum(event_energies[caloparticle] / Calo_multi)\n",
    "            \n",
    "            max_shared_energy = 0.0\n",
    "            best_tst_idx = None\n",
    "            \n",
    "            # Loop over all Tracksters\n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                # Calculate shared detector IDs (assuming caloparticle and trackster are lists/arrays of detector IDs)\n",
    "                shared_det_ids = set(caloparticle).intersection(set(trackster))\n",
    "                if not shared_det_ids:\n",
    "                    continue  # No shared detector IDs\n",
    "\n",
    "                # Calculate shared energy\n",
    "                shared_energy = np.sum(event_energies[list(shared_det_ids)])\n",
    "\n",
    "                # Check if shared_energy meets the threshold\n",
    "                if shared_energy >= 0.5 * cp_energy and shared_energy > max_shared_energy:\n",
    "                    max_shared_energy = shared_energy\n",
    "                    best_tst_idx = trackster_idx\n",
    "                    \n",
    "            if best_tst_idx is not None:\n",
    "                associations[calo_idx] = best_tst_idx\n",
    "\n",
    "        # --- Determine energy range from reconstructed showers for consistent y-axis ---\n",
    "        # (if there is at least one reconstructed profile)\n",
    "        if reconstructed_profiles:\n",
    "            min_energy = min(profile['energy'].min() for profile in reconstructed_profiles)\n",
    "            max_energy = max(profile['energy'].max() for profile in reconstructed_profiles)\n",
    "        else:\n",
    "            min_energy, max_energy = 0, 1\n",
    "\n",
    "        # --- Plot simulated profiles (2D) ---\n",
    "        fig, axes = plt.subplots(1, len(simulated_profiles) + 1, figsize=(15, 3), constrained_layout=True)\n",
    "        fig.suptitle(f'Event {event_index + 1}', fontsize=16)\n",
    "\n",
    "        for idx, profile in enumerate(simulated_profiles):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(profile['layer_id'], profile['energy'],\n",
    "                    marker='o', linestyle='-', color=cp_color_map[idx],\n",
    "                    label=f\"E = {total_cp_energies[idx]:.2f} GeV\")\n",
    "            ax.set_ylim(min_energy, max_energy)\n",
    "            ax.set_title(f'Simulated Shower {idx + 1}')\n",
    "            ax.set_xlabel('Detector Layer')\n",
    "            ax.set_ylabel('Energy')\n",
    "            ax.legend()\n",
    "\n",
    "        # --- Plot all reconstructed profiles in a single 2D plot ---\n",
    "        ax = axes[-1]\n",
    "        for idx, profile in enumerate(reconstructed_profiles):\n",
    "            # Look up the corresponding calo (if any) using associations\n",
    "            associated_cp = [cp_idx for cp_idx, tst_idx in associations.items() if tst_idx == idx]\n",
    "            # Use the simulated shower color if there is an association; otherwise, default to gray\n",
    "            color = cp_color_map[associated_cp[0]] if associated_cp else 'gray'\n",
    "            ax.plot(profile['layer_id'], profile['energy'],\n",
    "                    marker='x', linestyle='--', color=color,\n",
    "                    label=f\"E = {total_tst_energies[idx]:.2f} GeV\")\n",
    "            \n",
    "        ax.set_ylim(min_energy, max_energy)\n",
    "        ax.set_title('Reconstructed Showers')\n",
    "        ax.set_xlabel('Detector Layer')\n",
    "        ax.set_ylabel('Energy')\n",
    "        ax.legend(title='Reco Energies', loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "        # --- Determine global energy scale and axis limits for 3D plots ---\n",
    "        all_energies = np.concatenate(cp_lc_energies_all + tst_lc_energies_all)\n",
    "        global_min_energy, global_max_energy = np.min(all_energies), np.max(all_energies)\n",
    "        x_lim = [min(event_LC_x), max(event_LC_x)]\n",
    "        y_lim = [min(event_LC_y), max(event_LC_y)]\n",
    "        z_lim = [min(event_LC_z), max(event_LC_z)]\n",
    "\n",
    "        # --- 3D Scatter Plot for All Simulated Layer Clusters ---\n",
    "        fig3d = plt.figure(figsize=(15, 7))\n",
    "        ax_3d = fig3d.add_subplot(1, 2, 1, projection='3d')\n",
    "        for idx, (pos, energies_) in enumerate(zip(cp_lc_pos_all, cp_lc_energies_all)):\n",
    "            x, y, z = pos\n",
    "            sizes = np.log1p(np.array(energies_)) * 100  # Scale sizes for visibility\n",
    "            ax_3d.scatter(x, y, z, c=cp_color_map[idx], alpha=0.3, s=sizes, edgecolors='k', label=f'Shower {idx+1}')\n",
    "        \n",
    "        ax_3d.set_xlim(x_lim)\n",
    "        ax_3d.set_ylim(y_lim)\n",
    "        ax_3d.set_zlim(z_lim)\n",
    "        ax_3d.set_title('3D visualisation of All Simulated Showers')\n",
    "        ax_3d.set_xlabel('X')\n",
    "        ax_3d.set_ylabel('Y')\n",
    "        ax_3d.set_zlabel('Z')\n",
    "        ax_3d.legend()\n",
    "\n",
    "        # --- 3D Scatter Plot for All Reconstructed Tracksters ---\n",
    "        ax_3d_reco = fig3d.add_subplot(1, 2, 2, projection='3d')\n",
    "        for idx, (pos, energies_) in enumerate(zip(tst_lc_pos_all, tst_lc_energies_all)):\n",
    "            x, y, z = pos\n",
    "            sizes = np.log1p(np.array(energies_)) * 100  # Scale sizes for visibility\n",
    "            # Look up association for the trackster to decide its color\n",
    "            associated_cp = [cp_idx for cp_idx, tst_idx in associations.items() if tst_idx == idx]\n",
    "            color = cp_color_map[associated_cp[0]] if associated_cp else 'gray'\n",
    "            ax_3d_reco.scatter(x, y, z, c=color, alpha=0.3, s=sizes, edgecolors='k', label=f'Trackster {idx+1}')\n",
    "        \n",
    "        ax_3d_reco.set_xlim(x_lim)\n",
    "        ax_3d_reco.set_ylim(y_lim)\n",
    "        ax_3d_reco.set_zlim(z_lim)\n",
    "        ax_3d_reco.set_title('3D visualisation of All Reconstructed Tracksters')\n",
    "        ax_3d_reco.set_xlabel('X')\n",
    "        ax_3d_reco.set_ylabel('Y')\n",
    "        ax_3d_reco.set_zlabel('Z')\n",
    "        ax_3d_reco.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "events = calculate_all_event_profile_data(GT_ind_filt, energies, recon_ind, layer, GT_mult_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1a4b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events = calculate_all_event_profile_data(GT_ind_filt, energies, MT_ind_filt, layer, GT_mult_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f57240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
