{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a7f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "\n",
    "#singularity shell --bind /afs/cern.ch/user/p/pkakhand/public/CL/  /afs/cern.ch/user/p/pkakhand/geometricdl.sif\n",
    "\n",
    "#singularity shell --bind /eos/project/c/contrast/public/solar/  /afs/cern.ch/user/p/pkakhand/geometricdl.sif\n",
    "#source /cvmfs/sft.cern.ch/lcg/views/LCG_103cuda/x86_64-centos9-gcc11-opt/setup.sh\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "def remove_duplicates(A,B):    \n",
    "    all_masks = []\n",
    "    for event_idx, event in enumerate(A):\n",
    "        flat_A = np.array(ak.flatten(A[event_idx]))\n",
    "        flat_B = np.array(ak.flatten(B[event_idx]))\n",
    "        \n",
    "        # Initialize a mask to keep track of which values to keep\n",
    "        mask = np.zeros_like(flat_A, dtype=bool)\n",
    "\n",
    "        # Iterate over the unique elements in A\n",
    "        for elem in np.unique(flat_A):\n",
    "            # Get the indices where the element occurs in A\n",
    "            indices = np.where(flat_A == elem)[0]\n",
    "\n",
    "            # If there's more than one occurrence, keep the one with the max B value\n",
    "            if len(indices) > 1:\n",
    "                max_index = indices[np.argmax(flat_B[indices])]\n",
    "                mask[max_index] = True\n",
    "            else:\n",
    "                # If there's only one occurrence, keep it\n",
    "                mask[indices[0]] = True\n",
    "\n",
    "        unflattened_mask = ak.unflatten(mask, ak.num(A[event_idx]))\n",
    "        all_masks.append(unflattened_mask)\n",
    "        \n",
    "    return ak.Array(all_masks)\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "        input: layer clusters\n",
    "\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp = 'train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.step_size = 500\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self,max_events):\n",
    "        counter = 0\n",
    "        arrLens0 = []\n",
    "        arrLens1 = []\n",
    "\n",
    "        print(\"### Loading data\")\n",
    "        for fi,path in enumerate(tqdm(self.raw_paths)):\n",
    "\n",
    "\n",
    "            if self.inp == 'train':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            elif self.inp == 'val':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            else:\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            \n",
    "            crosstree =  uproot.open(path)[cluster_path]\n",
    "            crosscounter = 0\n",
    "            for array in uproot.iterate(f\"{path}:{sim_path}\", [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n",
    "            \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\"], step_size=self.step_size):\n",
    "            \n",
    "                tmp_stsCP_vertices_x = array['vertices_x']\n",
    "                tmp_stsCP_vertices_y = array['vertices_y']\n",
    "                tmp_stsCP_vertices_z = array['vertices_z']\n",
    "                tmp_stsCP_vertices_energy = array['vertices_energy']\n",
    "                tmp_stsCP_vertices_time = array['vertices_time']\n",
    "                tmp_stsCP_vertices_indexes = array['vertices_indexes']\n",
    "                tmp_stsCP_barycenter_x = array['barycenter_x']\n",
    "                tmp_stsCP_barycenter_y = array['barycenter_y']\n",
    "                tmp_stsCP_barycenter_z = array['barycenter_z']\n",
    "\n",
    "\n",
    "                tmp_stsCP_vertices_multiplicity = array['vertices_multiplicity']\n",
    "                \n",
    "                # weighted energies (A LC appears in its caloparticle assignment array as the energy it contributes not full energy)\n",
    "                #tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy * tmp_stsCP_vertices_multiplicity\n",
    "                \n",
    "                self.step_size = min(self.step_size,len(tmp_stsCP_vertices_x))\n",
    "\n",
    "\n",
    "                # Code block for reading from other tree\n",
    "                tmp_all_vertices_layer_id = crosstree['cluster_layer_id'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                #tmp_all_vertices_radius = crosstree['cluster_radius'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_noh = crosstree['cluster_number_of_hits'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_eta = crosstree['position_eta'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_phi = crosstree['position_phi'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                crosscounter += 1\n",
    "\n",
    "                layer_id_list = []\n",
    "                radius_list = []\n",
    "                noh_list = []\n",
    "                eta_list = []\n",
    "                phi_list = []\n",
    "                for evt_row in range(len(tmp_all_vertices_noh)):\n",
    "                    #print(\"Event no: %i\"%evt_row)\n",
    "                    #print(\"There are %i particles in this event\"%len(tmp_stsCP_vertices_indexes[evt_row]))\n",
    "                    layer_id_list_one_event = []\n",
    "                    #radius_list_one_event = []\n",
    "                    noh_list_one_event = []\n",
    "                    eta_list_one_event = []\n",
    "                    phi_list_one_event = []\n",
    "                    for particle in range(len(tmp_stsCP_vertices_indexes[evt_row])):\n",
    "                        #print(\"Particle no: %i\"%particle)\n",
    "                        #print(\"A\")\n",
    "                        #print(np.array(tmp_all_vertices_radius[evt_row]).shape)\n",
    "                        #print(\"B\")\n",
    "                        #print(np.array(tmp_stsCP_vertices_indexes[evt_row][particle]).shape)\n",
    "                        #print(\"C\")\n",
    "                        tmp_stsCP_vertices_layer_id_one_particle = tmp_all_vertices_layer_id[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #tmp_stsCP_vertices_radius_one_particle = tmp_all_vertices_radius[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_noh_one_particle = tmp_all_vertices_noh[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_eta_one_particle = tmp_all_vertices_eta[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_phi_one_particle = tmp_all_vertices_phi[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #print(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        layer_id_list_one_event.append(tmp_stsCP_vertices_layer_id_one_particle)\n",
    "                        #radius_list_one_event.append(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        noh_list_one_event.append(tmp_stsCP_vertices_noh_one_particle)\n",
    "                        eta_list_one_event.append(tmp_stsCP_vertices_eta_one_particle)\n",
    "                        phi_list_one_event.append(tmp_stsCP_vertices_phi_one_particle)\n",
    "                    layer_id_list.append(layer_id_list_one_event)\n",
    "                    #radius_list.append(radius_list_one_event)\n",
    "                    noh_list.append(noh_list_one_event)\n",
    "                    eta_list.append(eta_list_one_event)\n",
    "                    phi_list.append(phi_list_one_event)\n",
    "                tmp_stsCP_vertices_layer_id = ak.Array(layer_id_list)                \n",
    "                #tmp_stsCP_vertices_radius = ak.Array(radius_list)                \n",
    "                tmp_stsCP_vertices_noh = ak.Array(noh_list)                \n",
    "                tmp_stsCP_vertices_eta = ak.Array(eta_list)                \n",
    "                tmp_stsCP_vertices_phi = ak.Array(phi_list)                \n",
    "                \n",
    "                # Apply filter noh > 1 for the LCs\n",
    "                skim_mask_noh = tmp_stsCP_vertices_noh > 1.0\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_noh]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_noh]\n",
    "                #tmp_stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_noh]\n",
    "                \n",
    "                # Remove duplicates by only allowing the caloparticle that contributed the most energy to a LC to actually contribute.\n",
    "                energyPercent = 1/tmp_stsCP_vertices_multiplicity\n",
    "                skim_mask_energyPercent = remove_duplicates(tmp_stsCP_vertices_indexes,energyPercent)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_energyPercent]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_energyPercent]\n",
    "                #tmp_stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes\n",
    "                tmp_stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_energyPercent]\n",
    "               \n",
    "                \n",
    "                \n",
    "                #SHOULD BE LEN(E) >= 2 for MULTI particles\n",
    "                skim_mask = []\n",
    "                for e in tmp_stsCP_vertices_x:\n",
    "                    if 2 <= len(e): #<------ only train on samples with > 1 particle\n",
    "                        skim_mask.append(True)\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask]\n",
    "                #tmp_stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes_unmasked[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes_filt[skim_mask]\n",
    "\n",
    "\n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_x = tmp_stsCP_vertices_x\n",
    "                    self.stsCP_vertices_y = tmp_stsCP_vertices_y\n",
    "                    self.stsCP_vertices_z = tmp_stsCP_vertices_z\n",
    "                    self.stsCP_vertices_energy = tmp_stsCP_vertices_energy\n",
    "                    self.stsCP_vertices_time = tmp_stsCP_vertices_time\n",
    "                    self.stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id\n",
    "                    #self.stsCP_vertices_radius = tmp_stsCP_vertices_radius\n",
    "                    self.stsCP_vertices_noh = tmp_stsCP_vertices_noh\n",
    "                    self.stsCP_vertices_eta = tmp_stsCP_vertices_eta\n",
    "                    self.stsCP_vertices_phi = tmp_stsCP_vertices_phi\n",
    "                    self.stsCP_vertices_indexes = tmp_stsCP_vertices_indexes\n",
    "                    #self.stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes_unmasked\n",
    "                    self.stsCP_barycenter_x = tmp_stsCP_barycenter_x\n",
    "                    self.stsCP_barycenter_y = tmp_stsCP_barycenter_y\n",
    "                    self.stsCP_barycenter_z = tmp_stsCP_barycenter_z\n",
    "                    self.stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity\n",
    "                    self.stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes_filt\n",
    "                else:\n",
    "                    self.stsCP_vertices_x = ak.concatenate((self.stsCP_vertices_x,tmp_stsCP_vertices_x))\n",
    "                    self.stsCP_vertices_y = ak.concatenate((self.stsCP_vertices_y,tmp_stsCP_vertices_y))\n",
    "                    self.stsCP_vertices_z = ak.concatenate((self.stsCP_vertices_z,tmp_stsCP_vertices_z))\n",
    "                    self.stsCP_vertices_energy = ak.concatenate((self.stsCP_vertices_energy,tmp_stsCP_vertices_energy))\n",
    "                    self.stsCP_vertices_time = ak.concatenate((self.stsCP_vertices_time,tmp_stsCP_vertices_time))\n",
    "                    self.stsCP_vertices_layer_id = ak.concatenate((self.stsCP_vertices_layer_id,tmp_stsCP_vertices_layer_id))\n",
    "                    #self.stsCP_vertices_radius = ak.concatenate((self.stsCP_vertices_radius,tmp_stsCP_vertices_radius))\n",
    "                    self.stsCP_vertices_noh = ak.concatenate((self.stsCP_vertices_noh,tmp_stsCP_vertices_noh))\n",
    "                    self.stsCP_vertices_eta = ak.concatenate((self.stsCP_vertices_eta,tmp_stsCP_vertices_eta))\n",
    "                    self.stsCP_vertices_phi = ak.concatenate((self.stsCP_vertices_phi,tmp_stsCP_vertices_phi))\n",
    "                    self.stsCP_vertices_indexes = ak.concatenate((self.stsCP_vertices_indexes,tmp_stsCP_vertices_indexes))\n",
    "                    #self.stsCP_vertices_indexes_unmasked =  ak.concatenate((self.stsCP_vertices_indexes_unmasked,tmp_stsCP_vertices_indexes_unmasked))\n",
    "                    self.stsCP_barycenter_x = ak.concatenate((self.stsCP_barycenter_x,tmp_stsCP_barycenter_x))\n",
    "                    self.stsCP_barycenter_y = ak.concatenate((self.stsCP_barycenter_y,tmp_stsCP_barycenter_y))\n",
    "                    self.stsCP_barycenter_z = ak.concatenate((self.stsCP_barycenter_z,tmp_stsCP_barycenter_z))\n",
    "                    self.stsCP_vertices_multiplicity = ak.concatenate((self.stsCP_vertices_multiplicity, tmp_stsCP_vertices_multiplicity))\n",
    "                    self.stsCP_vertices_indexes_filt = ak.concatenate((self.stsCP_vertices_indexes_filt,tmp_stsCP_vertices_indexes_filt))\n",
    "                #print(len(self.stsCP_vertices_x))\n",
    "                counter += 1\n",
    "                if len(self.stsCP_vertices_x) > max_events:\n",
    "                    print(f\"Reached {max_events}!\")\n",
    "                    break\n",
    "            if len(self.stsCP_vertices_x) > max_events:\n",
    "                break\n",
    "     \n",
    "            \n",
    "            \n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            'Dataset not found. Please download it from {} and move all '\n",
    "            '*.z files to {}'.format(self.url, self.raw_dir))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.stsCP_vertices_x)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        \n",
    "        #raw_files = [osp.join(self.raw_dir, 'step3_NTUPLE.root')]\n",
    "\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Find the maximum index to determine the array length\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "\n",
    "            # Initialize an array with the correct size, filled with a placeholder (e.g., -1)\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "\n",
    "            # Populate the array based on the dictionary\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx in indices:\n",
    "                    reconstructed[idx] = value\n",
    "\n",
    "            return reconstructed\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    " \n",
    "        lc_x = self.stsCP_vertices_x[idx]\n",
    "        #print(ak.to_numpy(lc_x[0]).shape)\n",
    "        #print(ak.to_numpy(lc_x[1]).shape)\n",
    "        flat_lc_x = np.expand_dims(np.array(ak.flatten(lc_x)),axis=1)\n",
    "        lc_y = self.stsCP_vertices_y[idx]\n",
    "        flat_lc_y = np.expand_dims(np.array(ak.flatten(lc_y)),axis=1)\n",
    "        lc_z = self.stsCP_vertices_z[idx]\n",
    "        flat_lc_z = np.expand_dims(np.array(ak.flatten(lc_z)),axis=1)\n",
    "        lc_e = self.stsCP_vertices_energy[idx]\n",
    "        flat_lc_e = np.expand_dims(np.array(ak.flatten(lc_e)),axis=1)     \n",
    "        lc_t = self.stsCP_vertices_time[idx]\n",
    "        flat_lc_t = np.expand_dims(np.array(ak.flatten(lc_t)),axis=1)  \n",
    "        lc_layer_id = self.stsCP_vertices_layer_id[idx]\n",
    "        flat_lc_layer_id = np.expand_dims(np.array(ak.flatten(lc_layer_id)),axis=1)  \n",
    "        #lc_radius = self.stsCP_vertices_radius[idx]\n",
    "        #flat_lc_radius = np.expand_dims(np.array(ak.flatten(lc_radius)),axis=1)  \n",
    "        lc_noh = self.stsCP_vertices_noh[idx]\n",
    "        flat_lc_noh = np.expand_dims(np.array(ak.flatten(lc_noh)),axis=1)  \n",
    "        lc_eta = self.stsCP_vertices_eta[idx]\n",
    "        flat_lc_eta = np.expand_dims(np.array(ak.flatten(lc_eta)),axis=1)  \n",
    "        lc_phi = self.stsCP_vertices_phi[idx]\n",
    "        flat_lc_phi = np.expand_dims(np.array(ak.flatten(lc_phi)),axis=1)  \n",
    "\n",
    "        #lc_rad = self.stsCP_vertices_radius[idx]\n",
    "        #flat_lc_rad = np.expand_dims(np.array(ak.flatten(lc_rad)),axis=1)  \n",
    "\n",
    "        lc_indexes = self.stsCP_vertices_indexes_filt[idx]\n",
    "        flat_lc_id = np.expand_dims(np.array(ak.flatten(lc_indexes)),axis=1)  \n",
    "        lc_multiplicity = self.stsCP_vertices_multiplicity[idx]\n",
    "\n",
    "        lc_indexes_nofilt = self.stsCP_vertices_indexes[idx]\n",
    "\n",
    "        #flat_lc_feats = np.concatenate((flat_lc_x,flat_lc_y,flat_lc_z,flat_lc_e),axis=-1)\n",
    "        flat_lc_feats = np.concatenate((flat_lc_x,flat_lc_y,flat_lc_z,flat_lc_e,\\\n",
    "                                        flat_lc_layer_id,flat_lc_noh,flat_lc_eta,flat_lc_phi),axis=-1)        \n",
    "\n",
    "        \n",
    "        # 4) Build up to 7 CP IDs per LC in 'flat_lc_id':\n",
    "        max_cp = 7\n",
    "        arr_cps_padded = []\n",
    "\n",
    "        # Convert 'lc_indexes_nofilt' from Awkward to a Python list-of-lists for easy membership checks:\n",
    "        # Each index in 'lc_indexes_nofilt_list' corresponds to a calo particle,\n",
    "        # and each sublist is the LCs that belong to that CP.\n",
    "        lc_indexes_nofilt_list = [list(subarr) for subarr in lc_indexes_nofilt]\n",
    "\n",
    "        # For each layer cluster in the filtered set:\n",
    "        for [lc_idx] in flat_lc_id:  # 'flat_lc_id' is shape (N,1), so we unpack it with [lc_idx]\n",
    "            # Find all CPs that contain this LC\n",
    "            found_cps = []\n",
    "            for cp_id, lc_list in enumerate(lc_indexes_nofilt_list):\n",
    "                if lc_idx in lc_list:\n",
    "                    found_cps.append(cp_id)\n",
    "\n",
    "            # Limit to max_cp\n",
    "            found_cps = found_cps[:max_cp]\n",
    "            # Pad with -1 if fewer than max_cp\n",
    "            while len(found_cps) < max_cp:\n",
    "                found_cps.append(-1)\n",
    "\n",
    "            arr_cps_padded.append(found_cps)\n",
    "\n",
    "        # 'arr_cps_padded' is now a list of length N, each entry is a list of exactly 7 CP IDs (or -1).\n",
    "        # Convert to NumPy => shape (N,7)\n",
    "        arr_cps_np = np.array(arr_cps_padded, dtype=np.int64)  # shape (N,7)\n",
    "\n",
    "        # Optionally convert to PyTorch\n",
    "        all_cps_tensor = torch.from_numpy(arr_cps_np)\n",
    "\n",
    "\n",
    "        # Create the Data object\n",
    "        x = torch.from_numpy(flat_lc_feats).float()\n",
    "        x_lc = x\n",
    "\n",
    "\n",
    "        \n",
    "        result = np.concatenate([np.full(len(subarr), i) for i, subarr in enumerate(lc_indexes)])\n",
    "        result_list = result.tolist()                \n",
    "        x_counts = lc_x\n",
    "        y = torch.from_numpy(np.array([0 for u in range(len(flat_lc_feats))])).float()\n",
    "        return Data(\n",
    "            x=x_lc, edge_index=edge_index, y=y,assoc = result_list, cp=all_cps_tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b60d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/3 [00:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/mix/train/\"\n",
    "#vpath = \"/vols/cms/mm1221/Data/mix/val/\"\n",
    "data_train = CCV1(ipath, max_events=10, inp = 'train')\n",
    "#data_val = CCV1(vpath, max_events=10, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss_curriculum_both(embeddings, pos_indices, group_ids,\n",
    "                                     temperature=0.1, alpha=1.0, neg_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Computes an NT-Xent style loss that blends both positive and negative mining,\n",
    "    while also excluding negatives that have cosine similarity >= neg_threshold.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings so that cosine similarity is simply the dot product.\n",
    "    norm_emb = F.normalize(embeddings, p=2, dim=1)  # shape (N, D)\n",
    "    # Compute full cosine similarity matrix.\n",
    "    sim_matrix = norm_emb @ norm_emb.t()  # shape (N, N)\n",
    "    N = embeddings.size(0)\n",
    "    idx = torch.arange(N, device=embeddings.device)\n",
    "    \n",
    "    # --- Positives ---\n",
    "    # Provided positive similarity.\n",
    "    pos_sim_orig = sim_matrix[idx, pos_indices.view(-1)]\n",
    "    # Blended positive similarity.\n",
    "    blended_pos = pos_sim_orig \n",
    "    \n",
    "    # --- Negatives ---\n",
    "    # Standard negative mask: group_ids differ.\n",
    "    neg_mask = (group_ids.unsqueeze(1) != group_ids.unsqueeze(0))\n",
    "    # Exclude any \"negatives\" whose similarity >= neg_threshold:\n",
    "    # i.e., only keep those with sim < neg_threshold.\n",
    "    neg_mask = neg_mask & (sim_matrix < neg_threshold)\n",
    "\n",
    "    # Count valid negatives per anchor; track anchors with zero valid negatives.\n",
    "    valid_neg_counts = neg_mask.sum(dim=1)\n",
    "    no_valid_neg = (valid_neg_counts == 0)\n",
    "    \n",
    "    # Random negative: pick one random index among valid negatives.\n",
    "    # Multiply random values by the neg_mask to restrict to valid neg entries.\n",
    "    rand_vals = torch.rand(sim_matrix.shape, device=embeddings.device)\n",
    "    # Trick: rand_vals*neg_mask.float() -> random in [0,1] for negatives and 0 otherwise;\n",
    "    # subtracting (1 - neg_mask.float()) makes invalid entries negative and excludes them.\n",
    "    rand_vals = rand_vals * neg_mask.float() - (1 - neg_mask.float())\n",
    "    rand_neg_indices = torch.argmax(rand_vals, dim=1)\n",
    "    rand_neg_sim = sim_matrix[idx, rand_neg_indices]\n",
    "    \n",
    "    # Hard negative: among all valid negatives, choose the one with maximum similarity.\n",
    "    sim_matrix_neg = sim_matrix.masked_fill(~neg_mask, -float('inf'))\n",
    "    hard_neg_sim, _ = sim_matrix_neg.max(dim=1)\n",
    "    # If no valid negatives, set hard_neg_sim to a default (e.g. -1.0).\n",
    "    hard_neg_sim = torch.where(no_valid_neg,\n",
    "                               torch.tensor(-1.0, device=embeddings.device),\n",
    "                               hard_neg_sim)\n",
    "    \n",
    "    # Blended negative similarity.\n",
    "    blended_neg = (1 - alpha) * rand_neg_sim + alpha * hard_neg_sim\n",
    "    \n",
    "    # --- Loss Computation ---\n",
    "    numer = torch.exp(blended_pos / temperature)\n",
    "    denom = numer + torch.exp(blended_neg / temperature)\n",
    "    loss = -torch.log(numer / denom)\n",
    "    \n",
    "    # If no valid negatives, the anchor can’t push away anything—exclude from loss.\n",
    "    loss = loss.masked_fill(no_valid_neg, 0.0)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def contrastive_loss_curriculum(embeddings, pos_indices, group_ids,\n",
    "                                temperature=0.1, alpha=1.0, neg_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Curriculum loss that uses both positive and negative blending,\n",
    "    ignoring negatives whose similarity >= neg_threshold.\n",
    "    \"\"\"\n",
    "    return contrastive_loss_curriculum_both(embeddings, pos_indices, group_ids,\n",
    "                                            temperature=temperature,\n",
    "                                            alpha=alpha,\n",
    "                                            neg_threshold=neg_threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "# Training and Testing Functions\n",
    "#################################\n",
    "\n",
    "def train_new(train_loader, model, optimizer, device,k_value, alpha):\n",
    "    model.train()\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert data.assoc to tensor if needed.\n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings = model(data.x, data.x_batch, edge_index)\n",
    "        \n",
    "        # Partition batch by event.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = torch.zeros(1, device=device)\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            loss_event = contrastive_loss_curriculum(event_embeddings, event_pos_indices,\n",
    "                                                     event_group_ids, temperature=0.1, alpha=alpha)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        loss = loss_event_total / len(counts)\n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device,k_value, alpha):\n",
    "    model.eval()\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    for data in tqdm(test_loader, desc=\"Validation\"):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "        \n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings = model(data.x, data.x_batch, edge_index)\n",
    "        \n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = torch.zeros(1, device=device)\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            loss_event = contrastive_loss_curriculum(event_embeddings, event_pos_indices,\n",
    "                                                     event_group_ids, temperature=0.1, alpha=alpha)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        total_loss += loss_event_total / len(counts)\n",
    "    return total_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import DynamicEdgeConv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8, k=20):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with DynamicEdgeConv layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of DynamicEdgeConv layers.\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            k (int): Number of nearest neighbors to use in DynamicEdgeConv.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.k = k\n",
    "\n",
    "        # Input feature normalization (BatchNorm1d normalizes feature-wise across samples)\n",
    "        # self.input_norm = nn.BatchNorm1d(8)\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers using DynamicEdgeConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # For odd-numbered layers (1-based index: i+1), use k//2\n",
    "            # For even-numbered layers, use k\n",
    "            if (i + 1) % 2 == 0:\n",
    "                current_k = self.k\n",
    "            else:\n",
    "                current_k = self.k // 4\n",
    "\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(p=dropout)\n",
    "            )\n",
    "            conv = DynamicEdgeConv(mlp, k=current_k, aggr=\"max\")\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 8).\n",
    "            batch (torch.Tensor): Batch vector that assigns each node to an example in the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Normalize input features\n",
    "        # x = self.input_norm(x)\n",
    "\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply DynamicEdgeConv layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for conv in self.convs:\n",
    "            feats = conv(feats, batch) + feats\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Loading data...\")\n",
    "\n",
    "print(\"Instantiating model...\")\n",
    "# Instantiate model.\n",
    "model = Net(\n",
    "    hidden_dim=128,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=128,\n",
    "    k=48\n",
    ").to(device)\n",
    "\n",
    "\n",
    "BS = 64\n",
    "\n",
    "# Setup optimizer and scheduler.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.5)\n",
    "\n",
    "# Create DataLoaders.\n",
    "train_loader = DataLoader(data_train, batch_size=BS, shuffle=True, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Setup output directory.\n",
    "output_dir = '/vols/cms/mm1221/hgcal/Mixed/LC/NegativeMining/runs/DEC/hd128nl3cd128k24_48/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# (Optionally, you could load a pretrained model here if needed.)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "patience = 300\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "print(\"Starting full training with curriculum for hard negative mining...\")\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # For epochs 1 to 150, gradually increase alpha from 0 to 1.\n",
    "    # From epoch 151 onward, set alpha = 1 (fully hard negatives).\n",
    "    if epoch < 75:\n",
    "        alpha = 0\n",
    "        alpha2 = 0\n",
    "    elif epoch < 150:\n",
    "        alpha = (epoch - 75) / 75.0  # Linearly increase from 0 to 1\n",
    "        alpha2 = 1.0\n",
    "    else:\n",
    "        alpha = 1\n",
    "        alpha2 = 1\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Alpha: {alpha:.2f}\")\n",
    "    train_loss = train_new(train_loader, model, optimizer, device,k_value, alpha)\n",
    "    val_loss = test_new(val_loader, model, device, k_value, alpha = alpha2)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model if validation loss improves.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Save intermediate checkpoint.\n",
    "    state_dicts = {'model': model.state_dict(),\n",
    "                   'opt': optimizer.state_dict(),\n",
    "                   'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch+1}.pt'))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.item():.8f}, Validation Loss: {val_loss.item():.8f}\")\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Save training history.\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, 'continued_training_loss.csv'), index=False)\n",
    "print(f\"Saved loss curves to {os.path.join(output_dir, 'continued_training_loss.csv')}\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "print(\"Training complete. Final model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
