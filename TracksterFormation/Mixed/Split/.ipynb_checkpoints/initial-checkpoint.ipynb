{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26f4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "\n",
    "# Function to find the highest numbered branch matching base_name in an uproot file\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "# Function to remove duplicates: for each event, only keep the entry with the highest B value for each unique element in A.\n",
    "def remove_duplicates(A, B):    \n",
    "    all_masks = []\n",
    "    for event_idx, event in enumerate(A):\n",
    "        flat_A = np.array(ak.flatten(A[event_idx]))\n",
    "        flat_B = np.array(ak.flatten(B[event_idx]))\n",
    "        mask = np.zeros_like(flat_A, dtype=bool)\n",
    "        for elem in np.unique(flat_A):\n",
    "            indices = np.where(flat_A == elem)[0]\n",
    "            if len(indices) > 1:\n",
    "                max_index = indices[np.argmax(flat_B[indices])]\n",
    "                mask[max_index] = True\n",
    "            else:\n",
    "                mask[indices[0]] = True\n",
    "        unflattened_mask = ak.unflatten(mask, ak.num(A[event_idx]))\n",
    "        all_masks.append(unflattened_mask)\n",
    "    return ak.Array(all_masks)\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Dataset for layer clusters.\n",
    "    For each event it builds:\n",
    "      - x: node features, shape (N, D)\n",
    "      - groups: top-3 contributing group IDs per node (N, 3)\n",
    "      - fractions: corresponding energy fractions (N, 3)\n",
    "      - x_pe: positive edge pairs, shape (N, 2)\n",
    "      - x_ne: negative edge pairs, shape (N, 2)\n",
    "    '''\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.step_size = 500\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        \n",
    "        # These will hold the precomputed groups and fractions arrays (one per event)\n",
    "        self.precomputed_groups = []\n",
    "        self.precomputed_fractions = []\n",
    "        \n",
    "        self.fill_data(max_events)\n",
    "        self.precompute_pairings()\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading data\")\n",
    "        for fi, path in enumerate(tqdm(self.raw_file_names)):\n",
    "            if self.inp == 'train':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "                track_path = find_highest_branch(path, 'tracksters')\n",
    "            elif self.inp == 'val':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "                track_path = find_highest_branch(path, 'tracksters')\n",
    "            else:\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "                track_path = find_highest_branch(path, 'tracksters')\n",
    "            \n",
    "            crosstree = uproot.open(path)[cluster_path]\n",
    "            crosscounter = 0\n",
    "            for array in uproot.iterate(f\"{path}:{sim_path}\", \n",
    "                                         [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n",
    "                                          \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \n",
    "                                          \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\"],\n",
    "                                         step_size=self.step_size):\n",
    "                tmp_stsCP_vertices_x = array['vertices_x']\n",
    "                tmp_stsCP_vertices_y = array['vertices_y']\n",
    "                tmp_stsCP_vertices_z = array['vertices_z']\n",
    "                tmp_stsCP_vertices_energy = array['vertices_energy']\n",
    "                tmp_stsCP_vertices_time = array['vertices_time']\n",
    "                tmp_stsCP_vertices_indexes = array['vertices_indexes']\n",
    "                tmp_stsCP_barycenter_x = array['barycenter_x']\n",
    "                tmp_stsCP_barycenter_y = array['barycenter_y']\n",
    "                tmp_stsCP_barycenter_z = array['barycenter_z']\n",
    "                tmp_stsCP_vertices_multiplicity = array['vertices_multiplicity']\n",
    "                \n",
    "                self.step_size = min(self.step_size, len(tmp_stsCP_vertices_x))\n",
    "\n",
    "                tmp_all_vertices_layer_id = crosstree['cluster_layer_id'].array(entry_start=crosscounter*self.step_size,\n",
    "                                                                                 entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_noh = crosstree['cluster_number_of_hits'].array(entry_start=crosscounter*self.step_size,\n",
    "                                                                                 entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_eta = crosstree['position_eta'].array(entry_start=crosscounter*self.step_size,\n",
    "                                                                       entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_phi = crosstree['position_phi'].array(entry_start=crosscounter*self.step_size,\n",
    "                                                                       entry_stop=(crosscounter+1)*self.step_size)\n",
    "                crosscounter += 1\n",
    "\n",
    "                layer_id_list = []\n",
    "                noh_list = []\n",
    "                eta_list = []\n",
    "                phi_list = []\n",
    "                for evt_row in range(len(tmp_all_vertices_noh)):\n",
    "                    layer_id_list_one_event = []\n",
    "                    noh_list_one_event = []\n",
    "                    eta_list_one_event = []\n",
    "                    phi_list_one_event = []\n",
    "                    for particle in range(len(tmp_stsCP_vertices_indexes[evt_row])):\n",
    "                        tmp_stsCP_vertices_layer_id_one_particle = tmp_all_vertices_layer_id[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_noh_one_particle = tmp_all_vertices_noh[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_eta_one_particle = tmp_all_vertices_eta[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_phi_one_particle = tmp_all_vertices_phi[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        layer_id_list_one_event.append(tmp_stsCP_vertices_layer_id_one_particle)\n",
    "                        noh_list_one_event.append(tmp_stsCP_vertices_noh_one_particle)\n",
    "                        eta_list_one_event.append(tmp_stsCP_vertices_eta_one_particle)\n",
    "                        phi_list_one_event.append(tmp_stsCP_vertices_phi_one_particle)\n",
    "                    layer_id_list.append(layer_id_list_one_event)\n",
    "                    noh_list.append(noh_list_one_event)\n",
    "                    eta_list.append(eta_list_one_event)\n",
    "                    phi_list.append(phi_list_one_event)\n",
    "                tmp_stsCP_vertices_layer_id = ak.Array(layer_id_list)\n",
    "                tmp_stsCP_vertices_noh = ak.Array(noh_list)\n",
    "                tmp_stsCP_vertices_eta = ak.Array(eta_list)\n",
    "                tmp_stsCP_vertices_phi = ak.Array(phi_list)\n",
    "                \n",
    "                skim_mask_noh = tmp_stsCP_vertices_noh > 1.0\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_noh]\n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in tmp_stsCP_vertices_x:\n",
    "                    skim_mask.append(len(e) >= 2)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask]\n",
    "\n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_indexes_unfilt = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_vertices_multiplicity_unfilt = tmp_stsCP_vertices_multiplicity\n",
    "                else:\n",
    "                    self.stsCP_vertices_indexes_unfilt = ak.concatenate(\n",
    "                        (self.stsCP_vertices_indexes_unfilt, tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_vertices_multiplicity_unfilt = ak.concatenate(\n",
    "                        (self.stsCP_vertices_multiplicity_unfilt, tmp_stsCP_vertices_multiplicity))\n",
    "                \n",
    "                energyPercent = 1 / tmp_stsCP_vertices_multiplicity\n",
    "                skim_mask_energyPercent = remove_duplicates(tmp_stsCP_vertices_indexes, energyPercent)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_energyPercent]\n",
    "                \n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_x = tmp_stsCP_vertices_x\n",
    "                    self.stsCP_vertices_y = tmp_stsCP_vertices_y\n",
    "                    self.stsCP_vertices_z = tmp_stsCP_vertices_z\n",
    "                    self.stsCP_vertices_energy = tmp_stsCP_vertices_energy\n",
    "                    self.stsCP_vertices_time = tmp_stsCP_vertices_time\n",
    "                    self.stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id\n",
    "                    self.stsCP_vertices_noh = tmp_stsCP_vertices_noh\n",
    "                    self.stsCP_vertices_eta = tmp_stsCP_vertices_eta\n",
    "                    self.stsCP_vertices_phi = tmp_stsCP_vertices_phi\n",
    "                    self.stsCP_vertices_indexes = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_barycenter_x = tmp_stsCP_barycenter_x\n",
    "                    self.stsCP_barycenter_y = tmp_stsCP_barycenter_y\n",
    "                    self.stsCP_barycenter_z = tmp_stsCP_barycenter_z\n",
    "                    self.stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity\n",
    "                    self.stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes_filt\n",
    "                else:\n",
    "                    self.stsCP_vertices_x = ak.concatenate((self.stsCP_vertices_x, tmp_stsCP_vertices_x))\n",
    "                    self.stsCP_vertices_y = ak.concatenate((self.stsCP_vertices_y, tmp_stsCP_vertices_y))\n",
    "                    self.stsCP_vertices_z = ak.concatenate((self.stsCP_vertices_z, tmp_stsCP_vertices_z))\n",
    "                    self.stsCP_vertices_energy = ak.concatenate((self.stsCP_vertices_energy, tmp_stsCP_vertices_energy))\n",
    "                    self.stsCP_vertices_time = ak.concatenate((self.stsCP_vertices_time, tmp_stsCP_vertices_time))\n",
    "                    self.stsCP_vertices_layer_id = ak.concatenate((self.stsCP_vertices_layer_id, tmp_stsCP_vertices_layer_id))\n",
    "                    self.stsCP_vertices_noh = ak.concatenate((self.stsCP_vertices_noh, tmp_stsCP_vertices_noh))\n",
    "                    self.stsCP_vertices_eta = ak.concatenate((self.stsCP_vertices_eta, tmp_stsCP_vertices_eta))\n",
    "                    self.stsCP_vertices_phi = ak.concatenate((self.stsCP_vertices_phi, tmp_stsCP_vertices_phi))\n",
    "                    self.stsCP_vertices_indexes = ak.concatenate((self.stsCP_vertices_indexes, tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_barycenter_x = ak.concatenate((self.stsCP_barycenter_x, tmp_stsCP_barycenter_x))\n",
    "                    self.stsCP_barycenter_y = ak.concatenate((self.stsCP_barycenter_y, tmp_stsCP_barycenter_y))\n",
    "                    self.stsCP_barycenter_z = ak.concatenate((self.stsCP_barycenter_z, tmp_stsCP_barycenter_z))\n",
    "                    self.stsCP_vertices_multiplicity = ak.concatenate((self.stsCP_vertices_multiplicity, tmp_stsCP_vertices_multiplicity))\n",
    "                    self.stsCP_vertices_indexes_filt = ak.concatenate((self.stsCP_vertices_indexes_filt, tmp_stsCP_vertices_indexes_filt))\n",
    "                \n",
    "                counter += 1\n",
    "                if len(self.stsCP_vertices_x) > max_events:\n",
    "                    print(f\"Reached {max_events}!\")\n",
    "                    break\n",
    "            if len(self.stsCP_vertices_x) > max_events:\n",
    "                break\n",
    "\n",
    "    def precompute_pairings(self):\n",
    "        \"\"\"\n",
    "        Precompute the groups and fractions arrays for each event and store them as attributes.\n",
    "        (Positive and negative edges will be computed dynamically in get().)\n",
    "        \"\"\"\n",
    "        n_events = len(self.stsCP_vertices_x)\n",
    "        for idx in range(n_events):\n",
    "            unfilt_cp_array = self.stsCP_vertices_indexes_unfilt[idx]\n",
    "            unfilt_mult_array = self.stsCP_vertices_multiplicity_unfilt[idx]\n",
    "            cluster_contributors = {}\n",
    "            for cp_id in range(len(unfilt_cp_array)):\n",
    "                for local_lc_id, cluster_id in enumerate(unfilt_cp_array[cp_id]):\n",
    "                    frac = 1.0 / unfilt_mult_array[cp_id][local_lc_id]\n",
    "                    cluster_contributors.setdefault(cluster_id, []).append((cp_id, frac))\n",
    "\n",
    "            final_cp_array = self.stsCP_vertices_indexes_filt[idx]\n",
    "            flattened_cluster_ids = ak.flatten(final_cp_array)\n",
    "            total_lc = len(flattened_cluster_ids)\n",
    "            groups_np = np.zeros((total_lc, 3), dtype=np.int64)\n",
    "            fractions_np = np.zeros((total_lc, 3), dtype=np.float32)\n",
    "            for global_lc, cluster_id in enumerate(flattened_cluster_ids):\n",
    "                contribs = cluster_contributors.get(cluster_id, [])\n",
    "                contribs_sorted = sorted(contribs, key=lambda x: x[1], reverse=True)\n",
    "                if len(contribs_sorted) == 0:\n",
    "                    top3 = [(0, 0.0)] * 3\n",
    "                else:\n",
    "                    top3 = contribs_sorted[:3]\n",
    "                    while len(top3) < 3:\n",
    "                        top3.append(top3[-1])\n",
    "                for i in range(3):\n",
    "                    groups_np[global_lc, i] = top3[i][0]\n",
    "                    fractions_np[global_lc, i] = top3[i][1]\n",
    "            self.precomputed_groups.append(groups_np)\n",
    "            self.precomputed_fractions.append(fractions_np)\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            'Dataset not found. Please download it from {} and move all '\n",
    "            '*.z files to {}'.format(self.url, self.raw_dir))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.stsCP_vertices_x)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def get(self, idx):\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        from torch_geometric.data import Data\n",
    "        import awkward as ak\n",
    "        import random\n",
    "\n",
    "        # 1) Flatten node features for the event\n",
    "        lc_x = self.stsCP_vertices_x[idx]\n",
    "        lc_y = self.stsCP_vertices_y[idx]\n",
    "        lc_z = self.stsCP_vertices_z[idx]\n",
    "        lc_e = self.stsCP_vertices_energy[idx]\n",
    "        lc_layer_id = self.stsCP_vertices_layer_id[idx]\n",
    "        lc_noh = self.stsCP_vertices_noh[idx]\n",
    "        lc_eta = self.stsCP_vertices_eta[idx]\n",
    "        lc_phi = self.stsCP_vertices_phi[idx]\n",
    "\n",
    "        flat_lc_x = np.expand_dims(np.array(ak.flatten(lc_x)), axis=1)\n",
    "        flat_lc_y = np.expand_dims(np.array(ak.flatten(lc_y)), axis=1)\n",
    "        flat_lc_z = np.expand_dims(np.array(ak.flatten(lc_z)), axis=1)\n",
    "        flat_lc_e = np.expand_dims(np.array(ak.flatten(lc_e)), axis=1)\n",
    "        flat_lc_layer_id = np.expand_dims(np.array(ak.flatten(lc_layer_id)), axis=1)\n",
    "        flat_lc_noh = np.expand_dims(np.array(ak.flatten(lc_noh)), axis=1)\n",
    "        flat_lc_eta = np.expand_dims(np.array(ak.flatten(lc_eta)), axis=1)\n",
    "        flat_lc_phi = np.expand_dims(np.array(ak.flatten(lc_phi)), axis=1)\n",
    "        flat_lc_feats = np.concatenate(\n",
    "            (flat_lc_x, flat_lc_y, flat_lc_z, flat_lc_e,\n",
    "             flat_lc_layer_id, flat_lc_noh, flat_lc_eta, flat_lc_phi),\n",
    "            axis=-1\n",
    "        )\n",
    "        total_lc = flat_lc_feats.shape[0]\n",
    "        x = torch.from_numpy(flat_lc_feats).float()\n",
    "\n",
    "        # 2) Retrieve precomputed groups and fractions\n",
    "        groups_np = self.precomputed_groups[idx]\n",
    "        fractions_np = self.precomputed_fractions[idx]\n",
    "\n",
    "        # 3) Dynamically compute positive and negative pairs based on groups and fractions\n",
    "        pos_pairs = np.empty(total_lc, dtype=np.int64)\n",
    "        neg_pairs = np.empty(total_lc, dtype=np.int64)\n",
    "        for i in range(total_lc):\n",
    "            pos_candidate = None\n",
    "            first_group = groups_np[i][0]\n",
    "            first_frac = fractions_np[i][0]\n",
    "            if first_frac == 1:\n",
    "                candidates = [j for j in range(total_lc) if j != i and groups_np[j][0] == first_group]\n",
    "                if candidates:\n",
    "                    candidates_exact = [j for j in candidates if fractions_np[j][0] == 1]\n",
    "                    if candidates_exact:\n",
    "                        pos_candidate = random.choice(candidates_exact)\n",
    "                    else:\n",
    "                        pos_candidate = random.choice(candidates)\n",
    "            else:\n",
    "                candidates = [j for j in range(total_lc) if j != i and set(groups_np[j][:2]) == set(groups_np[i][:2])]\n",
    "                if candidates:\n",
    "                    mean2_i = np.mean(fractions_np[i][:2])\n",
    "                    pos_candidate = min(candidates, key=lambda j: abs(np.mean(fractions_np[j][:2]) - mean2_i))\n",
    "                else:\n",
    "                    candidates = [j for j in range(total_lc) if j != i and groups_np[j][0] == first_group]\n",
    "                    if candidates:\n",
    "                        pos_candidate = random.choice(candidates)\n",
    "            if pos_candidate is None:\n",
    "                remaining = [j for j in range(total_lc) if j != i]\n",
    "                pos_candidate = random.choice(remaining) if remaining else i\n",
    "            pos_pairs[i] = pos_candidate\n",
    "\n",
    "            orig_set = set(groups_np[i])\n",
    "            neg_candidate = None\n",
    "            candidates = [j for j in range(total_lc) if j != i and \n",
    "                          (groups_np[j][0] not in orig_set or fractions_np[j][0] == 0)]\n",
    "            if candidates:\n",
    "                neg_candidate = random.choice(candidates)\n",
    "            else:\n",
    "                candidates = [j for j in range(total_lc) if j != i and groups_np[j][0] != groups_np[i][0]]\n",
    "                if candidates:\n",
    "                    neg_candidate = random.choice(candidates)\n",
    "                else:\n",
    "                    neg_candidate = (i + 1) % total_lc\n",
    "            neg_pairs[i] = neg_candidate\n",
    "\n",
    "        pos_edge = torch.from_numpy(np.column_stack((np.arange(total_lc), pos_pairs))).long()\n",
    "        neg_edge = torch.from_numpy(np.column_stack((np.arange(total_lc), neg_pairs))).long()\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            groups=torch.from_numpy(groups_np),\n",
    "            fractions=torch.from_numpy(fractions_np),\n",
    "            x_pe=pos_edge,\n",
    "            x_ne=neg_edge\n",
    "        )\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91249a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10!\n",
      "Precomputing groups for event 0\n",
      "Precomputing groups for event 1\n",
      "Precomputing groups for event 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing groups for event 3\n",
      "Precomputing groups for event 4\n",
      "Precomputing groups for event 5\n",
      "Precomputing groups for event 6\n",
      "Precomputing groups for event 7\n",
      "Precomputing groups for event 8\n",
      "Precomputing groups for event 9\n",
      "Precomputing groups for event 10\n",
      "Precomputing groups for event 11\n",
      "Precomputing groups for event 12\n",
      "Precomputing groups for event 13\n",
      "Precomputing groups for event 14\n",
      "Precomputing groups for event 15\n",
      "Precomputing groups for event 16\n",
      "Precomputing groups for event 17\n",
      "Precomputing groups for event 18\n",
      "Precomputing groups for event 19\n",
      "Precomputing groups for event 20\n",
      "Precomputing groups for event 21\n",
      "Precomputing groups for event 22\n",
      "Precomputing groups for event 23\n",
      "Precomputing groups for event 24\n",
      "Precomputing groups for event 25\n",
      "Precomputing groups for event 26\n",
      "Precomputing groups for event 27\n",
      "Precomputing groups for event 28\n",
      "Precomputing groups for event 29\n",
      "Precomputing groups for event 30\n",
      "Precomputing groups for event 31\n",
      "Precomputing groups for event 32\n",
      "Precomputing groups for event 33\n",
      "Precomputing groups for event 34\n",
      "Precomputing groups for event 35\n",
      "Precomputing groups for event 36\n",
      "Precomputing groups for event 37\n",
      "Precomputing groups for event 38\n",
      "Precomputing groups for event 39\n",
      "Precomputing groups for event 40\n",
      "Precomputing groups for event 41\n",
      "Precomputing groups for event 42\n",
      "Precomputing groups for event 43\n",
      "Precomputing groups for event 44\n",
      "Precomputing groups for event 45\n",
      "Precomputing groups for event 46\n",
      "Precomputing groups for event 47\n",
      "Precomputing groups for event 48\n",
      "Precomputing groups for event 49\n",
      "Precomputing groups for event 50\n",
      "Precomputing groups for event 51\n",
      "Precomputing groups for event 52\n",
      "Precomputing groups for event 53\n",
      "Precomputing groups for event 54\n",
      "Precomputing groups for event 55\n",
      "Precomputing groups for event 56\n",
      "Precomputing groups for event 57\n",
      "Precomputing groups for event 58\n",
      "Precomputing groups for event 59\n",
      "Precomputing groups for event 60\n",
      "Precomputing groups for event 61\n",
      "Precomputing groups for event 62\n",
      "Precomputing groups for event 63\n",
      "Precomputing groups for event 64\n",
      "Precomputing groups for event 65\n",
      "Precomputing groups for event 66\n",
      "Precomputing groups for event 67\n",
      "Precomputing groups for event 68\n",
      "Precomputing groups for event 69\n",
      "Precomputing groups for event 70\n",
      "Precomputing groups for event 71\n",
      "Precomputing groups for event 72\n",
      "Precomputing groups for event 73\n",
      "Precomputing groups for event 74\n",
      "Precomputing groups for event 75\n",
      "Precomputing groups for event 76\n",
      "Precomputing groups for event 77\n",
      "Precomputing groups for event 78\n",
      "Precomputing groups for event 79\n",
      "Precomputing groups for event 80\n",
      "Precomputing groups for event 81\n",
      "Precomputing groups for event 82\n",
      "Precomputing groups for event 83\n",
      "Precomputing groups for event 84\n",
      "Precomputing groups for event 85\n",
      "Precomputing groups for event 86\n",
      "Precomputing groups for event 87\n",
      "Precomputing groups for event 88\n",
      "Precomputing groups for event 89\n",
      "Precomputing groups for event 90\n",
      "Precomputing groups for event 91\n",
      "Precomputing groups for event 92\n",
      "Precomputing groups for event 93\n",
      "Precomputing groups for event 94\n",
      "Precomputing groups for event 95\n",
      "Precomputing groups for event 96\n",
      "Precomputing groups for event 97\n",
      "Precomputing groups for event 98\n",
      "Precomputing groups for event 99\n",
      "Precomputing groups for event 100\n",
      "Precomputing groups for event 101\n",
      "Precomputing groups for event 102\n",
      "Precomputing groups for event 103\n",
      "Precomputing groups for event 104\n",
      "Precomputing groups for event 105\n",
      "Precomputing groups for event 106\n",
      "Precomputing groups for event 107\n",
      "Precomputing groups for event 108\n",
      "Precomputing groups for event 109\n",
      "Precomputing groups for event 110\n",
      "Precomputing groups for event 111\n",
      "Precomputing groups for event 112\n",
      "Precomputing groups for event 113\n",
      "Precomputing groups for event 114\n",
      "Precomputing groups for event 115\n",
      "Precomputing groups for event 116\n",
      "Precomputing groups for event 117\n",
      "Precomputing groups for event 118\n",
      "Precomputing groups for event 119\n",
      "Precomputing groups for event 120\n",
      "Precomputing groups for event 121\n",
      "Precomputing groups for event 122\n",
      "Precomputing groups for event 123\n",
      "Precomputing groups for event 124\n",
      "Precomputing groups for event 125\n",
      "Precomputing groups for event 126\n",
      "Precomputing groups for event 127\n",
      "Precomputing groups for event 128\n",
      "Precomputing groups for event 129\n",
      "Precomputing groups for event 130\n",
      "Precomputing groups for event 131\n",
      "Precomputing groups for event 132\n",
      "Precomputing groups for event 133\n",
      "Precomputing groups for event 134\n",
      "Precomputing groups for event 135\n",
      "Precomputing groups for event 136\n",
      "Precomputing groups for event 137\n",
      "Precomputing groups for event 138\n",
      "Precomputing groups for event 139\n",
      "Precomputing groups for event 140\n",
      "Precomputing groups for event 141\n",
      "Precomputing groups for event 142\n",
      "Precomputing groups for event 143\n",
      "Precomputing groups for event 144\n",
      "Precomputing groups for event 145\n",
      "Precomputing groups for event 146\n",
      "Precomputing groups for event 147\n",
      "Precomputing groups for event 148\n",
      "Precomputing groups for event 149\n",
      "Precomputing groups for event 150\n",
      "Precomputing groups for event 151\n",
      "Precomputing groups for event 152\n",
      "Precomputing groups for event 153\n",
      "Precomputing groups for event 154\n",
      "Precomputing groups for event 155\n",
      "Precomputing groups for event 156\n",
      "Precomputing groups for event 157\n",
      "Precomputing groups for event 158\n",
      "Precomputing groups for event 159\n",
      "Precomputing groups for event 160\n",
      "Precomputing groups for event 161\n",
      "Precomputing groups for event 162\n",
      "Precomputing groups for event 163\n",
      "Precomputing groups for event 164\n",
      "Precomputing groups for event 165\n",
      "Precomputing groups for event 166\n",
      "Precomputing groups for event 167\n",
      "Precomputing groups for event 168\n",
      "Precomputing groups for event 169\n",
      "Precomputing groups for event 170\n",
      "Precomputing groups for event 171\n",
      "Precomputing groups for event 172\n",
      "Precomputing groups for event 173\n",
      "Precomputing groups for event 174\n",
      "Precomputing groups for event 175\n",
      "Precomputing groups for event 176\n",
      "Precomputing groups for event 177\n",
      "Precomputing groups for event 178\n",
      "Precomputing groups for event 179\n",
      "Precomputing groups for event 180\n",
      "Precomputing groups for event 181\n",
      "Precomputing groups for event 182\n",
      "Precomputing groups for event 183\n",
      "Precomputing groups for event 184\n",
      "Precomputing groups for event 185\n",
      "Precomputing groups for event 186\n",
      "Precomputing groups for event 187\n",
      "Precomputing groups for event 188\n",
      "Precomputing groups for event 189\n",
      "Precomputing groups for event 190\n",
      "Precomputing groups for event 191\n",
      "Precomputing groups for event 192\n",
      "Precomputing groups for event 193\n",
      "Precomputing groups for event 194\n",
      "Precomputing groups for event 195\n",
      "Precomputing groups for event 196\n",
      "Precomputing groups for event 197\n",
      "Precomputing groups for event 198\n",
      "Precomputing groups for event 199\n",
      "Precomputing groups for event 200\n",
      "Precomputing groups for event 201\n",
      "Precomputing groups for event 202\n",
      "Precomputing groups for event 203\n",
      "Precomputing groups for event 204\n",
      "Precomputing groups for event 205\n",
      "Precomputing groups for event 206\n",
      "Precomputing groups for event 207\n",
      "Precomputing groups for event 208\n",
      "Precomputing groups for event 209\n",
      "Precomputing groups for event 210\n",
      "Precomputing groups for event 211\n",
      "Precomputing groups for event 212\n",
      "Precomputing groups for event 213\n",
      "Precomputing groups for event 214\n",
      "Precomputing groups for event 215\n",
      "Precomputing groups for event 216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_237867/3827912392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mipath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/vols/cms/mm1221/Data/mix/train/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/vols/cms/mm1221/Data/mix/val/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_237867/898537760.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, max_events, inp)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompute_pairings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_237867/898537760.py\u001b[0m in \u001b[0;36mprecompute_pairings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcp_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfilt_cp_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlocal_lc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munfilt_cp_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                     \u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munfilt_mult_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlocal_lc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                     \u001b[0mcluster_contributors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/mix/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/mix/val/\"\n",
    "data_train = CCV1(ipath, max_events=10, inp = 'train')\n",
    "data_val = CCV1(vpath, max_events=10, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc312a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data_train[1].groups[0])\n",
    "print(data_train[1].fractions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f58289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[1].groups[96])\n",
    "print(data_train[1].fractions[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c504d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 338],\n",
      "        [  1, 203],\n",
      "        [  2, 324],\n",
      "        [  3, 211],\n",
      "        [  4, 147],\n",
      "        [  5, 104],\n",
      "        [  6,  84],\n",
      "        [  7, 325],\n",
      "        [  8, 162],\n",
      "        [  9, 322],\n",
      "        [ 10, 297],\n",
      "        [ 11, 190],\n",
      "        [ 12, 178],\n",
      "        [ 13, 195],\n",
      "        [ 14, 341],\n",
      "        [ 15, 156],\n",
      "        [ 16, 134],\n",
      "        [ 17, 103],\n",
      "        [ 18, 220],\n",
      "        [ 19, 218],\n",
      "        [ 20, 172],\n",
      "        [ 21, 111],\n",
      "        [ 22, 193],\n",
      "        [ 23, 118],\n",
      "        [ 24, 313],\n",
      "        [ 25, 223],\n",
      "        [ 26,  97],\n",
      "        [ 27, 204],\n",
      "        [ 28, 146],\n",
      "        [ 29, 237],\n",
      "        [ 30, 217],\n",
      "        [ 31, 265],\n",
      "        [ 32, 284],\n",
      "        [ 33, 361],\n",
      "        [ 34, 186],\n",
      "        [ 35, 116],\n",
      "        [ 36, 342],\n",
      "        [ 37, 298],\n",
      "        [ 38, 347],\n",
      "        [ 39, 297],\n",
      "        [ 40, 244],\n",
      "        [ 41, 291],\n",
      "        [ 42, 311],\n",
      "        [ 43, 112],\n",
      "        [ 44, 102],\n",
      "        [ 45, 317],\n",
      "        [ 46, 337],\n",
      "        [ 47, 335],\n",
      "        [ 48, 273],\n",
      "        [ 49, 279],\n",
      "        [ 50, 236],\n",
      "        [ 51, 343],\n",
      "        [ 52, 130],\n",
      "        [ 53, 203],\n",
      "        [ 54, 231],\n",
      "        [ 55, 162],\n",
      "        [ 56, 279],\n",
      "        [ 57, 232],\n",
      "        [ 58, 299],\n",
      "        [ 59, 341],\n",
      "        [ 60, 139],\n",
      "        [ 61, 220],\n",
      "        [ 62, 165],\n",
      "        [ 63, 207],\n",
      "        [ 64, 228],\n",
      "        [ 65, 156],\n",
      "        [ 66, 330],\n",
      "        [ 67, 322],\n",
      "        [ 68, 167],\n",
      "        [ 69, 293],\n",
      "        [ 70, 196],\n",
      "        [ 71, 201],\n",
      "        [ 72, 252],\n",
      "        [ 73, 296],\n",
      "        [ 74, 218],\n",
      "        [ 75, 296],\n",
      "        [ 76, 208],\n",
      "        [ 77, 222],\n",
      "        [ 78, 152],\n",
      "        [ 79, 160],\n",
      "        [ 80, 203],\n",
      "        [ 81, 285],\n",
      "        [ 82,  36],\n",
      "        [ 83, 358],\n",
      "        [ 84, 343],\n",
      "        [ 85,  47],\n",
      "        [ 86, 310],\n",
      "        [ 87, 315],\n",
      "        [ 88, 205],\n",
      "        [ 89, 299],\n",
      "        [ 90,  80],\n",
      "        [ 91, 234],\n",
      "        [ 92, 226],\n",
      "        [ 93, 210],\n",
      "        [ 94, 174],\n",
      "        [ 95, 200],\n",
      "        [ 96,  19],\n",
      "        [ 97, 350],\n",
      "        [ 98,  47],\n",
      "        [ 99, 265],\n",
      "        [100, 193],\n",
      "        [101, 280],\n",
      "        [102,  40],\n",
      "        [103, 241],\n",
      "        [104, 293],\n",
      "        [105,  66],\n",
      "        [106, 165],\n",
      "        [107, 319],\n",
      "        [108, 205],\n",
      "        [109, 342],\n",
      "        [110, 184],\n",
      "        [111, 211],\n",
      "        [112, 172],\n",
      "        [113, 274],\n",
      "        [114, 236],\n",
      "        [115, 224],\n",
      "        [116, 238],\n",
      "        [117, 249],\n",
      "        [118, 155],\n",
      "        [119, 280],\n",
      "        [120, 255],\n",
      "        [121, 190],\n",
      "        [122, 302],\n",
      "        [123, 188],\n",
      "        [124, 161],\n",
      "        [125, 164],\n",
      "        [126, 184],\n",
      "        [127, 293],\n",
      "        [128,  56],\n",
      "        [129, 342],\n",
      "        [130, 340],\n",
      "        [131, 321],\n",
      "        [132, 361],\n",
      "        [133,  69],\n",
      "        [134, 347],\n",
      "        [135, 326],\n",
      "        [136, 337],\n",
      "        [137, 168],\n",
      "        [138,   4],\n",
      "        [139, 289],\n",
      "        [140, 242],\n",
      "        [141, 332],\n",
      "        [142,  14],\n",
      "        [143, 205],\n",
      "        [144, 160],\n",
      "        [145,  20],\n",
      "        [146,  15],\n",
      "        [147,  55],\n",
      "        [148,  10],\n",
      "        [149, 285],\n",
      "        [150, 296],\n",
      "        [151, 278],\n",
      "        [152, 181],\n",
      "        [153, 191],\n",
      "        [154, 324],\n",
      "        [155,  87],\n",
      "        [156,  69],\n",
      "        [157,   4],\n",
      "        [158,  59],\n",
      "        [159, 248],\n",
      "        [160, 226],\n",
      "        [161, 347],\n",
      "        [162,  65],\n",
      "        [163, 128],\n",
      "        [164, 124],\n",
      "        [165,  36],\n",
      "        [166,  44],\n",
      "        [167, 144],\n",
      "        [168,  81],\n",
      "        [169, 126],\n",
      "        [170, 328],\n",
      "        [171, 325],\n",
      "        [172,  56],\n",
      "        [173, 243],\n",
      "        [174,  59],\n",
      "        [175, 257],\n",
      "        [176, 245],\n",
      "        [177, 320],\n",
      "        [178, 148],\n",
      "        [179, 305],\n",
      "        [180, 336],\n",
      "        [181, 151],\n",
      "        [182, 281],\n",
      "        [183,   6],\n",
      "        [184, 303],\n",
      "        [185, 213],\n",
      "        [186,  14],\n",
      "        [187,  17],\n",
      "        [188, 282],\n",
      "        [189, 154],\n",
      "        [190,  57],\n",
      "        [191,  84],\n",
      "        [192,  26],\n",
      "        [193, 349],\n",
      "        [194, 207],\n",
      "        [195,  80],\n",
      "        [196, 222],\n",
      "        [197, 148],\n",
      "        [198, 355],\n",
      "        [199, 289],\n",
      "        [200, 215],\n",
      "        [201, 294],\n",
      "        [202,  11],\n",
      "        [203, 318],\n",
      "        [204, 341],\n",
      "        [205, 170],\n",
      "        [206, 283],\n",
      "        [207,  57],\n",
      "        [208, 172],\n",
      "        [209, 188],\n",
      "        [210, 112],\n",
      "        [211,  50],\n",
      "        [212, 322],\n",
      "        [213, 364],\n",
      "        [214, 289],\n",
      "        [215, 105],\n",
      "        [216, 164],\n",
      "        [217, 139],\n",
      "        [218, 280],\n",
      "        [219, 352],\n",
      "        [220, 181],\n",
      "        [221,  89],\n",
      "        [222, 360],\n",
      "        [223, 122],\n",
      "        [224, 173],\n",
      "        [225, 343],\n",
      "        [226,  33],\n",
      "        [227, 198],\n",
      "        [228, 331],\n",
      "        [229, 306],\n",
      "        [230,   5],\n",
      "        [231, 148],\n",
      "        [232,  65],\n",
      "        [233, 183],\n",
      "        [234, 317],\n",
      "        [235,   9],\n",
      "        [236,   0],\n",
      "        [237, 360],\n",
      "        [238, 302],\n",
      "        [239, 362],\n",
      "        [240, 104],\n",
      "        [241,  93],\n",
      "        [242,  61],\n",
      "        [243, 295],\n",
      "        [244, 282],\n",
      "        [245, 339],\n",
      "        [246,  45],\n",
      "        [247,  49],\n",
      "        [248, 295],\n",
      "        [249,  86],\n",
      "        [250, 350],\n",
      "        [251,  11],\n",
      "        [252, 185],\n",
      "        [253,  57],\n",
      "        [254, 192],\n",
      "        [255, 352],\n",
      "        [256, 294],\n",
      "        [257, 302],\n",
      "        [258,  61],\n",
      "        [259,  49],\n",
      "        [260, 162],\n",
      "        [261,  35],\n",
      "        [262, 348],\n",
      "        [263,  50],\n",
      "        [264, 277],\n",
      "        [265, 165],\n",
      "        [266, 160],\n",
      "        [267, 181],\n",
      "        [268, 318],\n",
      "        [269, 357],\n",
      "        [270, 201],\n",
      "        [271, 174],\n",
      "        [272, 354],\n",
      "        [273, 332],\n",
      "        [274,  85],\n",
      "        [275, 174],\n",
      "        [276,   8],\n",
      "        [277,  56],\n",
      "        [278,  58],\n",
      "        [279,  34],\n",
      "        [280, 232],\n",
      "        [281,  32],\n",
      "        [282, 192],\n",
      "        [283, 117],\n",
      "        [284, 270],\n",
      "        [285,  20],\n",
      "        [286, 100],\n",
      "        [287,  17],\n",
      "        [288, 132],\n",
      "        [289, 170],\n",
      "        [290, 226],\n",
      "        [291, 140],\n",
      "        [292, 176],\n",
      "        [293,  12],\n",
      "        [294, 192],\n",
      "        [295, 126],\n",
      "        [296,  67],\n",
      "        [297, 230],\n",
      "        [298,   7],\n",
      "        [299, 153],\n",
      "        [300, 266],\n",
      "        [301,  60],\n",
      "        [302,  36],\n",
      "        [303, 258],\n",
      "        [304,  41],\n",
      "        [305,  83],\n",
      "        [306, 191],\n",
      "        [307, 230],\n",
      "        [308, 106],\n",
      "        [309, 265],\n",
      "        [310, 131],\n",
      "        [311,   3],\n",
      "        [312,  18],\n",
      "        [313,  49],\n",
      "        [314, 239],\n",
      "        [315, 259],\n",
      "        [316, 168],\n",
      "        [317,  71],\n",
      "        [318,  43],\n",
      "        [319, 216],\n",
      "        [320, 262],\n",
      "        [321, 223],\n",
      "        [322, 153],\n",
      "        [323,  19],\n",
      "        [324, 191],\n",
      "        [325, 264],\n",
      "        [326,  64],\n",
      "        [327, 108],\n",
      "        [328,  54],\n",
      "        [329, 265],\n",
      "        [330, 128],\n",
      "        [331, 173],\n",
      "        [332,  31],\n",
      "        [333, 200],\n",
      "        [334, 205],\n",
      "        [335,   8],\n",
      "        [336, 205],\n",
      "        [337, 160],\n",
      "        [338, 105],\n",
      "        [339,  60],\n",
      "        [340, 204],\n",
      "        [341,  26],\n",
      "        [342, 224],\n",
      "        [343,  14],\n",
      "        [344,  58],\n",
      "        [345,  39],\n",
      "        [346,  19],\n",
      "        [347, 253],\n",
      "        [348, 106],\n",
      "        [349,  37],\n",
      "        [350,  19],\n",
      "        [351,  62],\n",
      "        [352, 269],\n",
      "        [353,  39],\n",
      "        [354,  53],\n",
      "        [355,  55],\n",
      "        [356, 208],\n",
      "        [357,  66],\n",
      "        [358, 190],\n",
      "        [359, 236],\n",
      "        [360, 121],\n",
      "        [361, 139],\n",
      "        [362, 121],\n",
      "        [363, 167],\n",
      "        [364, 187],\n",
      "        [365, 160],\n",
      "        [366, 137],\n",
      "        [367,  56],\n",
      "        [368, 158]])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[3].x_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56585e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import DynamicEdgeConv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8, k=20):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with DynamicEdgeConv layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of DynamicEdgeConv layers.\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            k (int): Number of nearest neighbors to use in DynamicEdgeConv.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.k = k\n",
    "\n",
    "        # Input feature normalization (BatchNorm1d normalizes feature-wise across samples)\n",
    "        # self.input_norm = nn.BatchNorm1d(8)\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers using DynamicEdgeConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # For odd-numbered layers (1-based index: i+1), use k//2\n",
    "            # For even-numbered layers, use k\n",
    "            if (i + 1) % 2 == 0:\n",
    "                current_k = self.k\n",
    "            else:\n",
    "                current_k = self.k\n",
    "\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(p=dropout)\n",
    "            )\n",
    "            conv = DynamicEdgeConv(mlp, k=current_k, aggr=\"max\")\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 8).\n",
    "            batch (torch.Tensor): Batch vector that assigns each node to an example in the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Normalize input features\n",
    "        # x = self.input_norm(x)\n",
    "\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply DynamicEdgeConv layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for conv in self.convs:\n",
    "            feats = conv(feats, batch) + feats\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201110e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def contrastive_loss_edges(embeddings, pos_edge, neg_edge, temperature=0.1, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes contrastive loss using precomputed positive and negative edges.\n",
    "    \n",
    "    For each node i, let pos = pos_edge[i, 1] and neg = neg_edge[i, 1].\n",
    "    The loss is computed as:\n",
    "    \n",
    "         L(i) = - log ( exp(sim(i, pos)/temperature) / \n",
    "                        (exp(sim(i, pos)/temperature) + exp(sim(i, neg)/temperature)) )\n",
    "              = - log(sigmoid((sim(i, pos)-sim(i, neg))/temperature))\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape (N, D) containing node embeddings.\n",
    "        pos_edge:     Tensor of shape (N, 2) where each row is [node, positive_node].\n",
    "        neg_edge:     Tensor of shape (N, 2) where each row is [node, negative_node].\n",
    "        temperature:  Temperature scaling factor.\n",
    "        eps:          Small constant for numerical stability.\n",
    "        \n",
    "    Returns:\n",
    "        Scalar tensor representing the average loss.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings to unit length.\n",
    "    norm_emb = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity for positive and negative pairs.\n",
    "    pos_sim = (norm_emb[pos_edge[:, 0]] * norm_emb[pos_edge[:, 1]]).sum(dim=1)\n",
    "    neg_sim = (norm_emb[neg_edge[:, 0]] * norm_emb[neg_edge[:, 1]]).sum(dim=1)\n",
    "\n",
    "    # Compute the difference scaled by temperature.\n",
    "    logits_diff = (pos_sim - neg_sim) / temperature\n",
    "    # Compute loss as -log(sigmoid(...))\n",
    "    loss = -torch.log(torch.sigmoid(logits_diff) + eps)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def train_new(train_loader, model, optimizer, device, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Training loop that uses the new contrastive loss based on precomputed edges.\n",
    "    \n",
    "    For each batch (partitioned by event using data.x_batch), the loss is computed for each event\n",
    "    separately based on the node embeddings and the stored positive (x_pe) and negative (x_ne) edges.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: PyTorch DataLoader yielding Data objects.\n",
    "        model:        The network model.\n",
    "        optimizer:    Optimizer.\n",
    "        device:       Torch device.\n",
    "        temperature:  Temperature scaling factor for the loss.\n",
    "        \n",
    "    Returns:\n",
    "        Average loss per sample over the training set.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_events = 0\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute embeddings.\n",
    "        embeddings, _ = model(data.x, data.x_batch)\n",
    "        \n",
    "        # Partition by event using data.x_batch.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            # Slice the positive and negative edge tensors for the event.\n",
    "            event_pos_edge = data.x_pe[start_idx:end_idx]\n",
    "            event_neg_edge = data.x_ne[start_idx:end_idx]\n",
    "\n",
    "            loss_event = contrastive_loss_edges(event_embeddings, event_pos_edge, event_neg_edge,\n",
    "                                                  temperature=temperature)\n",
    "            loss_event_total += loss_event\n",
    "            n_events += 1\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        loss = loss_event_total / (len(counts) if len(counts) > 0 else 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / n_events if n_events > 0 else 0.0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Validation loop that uses the new contrastive loss based on precomputed edges.\n",
    "    \n",
    "    Args:\n",
    "        test_loader: PyTorch DataLoader yielding Data objects.\n",
    "        model:       The network model.\n",
    "        device:      Torch device.\n",
    "        temperature: Temperature scaling factor.\n",
    "        \n",
    "    Returns:\n",
    "        Average loss per sample over the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_events = 0\n",
    "    for data in tqdm(test_loader, desc=\"Validation\"):\n",
    "        data = data.to(device)\n",
    "        embeddings, _ = model(data.x, data.x_batch)\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_pos_edge = data.x_pe[start_idx:end_idx]\n",
    "            event_neg_edge = data.x_ne[start_idx:end_idx]\n",
    "            \n",
    "            loss_event = contrastive_loss_edges(event_embeddings, event_pos_edge, event_neg_edge,\n",
    "                                                  temperature=temperature)\n",
    "            loss_event_total += loss_event\n",
    "            n_events += 1\n",
    "            start_idx = end_idx\n",
    "        total_loss += loss_event_total / (len(counts) if len(counts) > 0 else 1)\n",
    "    return total_loss / n_events if n_events > 0 else 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4811314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Alpha: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███                             | 44/456 [00:29<04:33,  1.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3452777/834369869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs} | Alpha: {alpha:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3452777/1267770454.py\u001b[0m in \u001b[0;36mtrain_new\u001b[0;34m(train_loader, model, optimizer, device, temperature)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mn_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 or (isinstance(idx, np.ndarray) and np.isscalar(idx))):\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3452777/3670420205.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0morig_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mneg_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             candidates = [j for j in range(total_lc) if j != i and \n\u001b[0m\u001b[1;32m    353\u001b[0m                           (groups_np[j][0] not in orig_set or fractions_np[j][0] == 0)]\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3452777/3670420205.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mneg_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             candidates = [j for j in range(total_lc) if j != i and \n\u001b[0;32m--> 353\u001b[0;31m                           (groups_np[j][0] not in orig_set or fractions_np[j][0] == 0)]\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0mneg_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model with passed hyperparameters\n",
    "model = Net(\n",
    "    hidden_dim=64,\n",
    "    num_layers=3,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=64,\n",
    "    k=16\n",
    ")\n",
    "\n",
    "BS = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# Load DataLoader with current batch_size\n",
    "train_loader = DataLoader(data_train, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Train and evaluate the model for the specified number of epochs\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Store train and validation losses for all epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_dir = '/vols/cms/mm1221/hgcal/elec5New/LC/Fraction/resultstest/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "patience =30    \n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # For epochs 1 to 150, gradually increase alpha from 0 to 1.\n",
    "    # From epoch 151 onward, set alpha = 1 (fully hard negatives).\n",
    "    alpha = 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Alpha: {alpha:.2f}\")\n",
    "    train_loss = train_new(train_loader, model, optimizer, device,temperature=0.1)\n",
    "    val_loss = test_new(val_loader, model, device,temperature=0.1)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model if validation loss improves.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Save intermediate checkpoint.\n",
    "    state_dicts = {'model': model.state_dict(),\n",
    "                   'opt': optimizer.state_dict(),\n",
    "                   'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch+1}.pt'))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.item():.8f}, Validation Loss: {val_loss.item():.8f}\")\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Save training history.\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, 'continued_training_loss.csv'), index=False)\n",
    "print(f\"Saved loss curves to {os.path.join(output_dir, 'continued_training_loss.csv')}\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "print(\"Training complete. Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05292047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[32].groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "23dd5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.9798e+01,  5.9121e+00,  3.4050e+02,  ...,  2.8000e+01,\n",
      "          1.9395e+00,  5.9172e-02],\n",
      "        [ 1.0117e+02,  6.8384e+00,  3.4355e+02,  ...,  2.8000e+01,\n",
      "          1.9345e+00,  6.7492e-02],\n",
      "        [ 1.0017e+02,  5.9966e+00,  3.4149e+02,  ...,  3.1000e+01,\n",
      "          1.9387e+00,  5.9793e-02],\n",
      "        ...,\n",
      "        [ 8.8137e+01, -4.5364e+00,  3.5338e+02,  ...,  2.0000e+00,\n",
      "          2.0957e+00, -5.1424e-02],\n",
      "        [ 8.4958e+01, -4.5540e+00,  3.5676e+02,  ...,  2.0000e+00,\n",
      "          2.1405e+00, -5.3552e-02],\n",
      "        [ 7.7862e+01, -6.1858e+00,  3.2521e+02,  ...,  2.0000e+00,\n",
      "          2.1337e+00, -7.9279e-02]])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[32].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26058846",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in ListOffsetArray64 attempting to get 3, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2173693/3549174803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jaxtracers_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in ListOffsetArray64 attempting to get 3, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)"
     ]
    }
   ],
   "source": [
    "print(lc_ind[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd1bb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_x = data['simtrackstersCP;2']['vertices_x'].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5be8e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.1, -24.8, -58.7, -40.5, -60.4, -48, ... -40.5, -41.2, -43.3, -45.4, -51.6, -56]\n"
     ]
    }
   ],
   "source": [
    "print(lc_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
