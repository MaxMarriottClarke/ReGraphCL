{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26f4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os.path as osp\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import os.path as osp  # This defines 'osp'\n",
    "import glob\n",
    "\n",
    "# Function to find the highest numbered branch matching base_name in an uproot file\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "# Function to remove duplicates: for each event, only keep the entry with the highest B value for each unique element in A.\n",
    "def remove_duplicates(A, B):    \n",
    "    all_masks = []\n",
    "    for event_idx, event in enumerate(A):\n",
    "        flat_A = np.array(ak.flatten(A[event_idx]))\n",
    "        flat_B = np.array(ak.flatten(B[event_idx]))\n",
    "        mask = np.zeros_like(flat_A, dtype=bool)\n",
    "        for elem in np.unique(flat_A):\n",
    "            indices = np.where(flat_A == elem)[0]\n",
    "            if len(indices) > 1:\n",
    "                max_index = indices[np.argmax(flat_B[indices])]\n",
    "                mask[max_index] = True\n",
    "            else:\n",
    "                mask[indices[0]] = True\n",
    "        unflattened_mask = ak.unflatten(mask, ak.num(A[event_idx]))\n",
    "        all_masks.append(unflattened_mask)\n",
    "    return ak.Array(all_masks)\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Dataset for layer clusters.\n",
    "    For each event it builds:\n",
    "      - x: node features, shape (N, D)\n",
    "      - groups: top-3 contributing group IDs per node (N, 3)\n",
    "      - fractions: corresponding energy fractions (N, 3)\n",
    "      - x_pe: positive edge pairs, shape (N, 2)\n",
    "      - x_ne: negative edge pairs, shape (N, 2)\n",
    "    '''\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.step_size = 500\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "\n",
    "        # These will hold the precomputed arrays (one per event)\n",
    "        self.precomputed_groups = []\n",
    "        self.precomputed_fractions = []\n",
    "        \n",
    "        self.fill_data(max_events)\n",
    "        self.precompute_pairings()\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading data\")\n",
    "        for fi, path in enumerate(sorted(glob.glob(osp.join(self.raw_dir, '*.root')))):\n",
    "            if self.inp in ['train', 'val']:\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "                track_path = find_highest_branch(path, 'tracksters')\n",
    "            else:\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "                track_path = find_highest_branch(path, 'tracksters')\n",
    "            \n",
    "            crosstree = uproot.open(path)[cluster_path]\n",
    "            crosscounter = 0\n",
    "            \n",
    "            # Create an iterator for the tracksters branch to load its 'vertices_indexes'\n",
    "            tracksters_iter = uproot.iterate(\n",
    "                f\"{path}:{track_path}\",\n",
    "                [\"vertices_indexes\"],\n",
    "                step_size=self.step_size\n",
    "            )\n",
    "            \n",
    "            for array in uproot.iterate(f\"{path}:{sim_path}\", \n",
    "                                         [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n",
    "                                          \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \n",
    "                                          \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\"],\n",
    "                                         step_size=self.step_size):\n",
    "                # Get the tracksters branch data for this iteration\n",
    "                tmp_tracksters_data = next(tracksters_iter)\n",
    "                tmp_tracksters_vertices_indexes = tmp_tracksters_data[\"vertices_indexes\"]\n",
    "                \n",
    "                # Load the simtrackstersCP branch arrays\n",
    "                tmp_stsCP_vertices_x = array['vertices_x']\n",
    "                tmp_stsCP_vertices_y = array['vertices_y']\n",
    "                tmp_stsCP_vertices_z = array['vertices_z']\n",
    "                tmp_stsCP_vertices_energy = array['vertices_energy']\n",
    "                tmp_stsCP_vertices_time = array['vertices_time']\n",
    "                tmp_stsCP_vertices_indexes = array['vertices_indexes']\n",
    "                tmp_stsCP_vertices_multiplicity = array['vertices_multiplicity']\n",
    "                tmp_stsCP_barycenter_x = array['barycenter_x']\n",
    "                tmp_stsCP_barycenter_y = array['barycenter_y']\n",
    "                tmp_stsCP_barycenter_z = array['barycenter_z']\n",
    "                \n",
    "                self.step_size = min(self.step_size, len(tmp_stsCP_vertices_x))\n",
    "\n",
    "                tmp_all_vertices_layer_id = crosstree['cluster_layer_id'].array(\n",
    "                    entry_start=crosscounter*self.step_size,\n",
    "                    entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_noh = crosstree['cluster_number_of_hits'].array(\n",
    "                    entry_start=crosscounter*self.step_size,\n",
    "                    entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_eta = crosstree['position_eta'].array(\n",
    "                    entry_start=crosscounter*self.step_size,\n",
    "                    entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_phi = crosstree['position_phi'].array(\n",
    "                    entry_start=crosscounter*self.step_size,\n",
    "                    entry_stop=(crosscounter+1)*self.step_size)\n",
    "                crosscounter += 1\n",
    "\n",
    "                layer_id_list = []\n",
    "                noh_list = []\n",
    "                eta_list = []\n",
    "                phi_list = []\n",
    "                for evt_row in range(len(tmp_all_vertices_noh)):\n",
    "                    layer_id_list_one_event = []\n",
    "                    noh_list_one_event = []\n",
    "                    eta_list_one_event = []\n",
    "                    phi_list_one_event = []\n",
    "                    for particle in range(len(tmp_stsCP_vertices_indexes[evt_row])):\n",
    "                        tmp_stsCP_vertices_layer_id_one_particle = tmp_all_vertices_layer_id[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_noh_one_particle = tmp_all_vertices_noh[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_eta_one_particle = tmp_all_vertices_eta[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_phi_one_particle = tmp_all_vertices_phi[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        layer_id_list_one_event.append(tmp_stsCP_vertices_layer_id_one_particle)\n",
    "                        noh_list_one_event.append(tmp_stsCP_vertices_noh_one_particle)\n",
    "                        eta_list_one_event.append(tmp_stsCP_vertices_eta_one_particle)\n",
    "                        phi_list_one_event.append(tmp_stsCP_vertices_phi_one_particle)\n",
    "                    layer_id_list.append(layer_id_list_one_event)\n",
    "                    noh_list.append(noh_list_one_event)\n",
    "                    eta_list.append(eta_list_one_event)\n",
    "                    phi_list.append(phi_list_one_event)\n",
    "                tmp_stsCP_vertices_layer_id = ak.Array(layer_id_list)\n",
    "                tmp_stsCP_vertices_noh = ak.Array(noh_list)\n",
    "                tmp_stsCP_vertices_eta = ak.Array(eta_list)\n",
    "                tmp_stsCP_vertices_phi = ak.Array(phi_list)\n",
    "                \n",
    "                # NEW FILTERING: Remove simtracksters entries whose index is not in any tracksters sub-array.\n",
    "                mask_list = []\n",
    "                for sim_evt, track_evt in zip(tmp_stsCP_vertices_indexes, tmp_tracksters_vertices_indexes):\n",
    "                    track_flat = ak.flatten(track_evt)\n",
    "                    track_set = set(ak.to_list(track_flat))\n",
    "                    sim_evt_list = ak.to_list(sim_evt)\n",
    "                    mask_evt = [[elem in track_set for elem in subarr] for subarr in sim_evt_list]\n",
    "                    mask_list.append(mask_evt)\n",
    "                mask_track = ak.Array(mask_list)\n",
    "\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[mask_track]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[mask_track]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[mask_track]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[mask_track]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[mask_track]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[mask_track]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[mask_track]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[mask_track]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[mask_track]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[mask_track]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[mask_track]\n",
    "\n",
    "                # Further filtering: remove events with fewer than 2 vertices.\n",
    "                skim_mask = [len(e) >= 2 for e in tmp_stsCP_vertices_x]\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask]\n",
    "\n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_indexes_unfilt = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_vertices_multiplicity_unfilt = tmp_stsCP_vertices_multiplicity\n",
    "                else:\n",
    "                    self.stsCP_vertices_indexes_unfilt = ak.concatenate(\n",
    "                        (self.stsCP_vertices_indexes_unfilt, tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_vertices_multiplicity_unfilt = ak.concatenate(\n",
    "                        (self.stsCP_vertices_multiplicity_unfilt, tmp_stsCP_vertices_multiplicity))\n",
    "                \n",
    "                energyPercent = 1 / tmp_stsCP_vertices_multiplicity\n",
    "                skim_mask_energyPercent = remove_duplicates(tmp_stsCP_vertices_indexes, energyPercent)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_energyPercent]\n",
    "                \n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_x = tmp_stsCP_vertices_x\n",
    "                    self.stsCP_vertices_y = tmp_stsCP_vertices_y\n",
    "                    self.stsCP_vertices_z = tmp_stsCP_vertices_z\n",
    "                    self.stsCP_vertices_energy = tmp_stsCP_vertices_energy\n",
    "                    self.stsCP_vertices_time = tmp_stsCP_vertices_time\n",
    "                    self.stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id\n",
    "                    self.stsCP_vertices_noh = tmp_stsCP_vertices_noh\n",
    "                    self.stsCP_vertices_eta = tmp_stsCP_vertices_eta\n",
    "                    self.stsCP_vertices_phi = tmp_stsCP_vertices_phi\n",
    "                    self.stsCP_vertices_indexes = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_barycenter_x = tmp_stsCP_barycenter_x\n",
    "                    self.stsCP_barycenter_y = tmp_stsCP_barycenter_y\n",
    "                    self.stsCP_barycenter_z = tmp_stsCP_barycenter_z\n",
    "                    self.stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity\n",
    "                    self.stsCP_vertices_indexes_filt = tmp_stsCP_vertices_indexes_filt\n",
    "                else:\n",
    "                    self.stsCP_vertices_x = ak.concatenate((self.stsCP_vertices_x, tmp_stsCP_vertices_x))\n",
    "                    self.stsCP_vertices_y = ak.concatenate((self.stsCP_vertices_y, tmp_stsCP_vertices_y))\n",
    "                    self.stsCP_vertices_z = ak.concatenate((self.stsCP_vertices_z, tmp_stsCP_vertices_z))\n",
    "                    self.stsCP_vertices_energy = ak.concatenate((self.stsCP_vertices_energy, tmp_stsCP_vertices_energy))\n",
    "                    self.stsCP_vertices_time = ak.concatenate((self.stsCP_vertices_time, tmp_stsCP_vertices_time))\n",
    "                    self.stsCP_vertices_layer_id = ak.concatenate((self.stsCP_vertices_layer_id, tmp_stsCP_vertices_layer_id))\n",
    "                    self.stsCP_vertices_noh = ak.concatenate((self.stsCP_vertices_noh, tmp_stsCP_vertices_noh))\n",
    "                    self.stsCP_vertices_eta = ak.concatenate((self.stsCP_vertices_eta, tmp_stsCP_vertices_eta))\n",
    "                    self.stsCP_vertices_phi = ak.concatenate((self.stsCP_vertices_phi, tmp_stsCP_vertices_phi))\n",
    "                    self.stsCP_vertices_indexes = ak.concatenate((self.stsCP_vertices_indexes, tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_barycenter_x = ak.concatenate((self.stsCP_barycenter_x, tmp_stsCP_barycenter_x))\n",
    "                    self.stsCP_barycenter_y = ak.concatenate((self.stsCP_barycenter_y, tmp_stsCP_barycenter_y))\n",
    "                    self.stsCP_barycenter_z = ak.concatenate((self.stsCP_barycenter_z, tmp_stsCP_barycenter_z))\n",
    "                    self.stsCP_vertices_multiplicity = ak.concatenate((self.stsCP_vertices_multiplicity, tmp_stsCP_vertices_multiplicity))\n",
    "                    self.stsCP_vertices_indexes_filt = ak.concatenate((self.stsCP_vertices_indexes_filt, tmp_stsCP_vertices_indexes_filt))\n",
    "                \n",
    "                counter += 1\n",
    "                if len(self.stsCP_vertices_x) > max_events:\n",
    "                    print(f\"Reached {max_events}!\")\n",
    "                    break\n",
    "            if len(self.stsCP_vertices_x) > max_events:\n",
    "                break\n",
    "\n",
    "    def precompute_pairings(self):\n",
    "        \"\"\"\n",
    "        Precompute the groups, fractions, and also the positive/negative edge pairs for each event.\n",
    "        \"\"\"\n",
    "        n_events = len(self.stsCP_vertices_x)\n",
    "        # (Clear previous lists if necessary)\n",
    "        self.precomputed_groups = []\n",
    "        self.precomputed_fractions = []\n",
    "        self.precomputed_pos_edges = []\n",
    "        self.precomputed_neg_edges = []\n",
    "        for idx in range(n_events):\n",
    "            unfilt_cp_array = self.stsCP_vertices_indexes_unfilt[idx]\n",
    "            unfilt_mult_array = self.stsCP_vertices_multiplicity_unfilt[idx]\n",
    "            cluster_contributors = {}\n",
    "            for cp_id in range(len(unfilt_cp_array)):\n",
    "                for local_lc_id, cluster_id in enumerate(unfilt_cp_array[cp_id]):\n",
    "                    frac = 1.0 / unfilt_mult_array[cp_id][local_lc_id]\n",
    "                    cluster_contributors.setdefault(cluster_id, []).append((cp_id, frac))\n",
    "            final_cp_array = self.stsCP_vertices_indexes_filt[idx]\n",
    "            flattened_cluster_ids = ak.flatten(final_cp_array)\n",
    "            total_lc = len(flattened_cluster_ids)\n",
    "            groups_np = np.zeros((total_lc, 3), dtype=np.int64)\n",
    "            fractions_np = np.zeros((total_lc, 3), dtype=np.float32)\n",
    "            for global_lc, cluster_id in enumerate(flattened_cluster_ids):\n",
    "                contribs = cluster_contributors.get(cluster_id, [])\n",
    "                contribs_sorted = sorted(contribs, key=lambda x: x[1], reverse=True)\n",
    "                if len(contribs_sorted) == 0:\n",
    "                    top3 = [(0, 0.0)] * 3\n",
    "                else:\n",
    "                    top3 = contribs_sorted[:3]\n",
    "                    while len(top3) < 3:\n",
    "                        top3.append(top3[-1])\n",
    "                for i in range(3):\n",
    "                    groups_np[global_lc, i] = top3[i][0]\n",
    "                    fractions_np[global_lc, i] = top3[i][1]\n",
    "            self.precomputed_groups.append(groups_np)\n",
    "            self.precomputed_fractions.append(fractions_np)\n",
    "            \n",
    "            \n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            'Dataset not found. Please download it from {} and move all '\n",
    "            '*.z files to {}'.format(self.url, self.raw_dir))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.stsCP_vertices_x)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def get(self, idx):\n",
    "        # 1) Flatten node features for the event\n",
    "        lc_x = self.stsCP_vertices_x[idx]\n",
    "        lc_y = self.stsCP_vertices_y[idx]\n",
    "        lc_z = self.stsCP_vertices_z[idx]\n",
    "        lc_e = self.stsCP_vertices_energy[idx]\n",
    "        lc_layer_id = self.stsCP_vertices_layer_id[idx]\n",
    "        lc_noh = self.stsCP_vertices_noh[idx]\n",
    "        lc_eta = self.stsCP_vertices_eta[idx]\n",
    "        lc_phi = self.stsCP_vertices_phi[idx]\n",
    "\n",
    "        flat_lc_x = np.expand_dims(np.array(ak.flatten(lc_x)), axis=1)\n",
    "        flat_lc_y = np.expand_dims(np.array(ak.flatten(lc_y)), axis=1)\n",
    "        flat_lc_z = np.expand_dims(np.array(ak.flatten(lc_z)), axis=1)\n",
    "        flat_lc_e = np.expand_dims(np.array(ak.flatten(lc_e)), axis=1)\n",
    "        flat_lc_layer_id = np.expand_dims(np.array(ak.flatten(lc_layer_id)), axis=1)\n",
    "        flat_lc_noh = np.expand_dims(np.array(ak.flatten(lc_noh)), axis=1)\n",
    "        flat_lc_eta = np.expand_dims(np.array(ak.flatten(lc_eta)), axis=1)\n",
    "        flat_lc_phi = np.expand_dims(np.array(ak.flatten(lc_phi)), axis=1)\n",
    "        flat_lc_feats = np.concatenate(\n",
    "            (flat_lc_x, flat_lc_y, flat_lc_z, flat_lc_e,\n",
    "             flat_lc_layer_id, flat_lc_noh, flat_lc_eta, flat_lc_phi),\n",
    "            axis=-1\n",
    "        )\n",
    "        total_lc = flat_lc_feats.shape[0]\n",
    "        x = torch.from_numpy(flat_lc_feats).float()\n",
    "\n",
    "        # 2) Retrieve precomputed groups, fractions, and edges\n",
    "        groups_np = self.precomputed_groups[idx]\n",
    "        fractions_np = self.precomputed_fractions[idx]\n",
    "\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            groups=torch.from_numpy(groups_np),\n",
    "            fractions=torch.from_numpy(fractions_np)\n",
    "\n",
    "        )\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91249a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading data\n",
      "Reached 10!\n",
      "### Loading data\n",
      "Reached 10!\n"
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/mix/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/mix/val/\"\n",
    "data_train = CCV1(ipath, max_events=10, inp = 'train')\n",
    "data_val = CCV1(vpath, max_events=10, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc312a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 3])\n",
      "tensor([0.9007, 0.0993, 0.0993])\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "print(data_train[0].groups[j])\n",
    "print(data_train[0].fractions[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f58289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 3])\n",
      "tensor([0.9484, 0.0516, 0.0516])\n"
     ]
    }
   ],
   "source": [
    "j=4\n",
    "print(data_train[0].groups[j])\n",
    "print(data_train[0].fractions[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56585e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import DynamicEdgeConv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, \n",
    "                 contrastive_dim=8, k=20):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with DynamicEdgeConv layers and two heads:\n",
    "        one for contrastive learning and one for predicting if a node is split.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of DynamicEdgeConv layers.\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            k (int): Number of nearest neighbors to use in DynamicEdgeConv.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.k = k\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers using DynamicEdgeConv layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # In this example, the same k is used for every layer.\n",
    "            current_k = self.k\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(p=dropout)\n",
    "            )\n",
    "            conv = DynamicEdgeConv(mlp, k=current_k, aggr=\"max\")\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Contrastive output head: produces node-level embeddings.\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, contrastive_dim)\n",
    "        )\n",
    "        \n",
    "        # Additional head for predicting whether a node is a split node.\n",
    "        self.split_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, 1)  # Produces a single logit per node.\n",
    "        )\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 8).\n",
    "            batch (torch.Tensor): Batch vector that assigns each node to an example.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (contrastive embeddings, split logits, batch)\n",
    "        \"\"\"\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply DynamicEdgeConv layers with residual connections.\n",
    "        feats = x_lc_enc\n",
    "        for conv in self.convs:\n",
    "            feats = conv(feats, batch) + feats\n",
    "\n",
    "        # Contrastive head output.\n",
    "        out = self.output(feats)\n",
    "        # Split prediction head output.\n",
    "        split_logit = self.split_head(feats)\n",
    "        return out, split_logit, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "201110e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_shared_energy_matrix_vectorized(groups, scores):\n",
    "    \"\"\"\n",
    "    Vectorized approach:\n",
    "      1) For each row i, collect the (group, fraction) *only once* per group.\n",
    "      2) For each group_id g, gather all (row_i, fraction_i) pairs.\n",
    "      3) Use a single big min(...) over all pairs in that group to update shared[i,j].\n",
    "    \"\"\"\n",
    "    N, num_slots = groups.shape\n",
    "    shared = torch.zeros(N, N)\n",
    "\n",
    "    # Step 1: Build a dictionary: g_id -> list of (row_index, fraction)\n",
    "    group_to_rows = defaultdict(list)\n",
    "\n",
    "    for i in range(N):\n",
    "        seen = set()\n",
    "        for s in range(num_slots):\n",
    "            g_id = groups[i, s].item()\n",
    "            if g_id not in seen:\n",
    "                seen.add(g_id)\n",
    "                frac = scores[i, s].item()\n",
    "                group_to_rows[g_id].append((i, frac))\n",
    "\n",
    "    # Step 2: For each group, vectorize the pairwise min\n",
    "    for g_id, row_frac_list in group_to_rows.items():\n",
    "        # row_frac_list: [(i1, frac1), (i2, frac2), ...]\n",
    "        if len(row_frac_list) < 2:\n",
    "            continue  # Only 1 row => no pairwise contribution\n",
    "\n",
    "        row_ix = torch.tensor([p[0] for p in row_frac_list], dtype=torch.long)\n",
    "        frac_ix = torch.tensor([p[1] for p in row_frac_list], dtype=torch.float)\n",
    "        # frac_ix has shape (m,)\n",
    "\n",
    "        # This gives an (m, m) matrix of min(...) between each pair of fractions\n",
    "        # – far more efficient than a Python loop over pairs.\n",
    "        min_matrix = torch.min(frac_ix.unsqueeze(0), frac_ix.unsqueeze(1))\n",
    "\n",
    "        # Now \"scatter-add\" these into the NxN 'shared' matrix\n",
    "        # The indexing trick: row_ix.unsqueeze(0) is shape (1,m), row_ix.unsqueeze(1) is shape (m,1),\n",
    "        # so shared[row_ix.unsqueeze(0), row_ix.unsqueeze(1)] is an (m, m) submatrix.\n",
    "        shared[row_ix.unsqueeze(0), row_ix.unsqueeze(1)] += min_matrix\n",
    "\n",
    "    return shared\n",
    "\n",
    "\n",
    "def contrastive_loss_fractional(embeddings, groups, scores, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Same final logic, but uses the vectorized build_shared_energy_matrix_vectorized\n",
    "    for speed.\n",
    "    \"\"\"\n",
    "    device = embeddings.device\n",
    "    N, D = embeddings.shape\n",
    "\n",
    "    # 1) Cosine similarity (N x N)\n",
    "    norm_emb = F.normalize(embeddings, p=2, dim=1)\n",
    "    sim_matrix = norm_emb @ norm_emb.t()\n",
    "\n",
    "    # 2) Build NxN \"shared energy\" matrix\n",
    "    shared_energy = build_shared_energy_matrix_vectorized(groups, scores).to(device)\n",
    "    # 3) Positive vs negative mask & weighting\n",
    "    pos_mask = (shared_energy >= 0.5)\n",
    "    neg_mask = ~pos_mask\n",
    "    pos_weight = torch.zeros_like(shared_energy, device=device)\n",
    "    neg_weight = torch.zeros_like(shared_energy, device=device)\n",
    "\n",
    "    pos_weight[pos_mask] = 2.0 * (shared_energy[pos_mask] - 0.5)\n",
    "    neg_weight[neg_mask] = 2.0 * (0.5 - shared_energy[neg_mask])\n",
    "    pos_weight.fill_diagonal_(0)\n",
    "    neg_weight.fill_diagonal_(0)\n",
    "\n",
    "    # 4) Softmax terms\n",
    "    exp_sim = torch.exp(sim_matrix / temperature)\n",
    "    numerator = (pos_weight * exp_sim).sum(dim=1)  # shape (N,)\n",
    "    denominator = ((pos_weight + neg_weight) * exp_sim).sum(dim=1)  # shape (N,)\n",
    "\n",
    "    # 5) Filter anchors with no positives\n",
    "    anchor_has_pos = (pos_weight.sum(dim=1) > 0)\n",
    "    valid_numerator = numerator[anchor_has_pos]\n",
    "    valid_denominator = denominator[anchor_has_pos]\n",
    "\n",
    "    if valid_numerator.numel() == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "    # 6) Final loss\n",
    "    loss_per_anchor = -torch.log(valid_numerator / (valid_denominator + 1e-8))\n",
    "    return loss_per_anchor.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train_new(train_loader, model, optimizer, device, temperature=0.1, alpha=1.0, beta=1.0, pos_weight=None):\n",
    "    \"\"\"\n",
    "    Training loop that uses the contrastive loss and an additional loss for split node prediction.\n",
    "    \n",
    "    The total loss is computed as:\n",
    "         loss = α * (contrastive loss) + β * (split loss)\n",
    "         \n",
    "    Additionally, the function prints the separate contrastive and split loss contributions per batch.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader yielding Data objects.\n",
    "        model: The network model.\n",
    "        optimizer: Optimizer.\n",
    "        device: Torch device.\n",
    "        temperature: Temperature scaling for contrastive loss.\n",
    "        alpha (float): Weighting factor for the contrastive loss.\n",
    "        beta (float): Weighting factor for the split loss.\n",
    "        pos_weight: Tensor or float for weighting positive examples in BCEWithLogitsLoss.\n",
    "                    If None, defaults to 2.0.\n",
    "        \n",
    "    Returns:\n",
    "        overall_loss, contrast_loss_avg, split_loss_avg: The average losses per node over the training set.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_contrast_loss = 0.0\n",
    "    total_split_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    # Set a default positive weight if not provided.\n",
    "    if pos_weight is None:\n",
    "        pos_weight = torch.tensor(2.0, device=device)\n",
    "\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute both contrastive embeddings and split logits.\n",
    "        embeddings, split_logits, _ = model(data.x, data.x_batch)\n",
    "        \n",
    "        # Partition by event using data.x_batch.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        contrastive_loss_sum = 0.0\n",
    "        split_loss_sum = 0.0\n",
    "        start_idx = 0\n",
    "        # Loop over each event in the batch.\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_split_logit = split_logits[start_idx:end_idx]  # shape: (count, 1)\n",
    "            event_groups = data.groups[start_idx:end_idx]\n",
    "            event_fractions = data.fractions[start_idx:end_idx]\n",
    "\n",
    "            # 1) Contrastive loss for the event.\n",
    "            loss_contrast = contrastive_loss_fractional(\n",
    "                event_embeddings, event_groups, event_fractions, temperature=temperature\n",
    "            )\n",
    "            contrastive_loss_sum += loss_contrast\n",
    "\n",
    "            # 2) Compute split label from event_fractions.\n",
    "            # For each node, count how many fraction values are >= 0.1.\n",
    "            below_threshold = (event_fractions >= 0.1).sum(dim=1)\n",
    "            split_label = (below_threshold >= 2).float()  # shape: (count,)\n",
    "\n",
    "            # 3) Compute BCEWithLogitsLoss for split classification.\n",
    "            event_split_logit = event_split_logit.view(-1)  # shape: (count,)\n",
    "            loss_split = F.binary_cross_entropy_with_logits(\n",
    "                event_split_logit, split_label, pos_weight=pos_weight\n",
    "            )\n",
    "            split_loss_sum += loss_split\n",
    "\n",
    "            n_samples += event_embeddings.size(0)\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # Average losses across events in the current batch.\n",
    "        num_events = len(counts)\n",
    "        batch_contrast_loss = contrastive_loss_sum / num_events\n",
    "        batch_split_loss = split_loss_sum / num_events\n",
    "        \n",
    "        total_batch_loss = alpha * batch_contrast_loss + beta * batch_split_loss\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item() * embeddings.size(0)\n",
    "        total_contrast_loss += batch_contrast_loss.item() * embeddings.size(0)\n",
    "        total_split_loss += batch_split_loss.item() * embeddings.size(0)\n",
    "        \n",
    "        \n",
    "    overall_loss = total_loss / n_samples\n",
    "    contrast_loss_avg = total_contrast_loss / n_samples\n",
    "    split_loss_avg = total_split_loss / n_samples\n",
    "    return overall_loss, contrast_loss_avg, split_loss_avg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device, temperature=0.1, alpha=1.0, beta=1.0, pos_weight=None):\n",
    "    \"\"\"\n",
    "    Validation loop that computes both contrastive loss and split loss.\n",
    "    \n",
    "    The total loss is computed as:\n",
    "         loss = α * (contrastive loss) + β * (split loss)\n",
    "    \n",
    "    Args:\n",
    "        test_loader: DataLoader yielding Data objects.\n",
    "        model: The network model.\n",
    "        device: Torch device.\n",
    "        temperature: Temperature scaling for contrastive loss.\n",
    "        alpha (float): Weighting factor for the contrastive loss.\n",
    "        beta (float): Weighting factor for the split loss.\n",
    "        pos_weight: Tensor or float for BCEWithLogitsLoss positive weighting.\n",
    "                    If None, defaults to 2.0.\n",
    "        \n",
    "    Returns:\n",
    "        overall_loss, contrast_loss_avg, split_loss_avg: The average losses per node over the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_contrast_loss = 0.0\n",
    "    total_split_loss = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    if pos_weight is None:\n",
    "        pos_weight = torch.tensor(2.0, device=device)\n",
    "\n",
    "    for data in tqdm(test_loader, desc=\"Validation\"):\n",
    "        data = data.to(device)\n",
    "        embeddings, split_logits, _ = model(data.x, data.x_batch)\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_split_logit = split_logits[start_idx:end_idx]\n",
    "            event_groups = data.groups[start_idx:end_idx]\n",
    "            event_fractions = data.fractions[start_idx:end_idx]\n",
    "            \n",
    "            loss_contrast = contrastive_loss_fractional(\n",
    "                event_embeddings, event_groups, event_fractions, temperature=temperature\n",
    "            )\n",
    "            \n",
    "            below_threshold = (event_fractions >= 0.1).sum(dim=1)\n",
    "            split_label = (below_threshold >= 2).float()  # shape: (count,)\n",
    "            event_split_logit = event_split_logit.view(-1)\n",
    "            loss_split = F.binary_cross_entropy_with_logits(\n",
    "                event_split_logit, split_label, pos_weight=pos_weight\n",
    "            )\n",
    "            \n",
    "            loss_event = alpha * loss_contrast + beta * loss_split\n",
    "            loss_event_total += loss_event\n",
    "            n_samples += event_embeddings.size(0)\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        total_loss += loss_event_total / (len(counts) if len(counts) > 0 else 1)\n",
    "        \n",
    "    overall_loss = total_loss / n_samples\n",
    "    contrast_loss_avg = total_contrast_loss / n_samples  # (Not accumulated separately in this loop)\n",
    "    split_loss_avg = total_split_loss / n_samples        # (Not accumulated separately in this loop)\n",
    "    return overall_loss, contrast_loss_avg, split_loss_avg\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4811314",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 431/456 [02:58<00:09,  2.69it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize model with passed hyperparameters\n",
    "model = Net(\n",
    "    hidden_dim=64,\n",
    "    num_layers=3,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=64,\n",
    "    k=16\n",
    ")\n",
    "\n",
    "BS = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load DataLoader with current batch_size\n",
    "train_loader = DataLoader(data_train, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Output directory for saving models and results\n",
    "output_dir = '/vols/cms/mm1221/hgcal/elec5New/LC/Fraction/resultstest/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize arrays for storing losses per epoch.\n",
    "train_overall_losses = []\n",
    "train_contrast_losses = []\n",
    "train_split_losses = []\n",
    "val_overall_losses = []\n",
    "val_contrast_losses = []\n",
    "val_split_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 30\n",
    "epochs = 5\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # For epochs 1 to 150, gradually increase alpha from 0 to 1.\n",
    "    # From epoch 151 onward, set alpha = 1 (fully hard negatives).\n",
    "    # Here, as an example, we use a fixed alpha. (You can change it as needed.)\n",
    "    alpha_val = 1.0  \n",
    "    beta_val = 1.0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} \")\n",
    "    \n",
    "    # The updated train_new now returns three losses: overall, contrastive, and split.\n",
    "    train_overall, train_contrast, train_split = train_new(\n",
    "        train_loader, model, optimizer, device, temperature=0.1, alpha=alpha_val, beta=beta_val\n",
    "    )\n",
    "    val_overall, val_contrast, val_split = test_new(\n",
    "        val_loader, model, device, temperature=0.1, alpha=alpha_val, beta=beta_val\n",
    "    )\n",
    "\n",
    "    train_overall_losses.append(train_overall)\n",
    "    train_contrast_losses.append(train_contrast)\n",
    "    train_split_losses.append(train_split)\n",
    "    val_overall_losses.append(val_overall)\n",
    "    val_contrast_losses.append(val_contrast)\n",
    "    val_split_losses.append(val_split)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model if validation loss improves.\n",
    "    if val_overall < best_val_loss:\n",
    "        best_val_loss = val_overall\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Save intermediate checkpoint.\n",
    "    state_dicts = {'model': model.state_dict(),\n",
    "                   'opt': optimizer.state_dict(),\n",
    "                   'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch+1}.pt'))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "          f\"Train Overall: {train_overall:.8f}, Contrast: {train_contrast:.8f}, Split: {train_split:.8f} | \"\n",
    "          f\"Val Overall: {val_overall:.8f}, Contrast: {val_contrast:.8f}, Split: {val_split:.8f}\")\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Save training history to CSV.\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_overall_losses) + 1)),\n",
    "    'train_overall_loss': train_overall_losses,\n",
    "    'train_contrast_loss': train_contrast_losses,\n",
    "    'train_split_loss': train_split_losses,\n",
    "    'val_overall_loss': val_overall_losses,\n",
    "    'val_contrast_loss': val_contrast_losses,\n",
    "    'val_split_loss': val_split_losses\n",
    "})\n",
    "results_csv_path = os.path.join(output_dir, 'continued_training_loss.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Saved loss curves to {results_csv_path}\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "print(\"Training complete. Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05292047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 8, 1])\n",
      "tensor([0.4331, 0.3189, 0.2480])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[1].groups[389])\n",
    "print(data_train[1].fractions[389])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23dd5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 9, 9])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(data_train[1].groups[388])\n",
    "print(data_train[1].fractions[388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26058846",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in ListOffsetArray64 attempting to get 3, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2173693/3549174803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jaxtracers_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in ListOffsetArray64 attempting to get 3, index out of range\n\n(https://github.com/scikit-hep/awkward-1.0/blob/1.10.3/src/libawkward/array/ListOffsetArray.cpp#L682)"
     ]
    }
   ],
   "source": [
    "print(lc_ind[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd1bb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_x = data['simtrackstersCP;2']['vertices_x'].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5be8e217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-59.1, -24.8, -58.7, -40.5, -60.4, -48, ... -40.5, -41.2, -43.3, -45.4, -51.6, -56]\n"
     ]
    }
   ],
   "source": [
    "print(lc_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
