{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9079592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "        input: layer clusters\n",
    "\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp = 'train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.step_size = 500\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self,max_events):\n",
    "        counter = 0\n",
    "        arrLens0 = []\n",
    "        arrLens1 = []\n",
    "\n",
    "        print(\"### Loading data\")\n",
    "        for fi,path in enumerate(tqdm.tqdm(self.raw_paths)):\n",
    "\n",
    "\n",
    "            if self.inp == 'train':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            elif self.inp == 'val':\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            else:\n",
    "                cluster_path = find_highest_branch(path, 'clusters')\n",
    "                sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            \n",
    "            crosstree =  uproot.open(path)[cluster_path]\n",
    "            crosscounter = 0\n",
    "            for array in uproot.iterate(f\"{path}:{sim_path}\", [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n",
    "            \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\"], step_size=self.step_size):\n",
    "            \n",
    "                tmp_stsCP_vertices_x = array['vertices_x']\n",
    "                tmp_stsCP_vertices_y = array['vertices_y']\n",
    "                tmp_stsCP_vertices_z = array['vertices_z']\n",
    "                tmp_stsCP_vertices_energy = array['vertices_energy']\n",
    "                tmp_stsCP_vertices_time = array['vertices_time']\n",
    "                tmp_stsCP_vertices_indexes = array['vertices_indexes']\n",
    "                tmp_stsCP_barycenter_x = array['barycenter_x']\n",
    "                tmp_stsCP_barycenter_y = array['barycenter_y']\n",
    "                tmp_stsCP_barycenter_z = array['barycenter_z']\n",
    "\n",
    "\n",
    "                tmp_stsCP_vertices_multiplicity = array['vertices_multiplicity']\n",
    "                \n",
    "                # weighted energies (A LC appears in its caloparticle assignment array as the energy it contributes not full energy)\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy * tmp_stsCP_vertices_multiplicity\n",
    "                \n",
    "                self.step_size = min(self.step_size,len(tmp_stsCP_vertices_x))\n",
    "\n",
    "\n",
    "                # Code block for reading from other tree\n",
    "                tmp_all_vertices_layer_id = crosstree['cluster_layer_id'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                #tmp_all_vertices_radius = crosstree['cluster_radius'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_noh = crosstree['cluster_number_of_hits'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_eta = crosstree['position_eta'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_phi = crosstree['position_phi'].array(entry_start=crosscounter*self.step_size,entry_stop=(crosscounter+1)*self.step_size)\n",
    "                crosscounter += 1\n",
    "\n",
    "                layer_id_list = []\n",
    "                radius_list = []\n",
    "                noh_list = []\n",
    "                eta_list = []\n",
    "                phi_list = []\n",
    "                for evt_row in range(len(tmp_all_vertices_noh)):\n",
    "                    #print(\"Event no: %i\"%evt_row)\n",
    "                    #print(\"There are %i particles in this event\"%len(tmp_stsCP_vertices_indexes[evt_row]))\n",
    "                    layer_id_list_one_event = []\n",
    "                    #radius_list_one_event = []\n",
    "                    noh_list_one_event = []\n",
    "                    eta_list_one_event = []\n",
    "                    phi_list_one_event = []\n",
    "                    for particle in range(len(tmp_stsCP_vertices_indexes[evt_row])):\n",
    "                        #print(\"Particle no: %i\"%particle)\n",
    "                        #print(\"A\")\n",
    "                        #print(np.array(tmp_all_vertices_radius[evt_row]).shape)\n",
    "                        #print(\"B\")\n",
    "                        #print(np.array(tmp_stsCP_vertices_indexes[evt_row][particle]).shape)\n",
    "                        #print(\"C\")\n",
    "                        tmp_stsCP_vertices_layer_id_one_particle = tmp_all_vertices_layer_id[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #tmp_stsCP_vertices_radius_one_particle = tmp_all_vertices_radius[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_noh_one_particle = tmp_all_vertices_noh[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_eta_one_particle = tmp_all_vertices_eta[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_phi_one_particle = tmp_all_vertices_phi[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #print(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        layer_id_list_one_event.append(tmp_stsCP_vertices_layer_id_one_particle)\n",
    "                        #radius_list_one_event.append(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        noh_list_one_event.append(tmp_stsCP_vertices_noh_one_particle)\n",
    "                        eta_list_one_event.append(tmp_stsCP_vertices_eta_one_particle)\n",
    "                        phi_list_one_event.append(tmp_stsCP_vertices_phi_one_particle)\n",
    "                    layer_id_list.append(layer_id_list_one_event)\n",
    "                    #radius_list.append(radius_list_one_event)\n",
    "                    noh_list.append(noh_list_one_event)\n",
    "                    eta_list.append(eta_list_one_event)\n",
    "                    phi_list.append(phi_list_one_event)\n",
    "                tmp_stsCP_vertices_layer_id = ak.Array(layer_id_list)                \n",
    "                #tmp_stsCP_vertices_radius = ak.Array(radius_list)                \n",
    "                tmp_stsCP_vertices_noh = ak.Array(noh_list)                \n",
    "                tmp_stsCP_vertices_eta = ak.Array(eta_list)                \n",
    "                tmp_stsCP_vertices_phi = ak.Array(phi_list)                \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                #SHOULD BE LEN(E) >= 2 for MULTI particles\n",
    "                skim_mask = []\n",
    "                for e in tmp_stsCP_vertices_x:\n",
    "                    if 2 <= len(e) <= 5: #<------ only train on samples with > 1 particle\n",
    "                        skim_mask.append(True)\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask]\n",
    "\n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_x = tmp_stsCP_vertices_x\n",
    "                    self.stsCP_vertices_y = tmp_stsCP_vertices_y\n",
    "                    self.stsCP_vertices_z = tmp_stsCP_vertices_z\n",
    "                    self.stsCP_vertices_energy = tmp_stsCP_vertices_energy\n",
    "                    self.stsCP_vertices_time = tmp_stsCP_vertices_time\n",
    "                    self.stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id\n",
    "                    #self.stsCP_vertices_radius = tmp_stsCP_vertices_radius\n",
    "                    self.stsCP_vertices_noh = tmp_stsCP_vertices_noh\n",
    "                    self.stsCP_vertices_eta = tmp_stsCP_vertices_eta\n",
    "                    self.stsCP_vertices_phi = tmp_stsCP_vertices_phi\n",
    "                    self.stsCP_vertices_indexes = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_barycenter_x = tmp_stsCP_barycenter_x\n",
    "                    self.stsCP_barycenter_y = tmp_stsCP_barycenter_y\n",
    "                    self.stsCP_barycenter_z = tmp_stsCP_barycenter_z\n",
    "                    self.stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity\n",
    "                else:\n",
    "                    self.stsCP_vertices_x = ak.concatenate((self.stsCP_vertices_x,tmp_stsCP_vertices_x))\n",
    "                    self.stsCP_vertices_y = ak.concatenate((self.stsCP_vertices_y,tmp_stsCP_vertices_y))\n",
    "                    self.stsCP_vertices_z = ak.concatenate((self.stsCP_vertices_z,tmp_stsCP_vertices_z))\n",
    "                    self.stsCP_vertices_energy = ak.concatenate((self.stsCP_vertices_energy,tmp_stsCP_vertices_energy))\n",
    "                    self.stsCP_vertices_time = ak.concatenate((self.stsCP_vertices_time,tmp_stsCP_vertices_time))\n",
    "                    self.stsCP_vertices_layer_id = ak.concatenate((self.stsCP_vertices_layer_id,tmp_stsCP_vertices_layer_id))\n",
    "                    #self.stsCP_vertices_radius = ak.concatenate((self.stsCP_vertices_radius,tmp_stsCP_vertices_radius))\n",
    "                    self.stsCP_vertices_noh = ak.concatenate((self.stsCP_vertices_noh,tmp_stsCP_vertices_noh))\n",
    "                    self.stsCP_vertices_eta = ak.concatenate((self.stsCP_vertices_eta,tmp_stsCP_vertices_eta))\n",
    "                    self.stsCP_vertices_phi = ak.concatenate((self.stsCP_vertices_phi,tmp_stsCP_vertices_phi))\n",
    "                    self.stsCP_vertices_indexes = ak.concatenate((self.stsCP_vertices_indexes,tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_barycenter_x = ak.concatenate((self.stsCP_barycenter_x,tmp_stsCP_barycenter_x))\n",
    "                    self.stsCP_barycenter_y = ak.concatenate((self.stsCP_barycenter_y,tmp_stsCP_barycenter_y))\n",
    "                    self.stsCP_barycenter_z = ak.concatenate((self.stsCP_barycenter_z,tmp_stsCP_barycenter_z))\n",
    "                    self.stsCP_vertices_multiplicity = ak.concatenate((self.stsCP_vertices_multiplicity, tmp_stsCP_vertices_multiplicity))\n",
    "\n",
    "                #print(len(self.stsCP_vertices_x))\n",
    "                counter += 1\n",
    "                if len(self.stsCP_vertices_x) > max_events:\n",
    "                    print(f\"Reached {max_events}!\")\n",
    "                    break\n",
    "            if len(self.stsCP_vertices_x) > max_events:\n",
    "                break\n",
    "     \n",
    "            \n",
    "            \n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            'Dataset not found. Please download it from {} and move all '\n",
    "            '*.z files to {}'.format(self.url, self.raw_dir))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.stsCP_vertices_x)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        \n",
    "        #raw_files = [osp.join(self.raw_dir, 'step3_NTUPLE.root')]\n",
    "\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    " \n",
    "        lc_x = self.stsCP_vertices_x[idx]\n",
    "        #print(ak.to_numpy(lc_x[0]).shape)\n",
    "        #print(ak.to_numpy(lc_x[1]).shape)\n",
    "        flat_lc_x = np.expand_dims(np.array(ak.flatten(lc_x)),axis=1)\n",
    "        lc_y = self.stsCP_vertices_y[idx]\n",
    "        flat_lc_y = np.expand_dims(np.array(ak.flatten(lc_y)),axis=1)\n",
    "        lc_z = self.stsCP_vertices_z[idx]\n",
    "        flat_lc_z = np.expand_dims(np.array(ak.flatten(lc_z)),axis=1)\n",
    "        lc_e = self.stsCP_vertices_energy[idx]\n",
    "        flat_lc_e = np.expand_dims(np.array(ak.flatten(lc_e)),axis=1)     \n",
    "        lc_t = self.stsCP_vertices_time[idx]\n",
    "        flat_lc_t = np.expand_dims(np.array(ak.flatten(lc_t)),axis=1)  \n",
    "        lc_layer_id = self.stsCP_vertices_layer_id[idx]\n",
    "        flat_lc_layer_id = np.expand_dims(np.array(ak.flatten(lc_layer_id)),axis=1)  \n",
    "        #lc_radius = self.stsCP_vertices_radius[idx]\n",
    "        #flat_lc_radius = np.expand_dims(np.array(ak.flatten(lc_radius)),axis=1)  \n",
    "        lc_noh = self.stsCP_vertices_noh[idx]\n",
    "        flat_lc_noh = np.expand_dims(np.array(ak.flatten(lc_noh)),axis=1)  \n",
    "        lc_eta = self.stsCP_vertices_eta[idx]\n",
    "        flat_lc_eta = np.expand_dims(np.array(ak.flatten(lc_eta)),axis=1)  \n",
    "        lc_phi = self.stsCP_vertices_phi[idx]\n",
    "        flat_lc_phi = np.expand_dims(np.array(ak.flatten(lc_phi)),axis=1)  \n",
    "        \n",
    "        lc_indexes = self.stsCP_vertices_indexes[idx]\n",
    "        flat_lc_indexes = np.expand_dims(np.array(ak.flatten(lc_indexes)),axis=1)  \n",
    "        lc_multiplicity = self.stsCP_vertices_multiplicity[idx]\n",
    "        flat_lc_multiplicity = np.expand_dims(np.array(ak.flatten(lc_multiplicity)),axis=1) \n",
    "\n",
    "        \n",
    "        mask = np.zeros_like(flat_lc_indexes, dtype=bool)\n",
    "\n",
    "        # Loop over each unique index.\n",
    "        for unique_idx in np.unique(flat_lc_indexes):\n",
    "            # Find the positions where this index occurs.\n",
    "            positions = np.where(flat_lc_indexes == unique_idx)[0]\n",
    "            # If there is only one occurrence, keep it.\n",
    "            if positions.size == 1:\n",
    "                mask[positions[0]] = True\n",
    "            else:\n",
    "                # If repeated, select the position with the minimum multiplicity.\n",
    "                pos_to_keep = positions[np.argmin(flat_lc_multiplicity[positions])]\n",
    "                mask[pos_to_keep] = True\n",
    "\n",
    "        # Now use this mask to filter all arrays:\n",
    "        flat_lc_x = flat_lc_x[mask].reshape(-1, 1)\n",
    "        flat_lc_y = flat_lc_y[mask].reshape(-1, 1)\n",
    "        flat_lc_z = flat_lc_z[mask].reshape(-1, 1)\n",
    "        flat_lc_e = flat_lc_e[mask].reshape(-1, 1)\n",
    "        flat_lc_t = flat_lc_t[mask].reshape(-1, 1)\n",
    "        flat_lc_layer_id = flat_lc_layer_id[mask].reshape(-1, 1)\n",
    "        flat_lc_noh = flat_lc_noh[mask].reshape(-1, 1)\n",
    "        flat_lc_eta = flat_lc_eta[mask].reshape(-1, 1)\n",
    "        flat_lc_phi = flat_lc_phi[mask].reshape(-1, 1)\n",
    "        flat_lc_indexes = flat_lc_indexes[mask].reshape(-1, 1)\n",
    "        flat_lc_multiplicity = flat_lc_multiplicity[mask].reshape(-1, 1)\n",
    "                                                               \n",
    "\n",
    "        flat_lc_feats = np.concatenate((flat_lc_x,flat_lc_y,flat_lc_z,flat_lc_e,\\\n",
    "                                        flat_lc_layer_id,flat_lc_noh,flat_lc_eta,flat_lc_phi),axis=-1)    \n",
    "\n",
    "        \n",
    "        max_index = max(np.max(subarr) for subarr in lc_indexes)\n",
    "\n",
    "        # Create an array to hold the association, initializing all values to -1 (meaning \"unassigned\")\n",
    "        assoc_array = -1 * np.ones(max_index + 1, dtype=int)\n",
    "        # Create an array to hold the best (i.e. smallest) multiplicity seen so far for each vertex.\n",
    "        best_mult = np.full(max_index + 1, np.inf)\n",
    "\n",
    "        # Loop over each sub-array in lc_indexes (and its corresponding multiplicity array)\n",
    "        for sub_idx, (indexes, mult_values) in enumerate(zip(lc_indexes, lc_multiplicity)):\n",
    "            # Loop over each index and its corresponding multiplicity value in the current sub-array.\n",
    "            for pos, vertex_index in enumerate(indexes):\n",
    "                candidate_mult = mult_values[pos]\n",
    "                # If this vertex hasn't been assigned yet, or if the current multiplicity is smaller than what we had,\n",
    "                # then update the association.\n",
    "                if assoc_array[vertex_index] == -1 or candidate_mult < best_mult[vertex_index]:\n",
    "                    assoc_array[vertex_index] = sub_idx\n",
    "                    best_mult[vertex_index] = candidate_mult\n",
    "\n",
    "\n",
    "\n",
    "        # Create the Data object\n",
    "        x = torch.from_numpy(flat_lc_feats).float()\n",
    "\n",
    "        return Data(\n",
    "            x=x, assoc = assoc_array\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cba6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/3 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 500!\n",
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/1 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 500!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/100k/5e/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/100k/5e/val/\"\n",
    "data_train = CCV1(ipath, max_events=500, inp = 'train')\n",
    "data_val = CCV1(vpath, max_events=500, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45344fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomStaticEdgeConv(nn.Module):\n",
    "    def __init__(self, nn_module):\n",
    "        super(CustomStaticEdgeConv, self).__init__()\n",
    "        self.nn_module = nn_module\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features of shape (N, F).\n",
    "            edge_index (torch.Tensor): Predefined edges [2, E], where E is the number of edges.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Node features after static edge aggregation.\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # Extract row (source) and col (target) nodes\n",
    "        x_center = x[row]\n",
    "        x_neighbor = x[col]\n",
    "\n",
    "        # Compute edge features (relative)\n",
    "        edge_features = torch.cat([x_center, x_neighbor - x_center], dim=-1)\n",
    "        edge_features = self.nn_module(edge_features)\n",
    "\n",
    "        # Aggregate features back to nodes\n",
    "        num_nodes = x.size(0)\n",
    "        node_features = torch.zeros(num_nodes, edge_features.size(-1), device=x.device)\n",
    "        node_features.index_add_(0, row, edge_features)\n",
    "\n",
    "        # Normalization (Divide by node degrees)\n",
    "        counts = torch.bincount(row, minlength=num_nodes).clamp(min=1).view(-1, 1)\n",
    "        node_features = node_features / counts\n",
    "\n",
    "        return node_features\n",
    "    \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with alternating StaticEdgeConv and GAT layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of convolutional layers (both StaticEdgeConv and GAT).\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            heads (int): Number of attention heads in GAT layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(15, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers, alternating between StaticEdgeConv and GAT\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Even-indexed layers: StaticEdgeConv\n",
    "            conv = CustomStaticEdgeConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                    nn.ELU(),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.Dropout(p=dropout)\n",
    "                )\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(16, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 15).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply convolutional layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            feats = conv(feats, edge_index) + feats  # Residual connection\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863332b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with passed hyperparameters\n",
    "model = Net(\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=16\n",
    ")\n",
    "\n",
    "k = 16\n",
    "BS = 64\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf74985",
   "metadata": {},
   "source": [
    "# Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55067c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_curriculum_both(embeddings, pos_indices, group_ids, temperature=0.1, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Computes an NT-Xent style loss that blends both positive and negative mining.\n",
    "    \n",
    "    For each anchor i:\n",
    "      - Provided positive similarity: pos_sim_orig = sim(embeddings[i], embeddings[pos_indices[i]])\n",
    "      - Hard positive similarity: hard_pos_sim = min { sim(embeddings[i], embeddings[j]) : \n",
    "                                                      j != i and group_ids[j] == group_ids[i] }\n",
    "      - Blended positive similarity: blended_pos = (1 - alpha) * pos_sim_orig + alpha * hard_pos_sim\n",
    "      - Random negative similarity: rand_neg_sim = similarity from a randomly chosen negative (group_ids differ)\n",
    "      - Hard negative similarity: hard_neg_sim = max { sim(embeddings[i], embeddings[j]) : \n",
    "                                                      group_ids[j] != group_ids[i] }\n",
    "      - Blended negative similarity: blended_neg = (1 - alpha) * rand_neg_sim + alpha * hard_neg_sim\n",
    "      \n",
    "    The loss per anchor is then:\n",
    "         loss_i = - log( exp(blended_pos/temperature) / [ exp(blended_pos/temperature) + exp(blended_neg/temperature) ] )\n",
    "    \n",
    "    Anchors that lack any valid positives or negatives contribute 0.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape (N, D) (raw outputs; they will be normalized inside).\n",
    "        pos_indices: 1D Tensor (length N) giving the index of the provided positive for each anchor.\n",
    "        group_ids: 1D Tensor (length N) of group identifiers.\n",
    "        temperature: Temperature scaling factor.\n",
    "        alpha: Blending parameter between random and hard mining (0: use only provided/random, 1: use only hard).\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss (mean over anchors).\n",
    "    \"\"\"\n",
    "    # Normalize embeddings so that cosine similarity is simply the dot product.\n",
    "    norm_emb = F.normalize(embeddings, p=2, dim=1)  # shape (N, D)\n",
    "    # Compute full cosine similarity matrix.\n",
    "    sim_matrix = norm_emb @ norm_emb.t()  # shape (N, N)\n",
    "    N = embeddings.size(0)\n",
    "    idx = torch.arange(N, device=embeddings.device)\n",
    "    \n",
    "    # --- Positives ---\n",
    "    # Provided positive similarity.\n",
    "    pos_sim_orig = sim_matrix[idx, pos_indices.view(-1)]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard positive: consider all other indices in the same group.\n",
    "    pos_mask = (group_ids.unsqueeze(1) == group_ids.unsqueeze(0))\n",
    "    # Exclude self (set diagonal to False)\n",
    "    pos_mask = pos_mask & ~torch.eye(N, dtype=torch.bool, device=embeddings.device)\n",
    "    # For each anchor, if there are valid positives, take the minimum similarity.\n",
    "    # To do that, copy sim_matrix and set invalid positions to a large number (e.g., 2.0).\n",
    "    sim_matrix_pos = sim_matrix.clone()\n",
    "    sim_matrix_pos[~pos_mask] = 2.0  # cosine similarity <= 1, so 2 is safe.\n",
    "    hard_pos_sim, _ = sim_matrix_pos.min(dim=1)\n",
    "    # If an anchor has no valid positive (should not happen if each group has >= 2 items), fallback to provided.\n",
    "    valid_pos_counts = pos_mask.sum(dim=1)\n",
    "    no_valid_pos = (valid_pos_counts == 0)\n",
    "    hard_pos_sim = torch.where(no_valid_pos, pos_sim_orig, hard_pos_sim)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Blended positive similarity.\n",
    "    blended_pos = pos_sim_orig\n",
    "    \n",
    "    # --- Negatives ---\n",
    "    # Mask for negatives: group_ids differ.\n",
    "    neg_mask = (group_ids.unsqueeze(1) != group_ids.unsqueeze(0))\n",
    "    valid_neg_counts = neg_mask.sum(dim=1)\n",
    "    # For anchors with no valid negatives, we later set loss to 0.\n",
    "    no_valid_neg = (valid_neg_counts == 0)\n",
    "    \n",
    "    # Random negative: for each anchor, select one random index among negatives.\n",
    "    rand_vals = torch.rand(sim_matrix.shape, device=embeddings.device)\n",
    "    rand_vals = rand_vals * neg_mask.float() - (1 - neg_mask.float())\n",
    "    rand_neg_indices = torch.argmax(rand_vals, dim=1)\n",
    "    rand_neg_sim = sim_matrix[idx, rand_neg_indices]\n",
    "    \n",
    "    # Hard negative: among all negatives, choose the one with maximum similarity.\n",
    "    sim_matrix_neg = sim_matrix.masked_fill(~neg_mask, -float('inf'))\n",
    "    hard_neg_sim, _ = sim_matrix_neg.max(dim=1)\n",
    "    # For anchors with no valid negatives, use -1.\n",
    "    hard_neg_sim = torch.where(no_valid_neg, torch.tensor(-1.0, device=embeddings.device), hard_neg_sim)\n",
    "    \n",
    "    # Blended negative similarity.\n",
    "    blended_neg = (1 - alpha) * rand_neg_sim + alpha * hard_neg_sim\n",
    "    \n",
    "    loss = -torch.log(\n",
    "    torch.exp(blended_pos / temperature) / \n",
    "    (torch.exp(blended_pos / temperature) + torch.exp(blended_neg / temperature)))\n",
    "    # For anchors with no valid negatives, set loss to 0.\n",
    "    loss = loss.masked_fill(no_valid_neg, 0.0)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def contrastive_loss_curriculum(embeddings, pos_indices, group_ids, temperature=0.1, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Curriculum loss that uses both positive and negative blending.\n",
    "    \n",
    "    Delegates to contrastive_loss_curriculum_both.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape (N, D).\n",
    "        pos_indices: 1D Tensor (length N).\n",
    "        group_ids: 1D Tensor (length N).\n",
    "        temperature: Temperature scaling factor.\n",
    "        alpha: Blending parameter.\n",
    "        \n",
    "    Returns:\n",
    "        Scalar loss.\n",
    "    \"\"\"\n",
    "    return contrastive_loss_curriculum_both(embeddings, pos_indices, group_ids, temperature, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87f2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new(train_loader, model, optimizer, device, k_value, alpha):\n",
    "    model.train()\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    for data in tqdm(train_loader, desc=\"Training\"):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert data.assoc to tensor if needed.\n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        # Partition batch by event.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = torch.zeros(1, device=device)\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            loss_event = contrastive_loss_curriculum(event_embeddings, event_pos_indices,\n",
    "                                                     event_group_ids, temperature=0.1, alpha=alpha)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        loss = loss_event_total / len(counts)\n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device, k_value):\n",
    "    model.eval()\n",
    "    total_loss = torch.zeros(1, device=device)\n",
    "    for data in tqdm(test_loader, desc=\"Validation\"):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "        \n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = torch.zeros(1, device=device)\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            loss_event = contrastive_loss_curriculum(event_embeddings, event_pos_indices,\n",
    "                                                     event_group_ids, temperature=0.1, alpha=1)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        total_loss += loss_event_total / len(counts)\n",
    "    return total_loss / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7daaab6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (2839019873.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3119007/2839019873.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    print(f\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Load DataLoader with current batch_size\n",
    "train_loader = DataLoader(data_train, batch_size=BS, shuffle=True, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Train and evaluate the model for the specified number of epochs\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Store train and validation losses for all epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_dir = '/vols/cms/mm1221/hgcal/elec5New/LC/NegativeMining/results/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # For epochs 1 to 150, gradually increase alpha from 0 to 1.\n",
    "    # From epoch 151 onward, set alpha = 1 (fully hard negatives).\n",
    "    if epoch < 10:\n",
    "        alpha = (epoch + 1) / 10.0\n",
    "    else:\n",
    "        alpha = 1.0\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Alpha: {alpha:.2f}\")\n",
    "    train_loss = train_new(train_loader, model, optimizer, device, k_value, alpha)\n",
    "    val_loss = test_new(val_loader, model, device, k_value)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model if validation loss improves.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Save intermediate checkpoint.\n",
    "    state_dicts = {'model': model.state_dict(),\n",
    "                   'opt': optimizer.state_dict(),\n",
    "                   'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch+1}.pt'))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.item():.8f}, Validation Loss: {val_loss.item():.8f}\")\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Save training history.\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, 'continued_training_loss.csv'), index=False)\n",
    "print(f\"Saved loss curves to {os.path.join(output_dir, 'continued_training_loss.csv')}\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "print(\"Training complete. Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850b145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
