{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "731a40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import os.path as osp  # This defines 'osp'\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Loads trackster-level features and associations for positive/negative edge creation.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading tracksters data\")\n",
    "\n",
    "\n",
    "        for path in tqdm(self.raw_paths):\n",
    "            print(path)\n",
    "            \n",
    "            tracksters_path = find_highest_branch(path, 'tracksters')\n",
    "            associations_path = find_highest_branch(path, 'associations')\n",
    "            simtrack = find_highest_branch(path, 'simtrackstersCP')\n",
    "            # Load tracksters features in chunks\n",
    "            for array in uproot.iterate(\n",
    "                f\"{path}:{tracksters_path}\",\n",
    "                [\n",
    "                    \"time\", \"raw_energy\",\n",
    "                    \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \n",
    "                    \"barycenter_eta\", \"barycenter_phi\",\n",
    "                    \"EV1\", \"EV2\", \"EV3\",\n",
    "                    \"eVector0_x\", \"eVector0_y\", \"eVector0_z\",\n",
    "                    \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\", \"raw_pt\", \"vertices_time\"\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                tmp_time = array[\"time\"]\n",
    "                tmp_raw_energy = array[\"raw_energy\"]\n",
    "                tmp_bx = array[\"barycenter_x\"]\n",
    "                tmp_by = array[\"barycenter_y\"]\n",
    "                tmp_bz = array[\"barycenter_z\"]\n",
    "                tmp_beta = array[\"barycenter_eta\"]\n",
    "                tmp_bphi = array[\"barycenter_phi\"]\n",
    "                tmp_EV1 = array[\"EV1\"]\n",
    "                tmp_EV2 = array[\"EV2\"]\n",
    "                tmp_EV3 = array[\"EV3\"]\n",
    "                tmp_eV0x = array[\"eVector0_x\"]\n",
    "                tmp_eV0y = array[\"eVector0_y\"]\n",
    "                tmp_eV0z = array[\"eVector0_z\"]\n",
    "                tmp_sigma1 = array[\"sigmaPCA1\"]\n",
    "                tmp_sigma2 = array[\"sigmaPCA2\"]\n",
    "                tmp_sigma3 = array[\"sigmaPCA3\"]\n",
    "                tmp_pt = array[\"raw_pt\"]\n",
    "                tmp_vt = array[\"vertices_time\"]\n",
    "                \n",
    "                \n",
    "                vert_array = []\n",
    "                for vert_chunk in uproot.iterate(\n",
    "                    f\"{path}:{simtrack}\",\n",
    "                    [\"barycenter_x\"],\n",
    "                ):\n",
    "                    vert_array = vert_chunk[\"barycenter_x\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "\n",
    "                # Now load the associations for the same events/chunk\n",
    "                # 'tsCLUE3D_recoToSim_CP' gives association arrays like [[1,0],[0,1],...]\n",
    "                # Make sure we read from the same events\n",
    "                tmp_array = []\n",
    "                score_array = []\n",
    "                for assoc_chunk in uproot.iterate(\n",
    "                    f\"{path}:{associations_path}\",\n",
    "                    [\"tsCLUE3D_recoToSim_CP\", \"tsCLUE3D_recoToSim_CP_score\"],\n",
    "                ):\n",
    "                    tmp_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP\"]\n",
    "                    score_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP_score\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in vert_array:\n",
    "                    if len(e) >= 2:\n",
    "                        skim_mask.append(True)\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "\n",
    "\n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                tmp_pt = tmp_pt[skim_mask]\n",
    "                tmp_vt = tmp_vt[skim_mask]\n",
    "                score_array = score_array[skim_mask]\n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in tmp_array:\n",
    "                    if 2 <= len(e):\n",
    "                        skim_mask.append(True)\n",
    "\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                        \n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                tmp_pt = tmp_pt[skim_mask]\n",
    "                tmp_vt = tmp_vt[skim_mask]\n",
    "                score_array = score_array[skim_mask]\n",
    "\n",
    "                \n",
    "                # Concatenate or initialize storage\n",
    "                if counter == 0:\n",
    "                    self.time = tmp_time\n",
    "                    self.raw_energy = tmp_raw_energy\n",
    "                    self.bx = tmp_bx\n",
    "                    self.by = tmp_by\n",
    "                    self.bz = tmp_bz\n",
    "                    self.beta = tmp_beta\n",
    "                    self.bphi = tmp_bphi\n",
    "                    self.EV1 = tmp_EV1\n",
    "                    self.EV2 = tmp_EV2\n",
    "                    self.EV3 = tmp_EV3\n",
    "                    self.eV0x = tmp_eV0x\n",
    "                    self.eV0y = tmp_eV0y\n",
    "                    self.eV0z = tmp_eV0z\n",
    "                    self.sigma1 = tmp_sigma1\n",
    "                    self.sigma2 = tmp_sigma2\n",
    "                    self.sigma3 = tmp_sigma3\n",
    "                    self.assoc = tmp_array\n",
    "                    self.pt = tmp_pt\n",
    "                    self.vt = tmp_vt\n",
    "                    self.score = score_array\n",
    "                else:\n",
    "                    self.time = ak.concatenate((self.time, tmp_time))\n",
    "                    self.raw_energy = ak.concatenate((self.raw_energy, tmp_raw_energy))\n",
    "                    self.bx = ak.concatenate((self.bx, tmp_bx))\n",
    "                    self.by = ak.concatenate((self.by, tmp_by))\n",
    "                    self.bz = ak.concatenate((self.bz, tmp_bz))\n",
    "                    self.beta = ak.concatenate((self.beta, tmp_beta))\n",
    "                    self.bphi = ak.concatenate((self.bphi, tmp_bphi))\n",
    "                    self.EV1 = ak.concatenate((self.EV1, tmp_EV1))\n",
    "                    self.EV2 = ak.concatenate((self.EV2, tmp_EV2))\n",
    "                    self.EV3 = ak.concatenate((self.EV3, tmp_EV3))\n",
    "                    self.eV0x = ak.concatenate((self.eV0x, tmp_eV0x))\n",
    "                    self.eV0y = ak.concatenate((self.eV0y, tmp_eV0y))\n",
    "                    self.eV0z = ak.concatenate((self.eV0z, tmp_eV0z))\n",
    "                    self.sigma1 = ak.concatenate((self.sigma1, tmp_sigma1))\n",
    "                    self.sigma2 = ak.concatenate((self.sigma2, tmp_sigma2))\n",
    "                    self.sigma3 = ak.concatenate((self.sigma3, tmp_sigma3))\n",
    "                    self.assoc = ak.concatenate((self.assoc, tmp_array))\n",
    "                    self.pt = ak.concatenate((self.pt, tmp_pt))\n",
    "                    self.vt = ak.concatenate((self.vt, tmp_vt))\n",
    "                    self.score = ak.concatenate((self.score, score_array))\n",
    "\n",
    "                counter += len(tmp_bx)\n",
    "                if counter >= max_events:\n",
    "                    print(f\"Reached {max_events} events!\")\n",
    "                    break\n",
    "            if counter >= max_events:\n",
    "                break\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            f'Dataset not found. Please download it from {self.url} and move all '\n",
    "            f'*.root files to {self.raw_dir}')\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Finds the maximum index and returns a 1D array listing the group for each index.\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx2 in indices:\n",
    "                    reconstructed[idx2] = value\n",
    "            return reconstructed\n",
    "\n",
    "        # Extract per-event arrays\n",
    "        event_time = self.time[idx]\n",
    "        event_raw_energy = self.raw_energy[idx]\n",
    "        event_bx = self.bx[idx]\n",
    "        event_by = self.by[idx]\n",
    "        event_bz = self.bz[idx]\n",
    "        event_beta = self.beta[idx]\n",
    "        event_bphi = self.bphi[idx]\n",
    "        event_EV1 = self.EV1[idx]\n",
    "        event_EV2 = self.EV2[idx]\n",
    "        event_EV3 = self.EV3[idx]\n",
    "        event_eV0x = self.eV0x[idx]\n",
    "        event_eV0y = self.eV0y[idx]\n",
    "        event_eV0z = self.eV0z[idx]\n",
    "        event_sigma1 = self.sigma1[idx]\n",
    "        event_sigma2 = self.sigma2[idx]\n",
    "        event_sigma3 = self.sigma3[idx]\n",
    "        event_assoc = self.assoc[idx]      # associations; e.g. [0, 4, 3, 2]\n",
    "        event_pt = self.pt[idx]\n",
    "        event_vt = self.vt[idx]\n",
    "        event_score = self.score[idx]      # scores; e.g. [0.000, 0.281, 1.0, 1.0]\n",
    "\n",
    "        # Convert each to NumPy\n",
    "        event_time = np.array(event_time)\n",
    "        event_raw_energy = np.array(event_raw_energy)\n",
    "        event_bx = np.array(event_bx)\n",
    "        event_by = np.array(event_by)\n",
    "        event_bz = np.array(event_bz)\n",
    "        event_beta = np.array(event_beta)\n",
    "        event_bphi = np.array(event_bphi)\n",
    "        event_EV1 = np.array(event_EV1)\n",
    "        event_EV2 = np.array(event_EV2)\n",
    "        event_EV3 = np.array(event_EV3)\n",
    "        event_eV0x = np.array(event_eV0x)\n",
    "        event_eV0y = np.array(event_eV0y)\n",
    "        event_eV0z = np.array(event_eV0z)\n",
    "        event_sigma1 = np.array(event_sigma1)\n",
    "        event_sigma2 = np.array(event_sigma2)\n",
    "        event_sigma3 = np.array(event_sigma3)\n",
    "        event_assoc = np.array(event_assoc)   # shape (N, ?) with nested arrays\n",
    "        event_pt = np.array(event_pt)\n",
    "        event_score = np.array(event_score)     # shape (N, ?) with nested arrays\n",
    "\n",
    "\n",
    "        # Stack trackster features into x.\n",
    "        flat_feats = np.column_stack((\n",
    "            event_bx, event_by, event_bz, event_raw_energy,\n",
    "            event_beta, event_bphi,\n",
    "            event_EV1, event_EV2, event_EV3,\n",
    "            event_eV0x, event_eV0y, event_eV0z,\n",
    "            event_sigma1, event_sigma2, event_sigma3,\n",
    "            event_pt\n",
    "        ))\n",
    "        x = torch.from_numpy(flat_feats).float()\n",
    "\n",
    "        # Convert associations & scores to tensors.\n",
    "        links_tensor = torch.from_numpy(event_assoc.astype(np.int64))\n",
    "        scores_tensor = torch.from_numpy(event_score).float()\n",
    "\n",
    "        # --- Truncate or pad each tensor to 4 columns ---\n",
    "        def ensure_four_columns(tensor):\n",
    "            if tensor.ndim == 1:\n",
    "                tensor = tensor.unsqueeze(1)\n",
    "            nrow, ncol = tensor.shape\n",
    "            if ncol > 4:\n",
    "                tensor = tensor[:, :4]\n",
    "            elif ncol < 4:\n",
    "                last_col = tensor[:, -1].unsqueeze(1)\n",
    "                repeat_count = 4 - ncol\n",
    "                repeated = last_col.repeat(1, repeat_count)\n",
    "                tensor = torch.cat([tensor, repeated], dim=1)\n",
    "            return tensor\n",
    "\n",
    "        scores_tensor = ensure_four_columns(scores_tensor)\n",
    "        links_tensor = ensure_four_columns(links_tensor)\n",
    "\n",
    "        \n",
    "\n",
    "        # Return the Data object with all fields.\n",
    "        return Data(\n",
    "            x=x,\n",
    "            scores=scores_tensor,\n",
    "            links=links_tensor\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e6b3a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/mix/train/raw/18k.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [01:15<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10000 events!\n",
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/mix/test/raw/test.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [01:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10000 events!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/mix/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/mix/test/\"\n",
    "data_train = CCV1(ipath, max_events=10000, inp='train')\n",
    "data_val = CCV1(vpath, max_events=10000, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1aa8574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss_fractional(embeddings, groups, scores, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Skips anchors that have no positive edges at all (pos_weight sums to 0).\n",
    "    \"\"\"\n",
    "    device = embeddings.device\n",
    "    N, D = embeddings.shape\n",
    "\n",
    "    # 1) Cosine similarity.\n",
    "    norm_emb = F.normalize(embeddings, p=2, dim=1)\n",
    "    sim_matrix = norm_emb @ norm_emb.t()\n",
    "\n",
    "    # 2) Calculate \"shared energy\" for each pair (N x N).\n",
    "    energy_i = (1.0 - scores).unsqueeze(1)  # shape (N, 1, num_slots)\n",
    "    energy_j = (1.0 - scores).unsqueeze(0)  # shape (1, N, num_slots)\n",
    "\n",
    "    groups_i = groups.unsqueeze(1)  # (N, 1, num_slots)\n",
    "    groups_j = groups.unsqueeze(0)  # (1, N, num_slots)\n",
    "    match = (groups_i.unsqueeze(-1) == groups_j.unsqueeze(-2)).float()\n",
    "    \n",
    "    min_energy = torch.min(energy_i.unsqueeze(-1), energy_j.unsqueeze(-2))\n",
    "    shared_energy = (match * min_energy).sum(dim=(-1, -2))  # shape (N, N)\n",
    "\n",
    "    # 3) Positive vs negative masks; compute weights.\n",
    "    pos_mask = (shared_energy >= 0.5)\n",
    "    neg_mask = ~pos_mask\n",
    "\n",
    "    pos_weight = torch.zeros_like(shared_energy, device=device)\n",
    "    neg_weight = torch.zeros_like(shared_energy, device=device)\n",
    "\n",
    "    pos_weight[pos_mask] = 2.0 * (shared_energy[pos_mask] - 0.5)\n",
    "    neg_weight[neg_mask] = 2.0 * (0.5 - shared_energy[neg_mask])\n",
    "\n",
    "    # Exclude self-similarity.\n",
    "    pos_weight.fill_diagonal_(0)\n",
    "    neg_weight.fill_diagonal_(0)\n",
    "\n",
    "    # 4) numerator / denominator for each anchor.\n",
    "    exp_sim = torch.exp(sim_matrix / temperature)\n",
    "    numerator = (pos_weight * exp_sim).sum(dim=1)  # (N,)\n",
    "    denominator = ((pos_weight + neg_weight) * exp_sim).sum(dim=1)  # (N,)\n",
    "\n",
    "    # 5) Skip anchors that have no positives at all.\n",
    "    #    We do that by checking if sum of pos_weight across j == 0.\n",
    "    #    If pos_weight[i,:] is all zero, then that anchor i has no positive edges.\n",
    "    anchor_has_pos = (pos_weight.sum(dim=1) > 0)  # shape (N,)\n",
    "\n",
    "    # 6) Compute contrastive loss only on valid anchors.\n",
    "    valid_numerator = numerator[anchor_has_pos]\n",
    "    valid_denominator = denominator[anchor_has_pos]\n",
    "\n",
    "    # Avoid log(0) by checking if denominator is zero (unlikely, but can happen).\n",
    "    # If you want to allow it safely, add a small epsilon: valid_denominator + eps\n",
    "    loss_per_anchor = -torch.log(valid_numerator / valid_denominator)\n",
    "\n",
    "    # If *all* anchors had no positives, we'd get an empty tensor here => .mean() = nan.\n",
    "    # Decide how you want to handle the \"all-skipped\" edge case.\n",
    "    if loss_per_anchor.numel() == 0:\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "    return loss_per_anchor.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "# Updated Training and Testing Functions\n",
    "#################################\n",
    "\n",
    "def train_new(train_loader, model, optimizer, device, k_value, alpha_unused=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    for data in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "\n",
    "        # Build k-NN graph using first 3 features.\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        # Partition batch by event.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_scores = data.scores[start_idx:end_idx]\n",
    "            event_links = data.links[start_idx:end_idx]\n",
    "            \n",
    "            loss_event = contrastive_loss_fractional(\n",
    "                embeddings=event_embeddings,\n",
    "                groups = event_links,\n",
    "                scores =event_scores,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        loss = loss_event_total / len(counts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * embeddings.size(0)\n",
    "        n_samples += embeddings.size(0)\n",
    "    return total_loss / n_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device, k_value, alpha_unused=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        \n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_scores = data.scores[start_idx:end_idx]\n",
    "            event_links = data.links[start_idx:end_idx]\n",
    "            loss_event = contrastive_loss_fractional(\n",
    "                embeddings=event_embeddings,\n",
    "                groups = event_links,\n",
    "                scores =event_scores,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        total_loss += loss_event_total / len(counts)\n",
    "        n_samples += embeddings.size(0)\n",
    "    return total_loss / n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e84190a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "\n",
    "\n",
    "class CustomStaticEdgeConv(nn.Module):\n",
    "    def __init__(self, nn_module):\n",
    "        super(CustomStaticEdgeConv, self).__init__()\n",
    "        self.nn_module = nn_module\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features of shape (N, F).\n",
    "            edge_index (torch.Tensor): Predefined edges [2, E], where E is the number of edges.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Node features after static edge aggregation.\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # Extract row (source) and col (target) nodes\n",
    "        x_center = x[row]\n",
    "        x_neighbor = x[col]\n",
    "\n",
    "        # Compute edge features (relative)\n",
    "        edge_features = torch.cat([x_center, x_neighbor - x_center], dim=-1)\n",
    "        edge_features = self.nn_module(edge_features)\n",
    "\n",
    "        # Aggregate features back to nodes\n",
    "        num_nodes = x.size(0)\n",
    "        node_features = torch.zeros(num_nodes, edge_features.size(-1), device=x.device)\n",
    "        node_features.index_add_(0, row, edge_features)\n",
    "\n",
    "        # Normalization (Divide by node degrees)\n",
    "        counts = torch.bincount(row, minlength=num_nodes).clamp(min=1).view(-1, 1)\n",
    "        node_features = node_features / counts\n",
    "\n",
    "        return node_features\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8, heads=4):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with alternating StaticEdgeConv and GAT layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of convolutional layers (both StaticEdgeConv and GAT).\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            heads (int): Number of attention heads in GAT layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(16, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers, alternating between StaticEdgeConv and GAT\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer_idx in range(num_layers):\n",
    "            conv = CustomStaticEdgeConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                    nn.ELU(),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.Dropout(p=dropout)\n",
    "                )\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 15).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply convolutional layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            feats = conv(feats, edge_index) + feats  # Residual connection\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce7937dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating model...\n",
      "Loading data...\n",
      "Starting full training with curriculum for hard negative mining...\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                      | 306/16214 [00:06<05:22, 49.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3704105/3018271829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3704105/3404448776.py\u001b[0m in \u001b[0;36mtrain_new\u001b[0;34m(train_loader, model, optimizer, device, k_value, alpha_unused)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     21\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_graphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inc_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minc_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mpropobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpropobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpropobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpropobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mpropobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Instantiating model...\")\n",
    "# Instantiate model.\n",
    "\n",
    "# Set device.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Loading data...\")\n",
    "model = Net(\n",
    "    hidden_dim=128,\n",
    "    num_layers=3,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=128\n",
    ").to(device)\n",
    "\n",
    "k_value = 24\n",
    "BS = 1\n",
    "\n",
    "# Setup optimizer and scheduler.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Create DataLoaders.\n",
    "train_loader = DataLoader(data_train, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Setup output directory.\n",
    "output_dir = '/vols/cms/mm1221/hgcal/Mixed/Track/Fraction/test/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "patience = 300\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "print(\"Starting full training with curriculum for hard negative mining...\")\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # For epochs 1 to 150, gradually increase alpha from 0 to 1.\n",
    "    # From epoch 151 onward, set alpha = 1 (fully hard negatives).\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train_new(train_loader, model, optimizer, device, k_value)\n",
    "    val_loss = test_new(val_loader, model, device, k_value)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save best model if validation loss improves.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Save intermediate checkpoint.\n",
    "    state_dicts = {'model': model.state_dict(),\n",
    "                   'opt': optimizer.state_dict(),\n",
    "                   'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch+1}.pt'))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.item():.8f}, Validation Loss: {val_loss.item():.8f}\")\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Save training history.\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "results_df.to_csv(os.path.join(output_dir, 'continued_training_loss.csv'), index=False)\n",
    "print(f\"Saved loss curves to {os.path.join(output_dir, 'continued_training_loss.csv')}\")\n",
    "\n",
    "# Save final model.\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pt'))\n",
    "print(\"Training complete. Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc36478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
