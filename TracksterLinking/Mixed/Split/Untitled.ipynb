{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c651a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import os.path as osp  # This defines 'osp'\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Loads trackster-level features and associations for positive/negative edge creation.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading tracksters data\")\n",
    "\n",
    "\n",
    "        for path in tqdm(self.raw_paths):\n",
    "            print(path)\n",
    "            \n",
    "            tracksters_path = find_highest_branch(path, 'tracksters')\n",
    "            associations_path = find_highest_branch(path, 'associations')\n",
    "            simtrack = find_highest_branch(path, 'simtrackstersCP')\n",
    "            # Load tracksters features in chunks\n",
    "            for array in uproot.iterate(\n",
    "                f\"{path}:{tracksters_path}\",\n",
    "                [\n",
    "                    \"time\", \"raw_energy\",\n",
    "                    \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \n",
    "                    \"barycenter_eta\", \"barycenter_phi\",\n",
    "                    \"EV1\", \"EV2\", \"EV3\",\n",
    "                    \"eVector0_x\", \"eVector0_y\", \"eVector0_z\",\n",
    "                    \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\", \"raw_pt\", \"vertices_time\"\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                tmp_time = array[\"time\"]\n",
    "                tmp_raw_energy = array[\"raw_energy\"]\n",
    "                tmp_bx = array[\"barycenter_x\"]\n",
    "                tmp_by = array[\"barycenter_y\"]\n",
    "                tmp_bz = array[\"barycenter_z\"]\n",
    "                tmp_beta = array[\"barycenter_eta\"]\n",
    "                tmp_bphi = array[\"barycenter_phi\"]\n",
    "                tmp_EV1 = array[\"EV1\"]\n",
    "                tmp_EV2 = array[\"EV2\"]\n",
    "                tmp_EV3 = array[\"EV3\"]\n",
    "                tmp_eV0x = array[\"eVector0_x\"]\n",
    "                tmp_eV0y = array[\"eVector0_y\"]\n",
    "                tmp_eV0z = array[\"eVector0_z\"]\n",
    "                tmp_sigma1 = array[\"sigmaPCA1\"]\n",
    "                tmp_sigma2 = array[\"sigmaPCA2\"]\n",
    "                tmp_sigma3 = array[\"sigmaPCA3\"]\n",
    "                tmp_pt = array[\"raw_pt\"]\n",
    "                tmp_vt = array[\"vertices_time\"]\n",
    "                \n",
    "                \n",
    "                vert_array = []\n",
    "                for vert_chunk in uproot.iterate(\n",
    "                    f\"{path}:{simtrack}\",\n",
    "                    [\"barycenter_x\"],\n",
    "                ):\n",
    "                    vert_array = vert_chunk[\"barycenter_x\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "\n",
    "                # Now load the associations for the same events/chunk\n",
    "                # 'tsCLUE3D_recoToSim_CP' gives association arrays like [[1,0],[0,1],...]\n",
    "                # Make sure we read from the same events\n",
    "                tmp_array = []\n",
    "                score_array = []\n",
    "                for assoc_chunk in uproot.iterate(\n",
    "                    f\"{path}:{associations_path}\",\n",
    "                    [\"tsCLUE3D_recoToSim_CP\", \"tsCLUE3D_recoToSim_CP_score\"],\n",
    "                ):\n",
    "                    tmp_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP\"]\n",
    "                    score_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP_score\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in vert_array:\n",
    "                    if len(e) >= 2:\n",
    "                        skim_mask.append(True)\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "\n",
    "\n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                tmp_pt = tmp_pt[skim_mask]\n",
    "                tmp_vt = tmp_vt[skim_mask]\n",
    "                score_array = score_array[skim_mask]\n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in tmp_array:\n",
    "                    if 2 <= len(e):\n",
    "                        skim_mask.append(True)\n",
    "\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                        \n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                tmp_pt = tmp_pt[skim_mask]\n",
    "                tmp_vt = tmp_vt[skim_mask]\n",
    "                score_array = score_array[skim_mask]\n",
    "\n",
    "                \n",
    "                # Concatenate or initialize storage\n",
    "                if counter == 0:\n",
    "                    self.time = tmp_time\n",
    "                    self.raw_energy = tmp_raw_energy\n",
    "                    self.bx = tmp_bx\n",
    "                    self.by = tmp_by\n",
    "                    self.bz = tmp_bz\n",
    "                    self.beta = tmp_beta\n",
    "                    self.bphi = tmp_bphi\n",
    "                    self.EV1 = tmp_EV1\n",
    "                    self.EV2 = tmp_EV2\n",
    "                    self.EV3 = tmp_EV3\n",
    "                    self.eV0x = tmp_eV0x\n",
    "                    self.eV0y = tmp_eV0y\n",
    "                    self.eV0z = tmp_eV0z\n",
    "                    self.sigma1 = tmp_sigma1\n",
    "                    self.sigma2 = tmp_sigma2\n",
    "                    self.sigma3 = tmp_sigma3\n",
    "                    self.assoc = tmp_array\n",
    "                    self.pt = tmp_pt\n",
    "                    self.vt = tmp_vt\n",
    "                    self.score = score_array\n",
    "                else:\n",
    "                    self.time = ak.concatenate((self.time, tmp_time))\n",
    "                    self.raw_energy = ak.concatenate((self.raw_energy, tmp_raw_energy))\n",
    "                    self.bx = ak.concatenate((self.bx, tmp_bx))\n",
    "                    self.by = ak.concatenate((self.by, tmp_by))\n",
    "                    self.bz = ak.concatenate((self.bz, tmp_bz))\n",
    "                    self.beta = ak.concatenate((self.beta, tmp_beta))\n",
    "                    self.bphi = ak.concatenate((self.bphi, tmp_bphi))\n",
    "                    self.EV1 = ak.concatenate((self.EV1, tmp_EV1))\n",
    "                    self.EV2 = ak.concatenate((self.EV2, tmp_EV2))\n",
    "                    self.EV3 = ak.concatenate((self.EV3, tmp_EV3))\n",
    "                    self.eV0x = ak.concatenate((self.eV0x, tmp_eV0x))\n",
    "                    self.eV0y = ak.concatenate((self.eV0y, tmp_eV0y))\n",
    "                    self.eV0z = ak.concatenate((self.eV0z, tmp_eV0z))\n",
    "                    self.sigma1 = ak.concatenate((self.sigma1, tmp_sigma1))\n",
    "                    self.sigma2 = ak.concatenate((self.sigma2, tmp_sigma2))\n",
    "                    self.sigma3 = ak.concatenate((self.sigma3, tmp_sigma3))\n",
    "                    self.assoc = ak.concatenate((self.assoc, tmp_array))\n",
    "                    self.pt = ak.concatenate((self.pt, tmp_pt))\n",
    "                    self.vt = ak.concatenate((self.vt, tmp_vt))\n",
    "                    self.score = ak.concatenate((self.score, score_array))\n",
    "\n",
    "                counter += len(tmp_bx)\n",
    "                if counter >= max_events:\n",
    "                    print(f\"Reached {max_events} events!\")\n",
    "                    break\n",
    "            if counter >= max_events:\n",
    "                break\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            f'Dataset not found. Please download it from {self.url} and move all '\n",
    "            f'*.root files to {self.raw_dir}')\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "\n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Finds the maximum index and returns a 1D array listing the group for each index.\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx2 in indices:\n",
    "                    reconstructed[idx2] = value\n",
    "            return reconstructed\n",
    "\n",
    "        # Extract per-event arrays\n",
    "        event_time = self.time[idx]\n",
    "        event_raw_energy = self.raw_energy[idx]\n",
    "        event_bx = self.bx[idx]\n",
    "        event_by = self.by[idx]\n",
    "        event_bz = self.bz[idx]\n",
    "        event_beta = self.beta[idx]\n",
    "        event_bphi = self.bphi[idx]\n",
    "        event_EV1 = self.EV1[idx]\n",
    "        event_EV2 = self.EV2[idx]\n",
    "        event_EV3 = self.EV3[idx]\n",
    "        event_eV0x = self.eV0x[idx]\n",
    "        event_eV0y = self.eV0y[idx]\n",
    "        event_eV0z = self.eV0z[idx]\n",
    "        event_sigma1 = self.sigma1[idx]\n",
    "        event_sigma2 = self.sigma2[idx]\n",
    "        event_sigma3 = self.sigma3[idx]\n",
    "        event_assoc = self.assoc[idx]      # associations; e.g. [0, 4, 3, 2]\n",
    "        event_pt = self.pt[idx]\n",
    "        event_vt = self.vt[idx]\n",
    "        event_score = self.score[idx]      # scores; e.g. [0.000, 0.281, 1.0, 1.0]\n",
    "\n",
    "        # Convert each to NumPy\n",
    "        event_time = np.array(event_time)\n",
    "        event_raw_energy = np.array(event_raw_energy)\n",
    "        event_bx = np.array(event_bx)\n",
    "        event_by = np.array(event_by)\n",
    "        event_bz = np.array(event_bz)\n",
    "        event_beta = np.array(event_beta)\n",
    "        event_bphi = np.array(event_bphi)\n",
    "        event_EV1 = np.array(event_EV1)\n",
    "        event_EV2 = np.array(event_EV2)\n",
    "        event_EV3 = np.array(event_EV3)\n",
    "        event_eV0x = np.array(event_eV0x)\n",
    "        event_eV0y = np.array(event_eV0y)\n",
    "        event_eV0z = np.array(event_eV0z)\n",
    "        event_sigma1 = np.array(event_sigma1)\n",
    "        event_sigma2 = np.array(event_sigma2)\n",
    "        event_sigma3 = np.array(event_sigma3)\n",
    "        event_assoc = np.array(event_assoc)   # shape (N, ?) with nested arrays\n",
    "        event_pt = np.array(event_pt)\n",
    "        event_score = np.array(event_score)     # shape (N, ?) with nested arrays\n",
    "\n",
    "\n",
    "        # Stack trackster features into x.\n",
    "        flat_feats = np.column_stack((\n",
    "            event_bx, event_by, event_bz, event_raw_energy,\n",
    "            event_beta, event_bphi,\n",
    "            event_EV1, event_EV2, event_EV3,\n",
    "            event_eV0x, event_eV0y, event_eV0z,\n",
    "            event_sigma1, event_sigma2, event_sigma3,\n",
    "            event_pt\n",
    "        ))\n",
    "        x = torch.from_numpy(flat_feats).float()\n",
    "\n",
    "        # Convert associations & scores to tensors.\n",
    "        links_tensor = torch.from_numpy(event_assoc.astype(np.int64))\n",
    "        scores_tensor = torch.from_numpy(event_score).float()\n",
    "\n",
    "        # --- Truncate or pad each tensor to 4 columns ---\n",
    "        def ensure_four_columns(tensor):\n",
    "            if tensor.ndim == 1:\n",
    "                tensor = tensor.unsqueeze(1)\n",
    "            nrow, ncol = tensor.shape\n",
    "            if ncol > 4:\n",
    "                tensor = tensor[:, :4]\n",
    "            elif ncol < 4:\n",
    "                last_col = tensor[:, -1].unsqueeze(1)\n",
    "                repeat_count = 4 - ncol\n",
    "                repeated = last_col.repeat(1, repeat_count)\n",
    "                tensor = torch.cat([tensor, repeated], dim=1)\n",
    "            return tensor\n",
    "\n",
    "        scores_tensor = ensure_four_columns(scores_tensor)\n",
    "        links_tensor = ensure_four_columns(links_tensor)\n",
    "\n",
    "        \n",
    "\n",
    "        # Return the Data object with all fields.\n",
    "        return Data(\n",
    "            x=x,\n",
    "            scores=scores_tensor,\n",
    "            links=links_tensor\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d450990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/mix/train/raw/18k.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                             | 0/3 [01:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10000 events!\n",
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/mix/test/raw/test.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0/1 [00:59<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 10000 events!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ipath = \"/vols/cms/mm1221/Data/mix/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/mix/test/\"\n",
    "data_train = CCV1(ipath, max_events=10000, inp='train')\n",
    "data_val = CCV1(vpath, max_events=10000, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b0cd3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating and loading pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [01:25<00:00,  5.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 422/422 [01:08<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.485464, Val Loss: 0.437143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [01:23<00:00,  6.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 422/422 [01:05<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train Loss: 0.455845, Val Loss: 0.437396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▌                                                                                                                                        | 33/507 [00:05<01:14,  6.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449945/1544409036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_split_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_split_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1449945/1544409036.py\u001b[0m in \u001b[0;36mtrain_split_classifier\u001b[0;34m(train_loader, pretrained_model, split_model, optimizer, device, k_value)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 or (isinstance(idx, np.ndarray) and np.isscalar(idx))):\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1449945/562446164.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Extract per-event arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mevent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mevent_raw_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_energy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mevent_bx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mto\u001b[0m \u001b[0madd\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \"\"\"\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "#####################################\n",
    "# Define your existing Net model\n",
    "#####################################\n",
    "\n",
    "class CustomStaticEdgeConv(nn.Module):\n",
    "    def __init__(self, nn_module):\n",
    "        super(CustomStaticEdgeConv, self).__init__()\n",
    "        self.nn_module = nn_module\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (N, F); edge_index: (2, E)\n",
    "        row, col = edge_index  # row: source nodes, col: target nodes\n",
    "        x_center = x[row]\n",
    "        x_neighbor = x[col]\n",
    "\n",
    "        # Compute relative edge features.\n",
    "        edge_features = torch.cat([x_center, x_neighbor - x_center], dim=-1)\n",
    "        edge_features = self.nn_module(edge_features)\n",
    "\n",
    "        # Aggregate back to nodes.\n",
    "        num_nodes = x.size(0)\n",
    "        node_features = torch.zeros(num_nodes, edge_features.size(-1), device=x.device)\n",
    "        node_features.index_add_(0, row, edge_features)\n",
    "\n",
    "        # Normalize by node degree.\n",
    "        counts = torch.bincount(row, minlength=num_nodes).clamp(min=1).view(-1, 1)\n",
    "        node_features = node_features / counts\n",
    "\n",
    "        return node_features\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8, heads=4):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        # Input encoder.\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(16, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Convolutional layers.\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer_idx in range(num_layers):\n",
    "            conv = CustomStaticEdgeConv(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                    nn.ELU(),\n",
    "                    nn.BatchNorm1d(hidden_dim),\n",
    "                    nn.Dropout(p=dropout)\n",
    "                )\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Output layer producing contrastive embeddings.\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Encode inputs.\n",
    "        x_lc_enc = self.lc_encode(x)  # (N, hidden_dim)\n",
    "        feats = x_lc_enc\n",
    "        for conv in self.convs:\n",
    "            feats = conv(feats, edge_index) + feats  # Residual connection\n",
    "        out = self.output(feats)\n",
    "        return out, batch\n",
    "\n",
    "#####################################\n",
    "# Define the Split Classifier (New Model)\n",
    "#####################################\n",
    "\n",
    "class SplitClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=32):\n",
    "        super(SplitClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output: single logit.\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # (N, 1)\n",
    "\n",
    "#####################################\n",
    "# Load Pretrained GNN Model\n",
    "#####################################\n",
    "\n",
    "print(\"Instantiating and loading pretrained model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Instantiate your pretrained model.\n",
    "pretrained_model = Net(\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=16\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint (update the path as needed).\n",
    "checkpoint_path = '/vols/cms/mm1221/hgcal/Mixed/Track/Fraction/runs/best_model.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "pretrained_model.load_state_dict(checkpoint)\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Freeze the pretrained model's parameters.\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#####################################\n",
    "# Instantiate and Setup Split Classifier\n",
    "#####################################\n",
    "\n",
    "embedding_dim = 16  # Must match the contrastive_dim from pretrained_model.\n",
    "split_model = SplitClassifier(in_dim=embedding_dim, hidden_dim=32).to(device)\n",
    "optimizer_split = torch.optim.Adam(split_model.parameters(), lr=1e-3)\n",
    "\n",
    "#####################################\n",
    "# Create DataLoaders (adjust as needed)\n",
    "#####################################\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "# Assume data_train and data_val are defined lists of Data objects.\n",
    "train_loader = DataLoader(data_train, batch_size=32, shuffle=True, follow_batch=['x'])\n",
    "val_loader   = DataLoader(data_val, batch_size=32, shuffle=False, follow_batch=['x'])\n",
    "k_value = 32\n",
    "\n",
    "#####################################\n",
    "# Define Training and Testing Functions for Split Classifier\n",
    "#####################################\n",
    "\n",
    "def train_split_classifier(train_loader, pretrained_model, split_model, optimizer, device, k_value):\n",
    "    pretrained_model.eval()  # Use fixed embeddings.\n",
    "    split_model.train()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    for data in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        # Extract embeddings with no gradient.\n",
    "        with torch.no_grad():\n",
    "            embeddings, _ = pretrained_model(data.x, edge_index, data.x_batch)\n",
    "        # Ground truth: a node is split if at least two of its scores are below 0.8.\n",
    "        split_labels = ((data.scores < 0.8).sum(dim=1) >= 2).float()  # (N,)\n",
    "        logits = split_model(embeddings).view(-1)  # (N,)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, split_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * embeddings.size(0)\n",
    "        n_samples += embeddings.size(0)\n",
    "    return total_loss / n_samples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_split_classifier(test_loader, pretrained_model, split_model, device, k_value):\n",
    "    pretrained_model.eval()\n",
    "    split_model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_samples = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        with torch.no_grad():\n",
    "            embeddings, _ = pretrained_model(data.x, edge_index, data.x_batch)\n",
    "        split_labels = ((data.scores < 0.8).sum(dim=1) >= 2).float()\n",
    "        logits = split_model(embeddings).view(-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, split_labels)\n",
    "        total_loss += loss.item() * embeddings.size(0)\n",
    "        n_samples += embeddings.size(0)\n",
    "    return total_loss / n_samples\n",
    "\n",
    "#####################################\n",
    "# Training Loop for the Split Classifier\n",
    "#####################################\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_split_classifier(train_loader, pretrained_model, split_model, optimizer_split, device, k_value)\n",
    "    val_loss = test_split_classifier(val_loader, pretrained_model, split_model, device, k_value)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    # Optionally, save the best model.\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(split_model.state_dict(), os.path.join('/vols/cms/mm1221/hgcal/Mixed/Track/Split/test/', 'split_best_model.pt'))\n",
    "\n",
    "#####################################\n",
    "# Save Final Split Classifier\n",
    "#####################################\n",
    "\n",
    "torch.save(split_model.state_dict(), os.path.join('/vols/cms/mm1221/hgcal/Mixed/Track/Split/test/', 'split_final_model.pt'))\n",
    "print(\"Split classifier training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dafb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
