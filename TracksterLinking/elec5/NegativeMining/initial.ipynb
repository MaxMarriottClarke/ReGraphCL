{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affc3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import os.path as osp  # This defines 'osp'\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Loads trackster-level features and associations for positive/negative edge creation.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading tracksters data\")\n",
    "\n",
    "\n",
    "        for path in tqdm(self.raw_paths):\n",
    "            print(path)\n",
    "            \n",
    "            tracksters_path = find_highest_branch(path, 'tracksters')\n",
    "            associations_path = find_highest_branch(path, 'associations')\n",
    "            simtrack = find_highest_branch(path, 'simtrackstersCP')\n",
    "            # Load tracksters features in chunks\n",
    "            for array in uproot.iterate(\n",
    "                f\"{path}:{tracksters_path}\",\n",
    "                [\n",
    "                    \"time\", \"raw_energy\",\n",
    "                    \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \n",
    "                    \"barycenter_eta\", \"barycenter_phi\",\n",
    "                    \"EV1\", \"EV2\", \"EV3\",\n",
    "                    \"eVector0_x\", \"eVector0_y\", \"eVector0_z\",\n",
    "                    \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\", \"raw_pt\", \"vertices_time\"\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                tmp_time = array[\"time\"]\n",
    "                tmp_raw_energy = array[\"raw_energy\"]\n",
    "                tmp_bx = array[\"barycenter_x\"]\n",
    "                tmp_by = array[\"barycenter_y\"]\n",
    "                tmp_bz = array[\"barycenter_z\"]\n",
    "                tmp_beta = array[\"barycenter_eta\"]\n",
    "                tmp_bphi = array[\"barycenter_phi\"]\n",
    "                tmp_EV1 = array[\"EV1\"]\n",
    "                tmp_EV2 = array[\"EV2\"]\n",
    "                tmp_EV3 = array[\"EV3\"]\n",
    "                tmp_eV0x = array[\"eVector0_x\"]\n",
    "                tmp_eV0y = array[\"eVector0_y\"]\n",
    "                tmp_eV0z = array[\"eVector0_z\"]\n",
    "                tmp_sigma1 = array[\"sigmaPCA1\"]\n",
    "                tmp_sigma2 = array[\"sigmaPCA2\"]\n",
    "                tmp_sigma3 = array[\"sigmaPCA3\"]\n",
    "                tmp_pt = array[\"raw_pt\"]\n",
    "                tmp_vt = array[\"vertices_time\"]\n",
    "                \n",
    "                \n",
    "                vert_array = []\n",
    "                for vert_chunk in uproot.iterate(\n",
    "                    f\"{path}:{simtrack}\",\n",
    "                    [\"barycenter_x\"],\n",
    "                ):\n",
    "                    vert_array = vert_chunk[\"barycenter_x\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "\n",
    "                # Now load the associations for the same events/chunk\n",
    "                # 'tsCLUE3D_recoToSim_CP' gives association arrays like [[1,0],[0,1],...]\n",
    "                # Make sure we read from the same events\n",
    "                tmp_array = []\n",
    "                for assoc_chunk in uproot.iterate(\n",
    "                    f\"{path}:{associations_path}\",\n",
    "                    [\"tsCLUE3D_recoToSim_CP\"],\n",
    "                ):\n",
    "                    tmp_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in vert_array:\n",
    "                    if len(e) >= 1:\n",
    "                        skim_mask.append(True)\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "\n",
    "\n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                tmp_pt = tmp_pt[skim_mask]\n",
    "                tmp_vt = tmp_vt[skim_mask]\n",
    "\n",
    "\n",
    "                \n",
    "                # Concatenate or initialize storage\n",
    "                if counter == 0:\n",
    "                    self.time = tmp_time\n",
    "                    self.raw_energy = tmp_raw_energy\n",
    "                    self.bx = tmp_bx\n",
    "                    self.by = tmp_by\n",
    "                    self.bz = tmp_bz\n",
    "                    self.beta = tmp_beta\n",
    "                    self.bphi = tmp_bphi\n",
    "                    self.EV1 = tmp_EV1\n",
    "                    self.EV2 = tmp_EV2\n",
    "                    self.EV3 = tmp_EV3\n",
    "                    self.eV0x = tmp_eV0x\n",
    "                    self.eV0y = tmp_eV0y\n",
    "                    self.eV0z = tmp_eV0z\n",
    "                    self.sigma1 = tmp_sigma1\n",
    "                    self.sigma2 = tmp_sigma2\n",
    "                    self.sigma3 = tmp_sigma3\n",
    "                    self.assoc = tmp_array\n",
    "                    self.pt = tmp_pt\n",
    "                    self.vt = tmp_vt\n",
    "                else:\n",
    "                    self.time = ak.concatenate((self.time, tmp_time))\n",
    "                    self.raw_energy = ak.concatenate((self.raw_energy, tmp_raw_energy))\n",
    "                    self.bx = ak.concatenate((self.bx, tmp_bx))\n",
    "                    self.by = ak.concatenate((self.by, tmp_by))\n",
    "                    self.bz = ak.concatenate((self.bz, tmp_bz))\n",
    "                    self.beta = ak.concatenate((self.beta, tmp_beta))\n",
    "                    self.bphi = ak.concatenate((self.bphi, tmp_bphi))\n",
    "                    self.EV1 = ak.concatenate((self.EV1, tmp_EV1))\n",
    "                    self.EV2 = ak.concatenate((self.EV2, tmp_EV2))\n",
    "                    self.EV3 = ak.concatenate((self.EV3, tmp_EV3))\n",
    "                    self.eV0x = ak.concatenate((self.eV0x, tmp_eV0x))\n",
    "                    self.eV0y = ak.concatenate((self.eV0y, tmp_eV0y))\n",
    "                    self.eV0z = ak.concatenate((self.eV0z, tmp_eV0z))\n",
    "                    self.sigma1 = ak.concatenate((self.sigma1, tmp_sigma1))\n",
    "                    self.sigma2 = ak.concatenate((self.sigma2, tmp_sigma2))\n",
    "                    self.sigma3 = ak.concatenate((self.sigma3, tmp_sigma3))\n",
    "                    self.assoc = ak.concatenate((self.assoc, tmp_array))\n",
    "                    self.pt = ak.concatenate((self.pt, tmp_pt))\n",
    "                    self.vt = ak.concatenate((self.vt, tmp_vt))\n",
    "\n",
    "                counter += len(tmp_bx)\n",
    "                if counter >= max_events:\n",
    "                    print(f\"Reached {max_events} events!\")\n",
    "                    break\n",
    "            if counter >= max_events:\n",
    "                break\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            f'Dataset not found. Please download it from {self.url} and move all '\n",
    "            f'*.root files to {self.raw_dir}')\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Find the maximum index to determine the array length\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "\n",
    "            # Initialize an array with the correct size, filled with a placeholder (e.g., -1)\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "\n",
    "            # Populate the array based on the dictionary\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx in indices:\n",
    "                    reconstructed[idx] = value\n",
    "\n",
    "            return reconstructed\n",
    "        # Extract per-event arrays\n",
    "        event_time = self.time[idx]\n",
    "        event_raw_energy = self.raw_energy[idx]\n",
    "        event_bx = self.bx[idx]\n",
    "        event_by = self.by[idx]\n",
    "        event_bz = self.bz[idx]\n",
    "        event_beta = self.beta[idx]\n",
    "        event_bphi = self.bphi[idx]\n",
    "        event_EV1 = self.EV1[idx]\n",
    "        event_EV2 = self.EV2[idx]\n",
    "        event_EV3 = self.EV3[idx]\n",
    "        event_eV0x = self.eV0x[idx]\n",
    "        event_eV0y = self.eV0y[idx]\n",
    "        event_eV0z = self.eV0z[idx]\n",
    "        event_sigma1 = self.sigma1[idx]\n",
    "        event_sigma2 = self.sigma2[idx]\n",
    "        event_sigma3 = self.sigma3[idx]\n",
    "        event_assoc = self.assoc[idx]  # Each is now an array (e.g., [0, 1, 2]) indicating the pion group\n",
    "        event_pt = self.pt[idx]\n",
    "        event_vt = self.vt[idx]\n",
    "\n",
    "        # Convert each to numpy arrays if needed\n",
    "        event_time = np.array(event_time)\n",
    "        event_raw_energy = np.array(event_raw_energy)\n",
    "        event_bx = np.array(event_bx)\n",
    "        event_by = np.array(event_by)\n",
    "        event_bz = np.array(event_bz)\n",
    "        event_beta = np.array(event_beta)\n",
    "        event_bphi = np.array(event_bphi)\n",
    "        event_EV1 = np.array(event_EV1)\n",
    "        event_EV2 = np.array(event_EV2)\n",
    "        event_EV3 = np.array(event_EV3)\n",
    "        event_eV0x = np.array(event_eV0x)\n",
    "        event_eV0y = np.array(event_eV0y)\n",
    "        event_eV0z = np.array(event_eV0z)\n",
    "        event_sigma1 = np.array(event_sigma1)\n",
    "        event_sigma2 = np.array(event_sigma2)\n",
    "        event_sigma3 = np.array(event_sigma3)\n",
    "        event_assoc = np.array(event_assoc)  # e.g. [[0,1,2], [2,0,1], [1,0,2], ...]\n",
    "        event_pt = np.array(event_pt)\n",
    "        \n",
    "        avg_vt = []\n",
    "        for vt in event_vt:\n",
    "            vt_arr = np.array(vt)  # Convert list of vertex times to a numpy array\n",
    "            valid_times = vt_arr[vt_arr != -99]  # Filter out invalid times (-99)\n",
    "            if valid_times.size > 0:\n",
    "                avg_vt.append(valid_times.mean())\n",
    "            else:\n",
    "                avg_vt.append(0)  # or np.nan if you prefer to denote missing values\n",
    "        avg_vt = np.array(avg_vt)\n",
    "\n",
    "        # Combine features\n",
    "        flat_feats = np.column_stack((\n",
    "            event_bx, event_by, event_bz, event_raw_energy,\n",
    "            event_beta, event_bphi,\n",
    "            event_EV1, event_EV2, event_EV3,\n",
    "            event_eV0x, event_eV0y, event_eV0z,\n",
    "            event_sigma1, event_sigma2, event_sigma3, event_pt\n",
    "        ))\n",
    "        x = torch.from_numpy(flat_feats).float()\n",
    "        assoc = event_assoc\n",
    "\n",
    "        total_tracksters = len(event_time)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple. Two tracksters belong\n",
    "        # to the same pion group if their association arrays (converted to tuples)\n",
    "        # match.\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple\n",
    "         # Group tracksters by the first element of event_assoc\n",
    "        assoc_groups = {}\n",
    "        for i, assoc in enumerate(event_assoc):\n",
    "            key = assoc[0]  # Only use the first element as the key\n",
    "            if key not in assoc_groups:\n",
    "                assoc_groups[key] = []\n",
    "            assoc_groups[key].append(i)\n",
    "        assoc_array = reconstruct_array(assoc_groups)\n",
    "        pos_edges = []\n",
    "        neg_edges = []\n",
    "        # Ensure positive edges always connect to another trackster in the same group if possible\n",
    "        for i in range(total_tracksters):\n",
    "            key = event_assoc[i][0]  # Get first element as group identifier\n",
    "            same_group = assoc_groups[key]\n",
    "\n",
    "            # --- Positive edge ---\n",
    "            if len(same_group) > 1:\n",
    "                # Always select another trackster from the same group\n",
    "                pos_target = random.choice([j for j in same_group if j != i])\n",
    "            else:\n",
    "                # No other trackster in the group, form a self-loop\n",
    "                pos_target = i\n",
    "            pos_edges.append([i, pos_target])\n",
    "\n",
    "            # --- Negative edge ---\n",
    "            neg_candidates = [j for j in range(total_tracksters) if event_assoc[j][0] != key]\n",
    "            if neg_candidates:\n",
    "                neg_target = random.choice(neg_candidates)\n",
    "            else:\n",
    "                neg_target = i\n",
    "            neg_edges.append([i, neg_target])\n",
    "\n",
    "        x_pos_edge = torch.tensor(pos_edges, dtype=torch.long)\n",
    "        x_neg_edge = torch.tensor(neg_edges, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, x_pe=x_pos_edge, x_ne=x_neg_edge, assoc = assoc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49549f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5e/val/raw/val.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [00:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 100 events!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vpath = \"/vols/cms/mm1221/Data/100k/5e/val/\"\n",
    "data_val = CCV1(vpath, max_events=100, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf03f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 Means: [-3.68170047e+00 -5.29421043e+01  3.35287048e+02  1.19959915e+02\n",
      "  2.54272842e+00 -1.64022648e+00  3.84049339e+01  7.47460783e-01\n",
      "  3.40673774e-01  4.04848438e-03 -1.45035386e-01  9.89418209e-01\n",
      "  6.13216829e+00  9.62122023e-01  9.81783867e-01  1.74052086e+01]\n",
      "Group 1 Means: [ 2.6506069e+00 -4.8803196e+01  3.4861060e+02  5.6607277e+01\n",
      "  2.6621523e+00 -1.5166208e+00  2.3438560e+01  2.1770787e-01\n",
      "  1.0208924e-01  1.5691033e-01 -3.7340480e-01  8.6298758e-01\n",
      "  4.2646890e+00  6.2055349e-01  1.1281140e+00  8.1703224e+00]\n",
      "Group 2 Means: [-4.6648107e+00 -4.9610035e+01  3.4267044e+02  3.9465828e+01\n",
      "  2.6250632e+00 -1.6644167e+00  2.9590332e+01  9.2051888e-01\n",
      "  4.2493060e-01  5.4838147e-02 -1.4676648e-01  9.7702831e-01\n",
      "  4.9075565e+00  1.1576029e+00  9.8233032e-01  4.8005271e+00]\n",
      "Group 3 Means: [-8.3863287e+00 -5.3563755e+01  3.3538666e+02  1.0880519e+02\n",
      "  2.5219197e+00 -1.7261027e+00  4.2169136e+01  3.2079679e-01\n",
      "  5.1162496e-02 -3.8178496e-02 -1.6212319e-01  9.8603165e-01\n",
      "  6.4884281e+00  5.1263613e-01  4.2261103e-01  1.8122362e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert features and associations to numpy arrays\n",
    "features = data_val[0].x.numpy()\n",
    "assoc = np.array(data_val[0].assoc)\n",
    "\n",
    "# Compute mean per group\n",
    "unique_groups = np.unique(assoc)\n",
    "means = {group: np.mean(features[assoc == group], axis=0) for group in unique_groups}\n",
    "\n",
    "# Print means for each feature per group\n",
    "for group, mean_values in means.items():\n",
    "    print(f\"Group {group} Means: {mean_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc7ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2841e+00, -4.8579e+01,  3.3563e+02,  1.1379e+02,  2.6273e+00,\n",
      "         -1.6588e+00,  3.9528e+01,  4.6912e-01,  1.1979e-01, -1.3397e-02,\n",
      "         -1.1855e-01,  9.9286e-01,  6.2841e+00,  6.2077e-01,  4.9127e-01,\n",
      "          1.3576e+01],\n",
      "        [-8.3863e+00, -5.3564e+01,  3.3539e+02,  1.0881e+02,  2.5219e+00,\n",
      "         -1.7261e+00,  4.2169e+01,  3.2080e-01,  5.1162e-02, -3.8178e-02,\n",
      "         -1.6212e-01,  9.8603e-01,  6.4884e+00,  5.1264e-01,  4.2261e-01,\n",
      "          1.8122e+01],\n",
      "        [ 2.2171e+00, -4.8338e+01,  3.3668e+02,  1.1233e+02,  2.6381e+00,\n",
      "         -1.5250e+00,  4.0239e+01,  2.3432e-01,  2.0418e-01,  1.4504e-02,\n",
      "         -1.3940e-01,  9.9013e-01,  6.2117e+00,  9.6617e-01,  1.0766e+00,\n",
      "          1.5744e+01],\n",
      "        [-3.6817e+00, -5.2942e+01,  3.3529e+02,  1.1996e+02,  2.5427e+00,\n",
      "         -1.6402e+00,  3.8405e+01,  7.4746e-01,  3.4067e-01,  4.0485e-03,\n",
      "         -1.4504e-01,  9.8942e-01,  6.1322e+00,  9.6212e-01,  9.8178e-01,\n",
      "          1.7405e+01],\n",
      "        [-8.2142e+00, -4.9883e+01,  3.4268e+02,  3.5675e+00,  2.6123e+00,\n",
      "         -1.7340e+00,  4.2702e+01,  1.6150e+00,  1.1059e+00,  1.3815e-01,\n",
      "         -5.4635e-03,  9.9040e-01,  6.2923e+00,  1.2713e+00,  2.0528e+00,\n",
      "          4.9324e-01],\n",
      "        [-1.4961e+00, -5.0369e+01,  3.4971e+02,  1.0438e+00,  2.6356e+00,\n",
      "         -1.6005e+00,  6.5411e+00,  6.7742e-01,  4.9084e-02,  3.9760e-02,\n",
      "         -3.1628e-01,  9.4783e-01,  2.1463e+00,  1.5807e+00,  4.0292e-01,\n",
      "          3.3272e-01],\n",
      "        [ 3.0841e+00, -4.9268e+01,  3.6055e+02,  8.8042e-01,  2.6862e+00,\n",
      "         -1.5083e+00,  6.6376e+00,  2.0110e-01,  7.0077e-11,  2.9932e-01,\n",
      "         -6.0740e-01,  7.3585e-01,  2.3177e+00,  2.7494e-01,  1.1797e+00,\n",
      "          5.9618e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(data_val[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a6a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 0, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print(data_val[0].assoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21686b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
