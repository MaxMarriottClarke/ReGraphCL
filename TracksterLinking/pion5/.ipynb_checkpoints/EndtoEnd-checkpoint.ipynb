{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b16bd0",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02fba57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "from torch_geometric.nn import knn_graph\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fd09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Loads trackster-level features and associations for positive/negative edge creation.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading tracksters data\")\n",
    "\n",
    "\n",
    "        for path in tqdm.tqdm(self.raw_paths):\n",
    "            print(path)\n",
    "            \n",
    "            tracksters_path = find_highest_branch(path, 'tracksters')\n",
    "            associations_path = find_highest_branch(path, 'associations')\n",
    "            simtrack = find_highest_branch(path, 'simtrackstersCP')\n",
    "            # Load tracksters features in chunks\n",
    "            for array in uproot.iterate(\n",
    "                f\"{path}:{tracksters_path}\",\n",
    "                [\n",
    "                    \"time\", \"raw_energy\",\n",
    "                    \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \n",
    "                    \"barycenter_eta\", \"barycenter_phi\",\n",
    "                    \"EV1\", \"EV2\", \"EV3\",\n",
    "                    \"eVector0_x\", \"eVector0_y\", \"eVector0_z\",\n",
    "                    \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\"\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                tmp_time = array[\"time\"]\n",
    "                tmp_raw_energy = array[\"raw_energy\"]\n",
    "                tmp_bx = array[\"barycenter_x\"]\n",
    "                tmp_by = array[\"barycenter_y\"]\n",
    "                tmp_bz = array[\"barycenter_z\"]\n",
    "                tmp_beta = array[\"barycenter_eta\"]\n",
    "                tmp_bphi = array[\"barycenter_phi\"]\n",
    "                tmp_EV1 = array[\"EV1\"]\n",
    "                tmp_EV2 = array[\"EV2\"]\n",
    "                tmp_EV3 = array[\"EV3\"]\n",
    "                tmp_eV0x = array[\"eVector0_x\"]\n",
    "                tmp_eV0y = array[\"eVector0_y\"]\n",
    "                tmp_eV0z = array[\"eVector0_z\"]\n",
    "                tmp_sigma1 = array[\"sigmaPCA1\"]\n",
    "                tmp_sigma2 = array[\"sigmaPCA2\"]\n",
    "                tmp_sigma3 = array[\"sigmaPCA3\"]\n",
    "                \n",
    "                \n",
    "                vert_array = []\n",
    "                for vert_chunk in uproot.iterate(\n",
    "                    f\"{path}:{simtrack}\",\n",
    "                    [\"barycenter_x\"],\n",
    "                ):\n",
    "                    vert_array = vert_chunk[\"barycenter_x\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "\n",
    "                # Now load the associations for the same events/chunk\n",
    "                # 'tsCLUE3D_recoToSim_CP' gives association arrays like [[1,0],[0,1],...]\n",
    "                # Make sure we read from the same events\n",
    "                tmp_array = []\n",
    "                for assoc_chunk in uproot.iterate(\n",
    "                    f\"{path}:{associations_path}\",\n",
    "                    [\"tsCLUE3D_recoToSim_CP\"],\n",
    "                ):\n",
    "                    tmp_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in vert_array:\n",
    "                    if len(e) >= 5:\n",
    "                        skim_mask.append(True)\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "\n",
    "\n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in tmp_array:\n",
    "                    if 2 <= len(e):\n",
    "                        skim_mask.append(True)\n",
    "\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                        \n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "\n",
    "                \n",
    "                # Concatenate or initialize storage\n",
    "                if counter == 0:\n",
    "                    self.time = tmp_time\n",
    "                    self.raw_energy = tmp_raw_energy\n",
    "                    self.bx = tmp_bx\n",
    "                    self.by = tmp_by\n",
    "                    self.bz = tmp_bz\n",
    "                    self.beta = tmp_beta\n",
    "                    self.bphi = tmp_bphi\n",
    "                    self.EV1 = tmp_EV1\n",
    "                    self.EV2 = tmp_EV2\n",
    "                    self.EV3 = tmp_EV3\n",
    "                    self.eV0x = tmp_eV0x\n",
    "                    self.eV0y = tmp_eV0y\n",
    "                    self.eV0z = tmp_eV0z\n",
    "                    self.sigma1 = tmp_sigma1\n",
    "                    self.sigma2 = tmp_sigma2\n",
    "                    self.sigma3 = tmp_sigma3\n",
    "                    self.assoc = tmp_array\n",
    "                else:\n",
    "                    self.time = ak.concatenate((self.time, tmp_time))\n",
    "                    self.raw_energy = ak.concatenate((self.raw_energy, tmp_raw_energy))\n",
    "                    self.bx = ak.concatenate((self.bx, tmp_bx))\n",
    "                    self.by = ak.concatenate((self.by, tmp_by))\n",
    "                    self.bz = ak.concatenate((self.bz, tmp_bz))\n",
    "                    self.beta = ak.concatenate((self.beta, tmp_beta))\n",
    "                    self.bphi = ak.concatenate((self.bphi, tmp_bphi))\n",
    "                    self.EV1 = ak.concatenate((self.EV1, tmp_EV1))\n",
    "                    self.EV2 = ak.concatenate((self.EV2, tmp_EV2))\n",
    "                    self.EV3 = ak.concatenate((self.EV3, tmp_EV3))\n",
    "                    self.eV0x = ak.concatenate((self.eV0x, tmp_eV0x))\n",
    "                    self.eV0y = ak.concatenate((self.eV0y, tmp_eV0y))\n",
    "                    self.eV0z = ak.concatenate((self.eV0z, tmp_eV0z))\n",
    "                    self.sigma1 = ak.concatenate((self.sigma1, tmp_sigma1))\n",
    "                    self.sigma2 = ak.concatenate((self.sigma2, tmp_sigma2))\n",
    "                    self.sigma3 = ak.concatenate((self.sigma3, tmp_sigma3))\n",
    "                    self.assoc = ak.concatenate((self.assoc, tmp_array))\n",
    "\n",
    "                counter += len(tmp_bx)\n",
    "                if counter >= max_events:\n",
    "                    print(f\"Reached {max_events} events!\")\n",
    "                    break\n",
    "            if counter >= max_events:\n",
    "                break\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            f'Dataset not found. Please download it from {self.url} and move all '\n",
    "            f'*.root files to {self.raw_dir}')\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Find the maximum index to determine the array length\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "\n",
    "            # Initialize an array with the correct size, filled with a placeholder (e.g., -1)\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "\n",
    "            # Populate the array based on the dictionary\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx in indices:\n",
    "                    reconstructed[idx] = value\n",
    "\n",
    "            return reconstructed\n",
    "        # Extract per-event arrays\n",
    "        event_time = self.time[idx]\n",
    "        event_raw_energy = self.raw_energy[idx]\n",
    "        event_bx = self.bx[idx]\n",
    "        event_by = self.by[idx]\n",
    "        event_bz = self.bz[idx]\n",
    "        event_beta = self.beta[idx]\n",
    "        event_bphi = self.bphi[idx]\n",
    "        event_EV1 = self.EV1[idx]\n",
    "        event_EV2 = self.EV2[idx]\n",
    "        event_EV3 = self.EV3[idx]\n",
    "        event_eV0x = self.eV0x[idx]\n",
    "        event_eV0y = self.eV0y[idx]\n",
    "        event_eV0z = self.eV0z[idx]\n",
    "        event_sigma1 = self.sigma1[idx]\n",
    "        event_sigma2 = self.sigma2[idx]\n",
    "        event_sigma3 = self.sigma3[idx]\n",
    "        event_assoc = self.assoc[idx]  # Each is now an array (e.g., [0, 1, 2]) indicating the pion group\n",
    "\n",
    "        # Convert each to numpy arrays if needed\n",
    "        event_time = np.array(event_time)\n",
    "        event_raw_energy = np.array(event_raw_energy)\n",
    "        event_bx = np.array(event_bx)\n",
    "        event_by = np.array(event_by)\n",
    "        event_bz = np.array(event_bz)\n",
    "        event_beta = np.array(event_beta)\n",
    "        event_bphi = np.array(event_bphi)\n",
    "        event_EV1 = np.array(event_EV1)\n",
    "        event_EV2 = np.array(event_EV2)\n",
    "        event_EV3 = np.array(event_EV3)\n",
    "        event_eV0x = np.array(event_eV0x)\n",
    "        event_eV0y = np.array(event_eV0y)\n",
    "        event_eV0z = np.array(event_eV0z)\n",
    "        event_sigma1 = np.array(event_sigma1)\n",
    "        event_sigma2 = np.array(event_sigma2)\n",
    "        event_sigma3 = np.array(event_sigma3)\n",
    "        event_assoc = np.array(event_assoc)  # e.g. [[0,1,2], [2,0,1], [1,0,2], ...]\n",
    "\n",
    "        # Combine features\n",
    "        flat_feats = np.column_stack((\n",
    "            event_bx, event_by, event_bz, event_raw_energy,\n",
    "            event_beta, event_bphi,\n",
    "            event_EV1, event_EV2, event_EV3,\n",
    "            event_eV0x, event_eV0y, event_eV0z,\n",
    "            event_sigma1, event_sigma2, event_sigma3\n",
    "        ))\n",
    "        x = torch.from_numpy(flat_feats).float()\n",
    "        assoc = event_assoc\n",
    "\n",
    "        total_tracksters = len(event_time)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple. Two tracksters belong\n",
    "        # to the same pion group if their association arrays (converted to tuples)\n",
    "        # match.\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple\n",
    "         # Group tracksters by the first element of event_assoc\n",
    "        assoc_groups = {}\n",
    "        for i, assoc in enumerate(event_assoc):\n",
    "            key = assoc[0]  # Only use the first element as the key\n",
    "            if key not in assoc_groups:\n",
    "                assoc_groups[key] = []\n",
    "            assoc_groups[key].append(i)\n",
    "        assoc_array = reconstruct_array(assoc_groups)\n",
    "        pos_edges = []\n",
    "        neg_edges = []\n",
    "        # Ensure positive edges always connect to another trackster in the same group if possible\n",
    "        for i in range(total_tracksters):\n",
    "            key = event_assoc[i][0]  # Get first element as group identifier\n",
    "            same_group = assoc_groups[key]\n",
    "\n",
    "            # --- Positive edge ---\n",
    "            if len(same_group) > 1:\n",
    "                # Always select another trackster from the same group\n",
    "                pos_target = random.choice([j for j in same_group if j != i])\n",
    "            else:\n",
    "                # No other trackster in the group, form a self-loop\n",
    "                pos_target = i\n",
    "            pos_edges.append([i, pos_target])\n",
    "\n",
    "            # --- Negative edge ---\n",
    "            neg_candidates = [j for j in range(total_tracksters) if event_assoc[j][0] != key]\n",
    "            if neg_candidates:\n",
    "                neg_target = random.choice(neg_candidates)\n",
    "            else:\n",
    "                neg_target = i\n",
    "            neg_edges.append([i, neg_target])\n",
    "\n",
    "        x_pos_edge = torch.tensor(pos_edges, dtype=torch.long)\n",
    "        x_neg_edge = torch.tensor(neg_edges, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, x_pe=x_pos_edge, x_ne=x_neg_edge, assoc = assoc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23701da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/train/raw/train.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:46<00:00, 46.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/val/raw/2k5pi.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 1/2 [00:01<00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/val/raw/8k5pi.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "ipath = \"/vols/cms/mm1221/Data/100k/5pi/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/100k/5pi/val/\"\n",
    "\n",
    "data_train = CCV1(ipath, max_events=80000, inp = 'train')\n",
    "data_val = CCV1(vpath, max_events=10000, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064ad03e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_410089/447961831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTrack_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tracksters;2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vertices_indexes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mGT_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simtrackstersCP;3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vertices_indexes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mGT_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simtrackstersCP;3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vertices_multiplicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mGT_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simtrackstersCP;3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'barycenter_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clusters;4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'energy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(self, interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, array_cache, library)\u001b[0m\n\u001b[1;32m   2206\u001b[0m                         \u001b[0mranges_or_baskets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasket_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange_or_basket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m         _ranges_or_baskets_to_arrays(\n\u001b[0m\u001b[1;32m   2209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m             \u001b[0mranges_or_baskets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36m_ranges_or_baskets_to_arrays\u001b[0;34m(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\u001b[0m\n\u001b[1;32m   3485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3486\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTBasket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel_TBasket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3487\u001b[0;31m             \u001b[0minterpretation_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_to_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/source/futures.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, task, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mImmediately\u001b[0m \u001b[0mruns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrivialFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36mbasket_to_array\u001b[0;34m(basket)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbranchid_num_baskets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m                 arrays[branch.cache_key] = interpretation.final_array(\n\u001b[0m\u001b[1;32m   3464\u001b[0m                     \u001b[0mbasket_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m                     \u001b[0mentry_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/interpretation/objects.py\u001b[0m in \u001b[0;36mfinal_array\u001b[0;34m(self, basket_arrays, entry_start, entry_stop, entry_offsets, library, branch)\u001b[0m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         self.hook_after_final_array(\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/interpretation/library.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self, array, branch, interpretation, entry_start, entry_stop)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 ) from err\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             unlabeled = awkward.from_iter(\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0m_object_to_awkward_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/operations/convert.py\u001b[0m in \u001b[0;36mfrom_iter\u001b[0;34m(iterable, highlevel, behavior, allow_record, initial, resize)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc11-opt/lib/python3.9/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Also load explicitely, used for analysis and plots\n",
    "data_path = '/vols/cms/mm1221/Data/100k/5pi/test/raw/test.root'\n",
    "data_file = uproot.open(data_path)\n",
    "\n",
    "Track_ind = data_file['tracksters;2']['vertices_indexes'].array()\n",
    "GT_ind = data_file['simtrackstersCP;3']['vertices_indexes'].array()\n",
    "GT_mult = data_file['simtrackstersCP;3']['vertices_multiplicity'].array()\n",
    "GT_bc = data_file['simtrackstersCP;3']['barycenter_x'].array()\n",
    "energies = data_file['clusters;4']['energy'].array()\n",
    "LC_x = data_file['clusters;4']['position_x'].array()\n",
    "LC_y = data_file['clusters;4']['position_y'].array()\n",
    "LC_z = data_file['clusters;4']['position_z'].array()\n",
    "LC_eta = data_file['clusters;4']['position_eta'].array()\n",
    "MT_ind = data_file['trackstersMerged;2']['vertices_indexes'].array()\n",
    "\n",
    "#1.3 Filter so get rid of events with 0 calo particles\n",
    "skim_mask = []\n",
    "for e in GT_bc:\n",
    "    if 5 <= len(e) :\n",
    "        skim_mask.append(True)\n",
    "    else:\n",
    "        skim_mask.append(False)\n",
    "\n",
    "Track_ind = Track_ind[skim_mask]\n",
    "GT_ind = GT_ind[skim_mask]\n",
    "GT_mult = GT_mult[skim_mask]\n",
    "energies = energies[skim_mask]\n",
    "LC_x = LC_x[skim_mask]\n",
    "LC_y = LC_y[skim_mask]\n",
    "LC_z = LC_z[skim_mask]\n",
    "LC_eta = LC_eta[skim_mask]\n",
    "MT_ind = MT_ind[skim_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20068f",
   "metadata": {},
   "source": [
    "# Initialise the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd51aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=1, concat=True, dropout=0.6, alpha=0.4):\n",
    "        \"\"\"\n",
    "        Initializes the Custom GAT Layer.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Input feature dimension.\n",
    "            out_dim (int): Output feature dimension per head.\n",
    "            heads (int): Number of attention heads.\n",
    "            concat (bool): Whether to concatenate the heads' output or average them.\n",
    "            dropout (float): Dropout rate on attention coefficients.\n",
    "            alpha (float): Negative slope for LeakyReLU.\n",
    "        \"\"\"\n",
    "        super(CustomGATLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear transformation for node features\n",
    "        self.W = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "\n",
    "        # Attention mechanism: a vector for each head\n",
    "        self.a_src = nn.Parameter(torch.zeros(heads, out_dim))\n",
    "        self.a_tgt = nn.Parameter(torch.zeros(heads, out_dim))\n",
    "        nn.init.xavier_uniform_(self.a_src.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a_tgt.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Optional batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(heads * out_dim) if concat else nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features of shape (N, in_dim).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated node features after attention-based aggregation.\n",
    "        \"\"\"\n",
    "        src, tgt = edge_index  # Source and target node indices\n",
    "        N = x.size(0)\n",
    "\n",
    "        # Apply linear transformation and reshape for multi-head attention\n",
    "        h = self.W(x)  # Shape: (N, heads * out_dim)\n",
    "        h = h.view(N, self.heads, self.out_dim)  # Shape: (N, heads, out_dim)\n",
    "\n",
    "        # Gather node features for each edge\n",
    "        h_src = h[src]  # Shape: (E, heads, out_dim)\n",
    "        h_tgt = h[tgt]  # Shape: (E, heads, out_dim)\n",
    "\n",
    "        # Compute attention coefficients using separate vectors for source and target\n",
    "        e_src = (h_src * self.a_src).sum(dim=-1)  # Shape: (E, heads)\n",
    "        e_tgt = (h_tgt * self.a_tgt).sum(dim=-1)  # Shape: (E, heads)\n",
    "        e = self.leakyrelu(e_src + e_tgt)  # Shape: (E, heads)\n",
    "\n",
    "        # Compute softmax normalization for attention coefficients\n",
    "        # To ensure numerical stability\n",
    "        e = e - e.max(dim=0, keepdim=True)[0]\n",
    "        alpha = torch.exp(e)  # Shape: (E, heads)\n",
    "\n",
    "        # Sum of attention coefficients for each target node and head\n",
    "        alpha_sum = torch.zeros(N, self.heads, device=x.device).scatter_add_(0, tgt.unsqueeze(-1).expand(-1, self.heads), alpha)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        alpha_sum = alpha_sum + 1e-16\n",
    "\n",
    "        # Normalize attention coefficients\n",
    "        alpha = alpha / alpha_sum[tgt]  # Shape: (E, heads)\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        # Weighted aggregation of source node features\n",
    "        h_prime = h_src * alpha.unsqueeze(-1)  # Shape: (E, heads, out_dim)\n",
    "\n",
    "        # Initialize output tensor and aggregate\n",
    "        out = torch.zeros(N, self.heads, self.out_dim, device=x.device)\n",
    "        out.scatter_add_(0, tgt.unsqueeze(-1).unsqueeze(-1).expand(-1, self.heads, self.out_dim), h_prime)  # Shape: (N, heads, out_dim)\n",
    "\n",
    "        # Concatenate or average the heads\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_dim)  # Shape: (N, heads*out_dim)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # Shape: (N, out_dim)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=3, dropout=0.3, contrastive_dim=8, heads=4):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with GAT layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Number of GAT layers.\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            heads (int): Number of attention heads in GAT layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(15, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # GAT layers with residual connections\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(\n",
    "                CustomGATLayer(\n",
    "                    in_dim=hidden_dim,\n",
    "                    out_dim=hidden_dim // heads if heads > 1 else hidden_dim,\n",
    "                    heads=heads,\n",
    "                    concat=True,\n",
    "                    dropout=0,\n",
    "                    alpha=0.4\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(16, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 6).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply GAT layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for conv in self.convs:\n",
    "            feats = conv(feats, edge_index) + feats  # Residual connection\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e7ef264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with passed hyperparameters\n",
    "model = Net(\n",
    "    hidden_dim=128,\n",
    "    num_layers=3,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=64,\n",
    "    heads=4\n",
    ")\n",
    "\n",
    "k = 16\n",
    "BS = 64\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9671942",
   "metadata": {},
   "source": [
    "# Define The Loss term and the Training + Val Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c018b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss( start_all, end_all, temperature=0.1):\n",
    "    xdevice = start_all.get_device()\n",
    "    z_start = F.normalize( start_all, dim=1 )\n",
    "    z_end = F.normalize( end_all, dim=1 )\n",
    "    positives = torch.exp(F.cosine_similarity(z_start[:int(len(z_start)/2)],z_end[:int(len(z_end)/2)],dim=1))\n",
    "    negatives = torch.exp(F.cosine_similarity(z_start[int(len(z_start)/2):],z_end[int(len(z_end)/2):],dim=1))\n",
    "    nominator = positives / temperature\n",
    "    denominator = negatives\n",
    "    #print(denominator\n",
    "    loss = torch.exp(-nominator.sum() / denominator.sum())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e16db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, device, k_value):\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in tqdm.tqdm(train_loader):\n",
    "        counter += 1\n",
    "\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        edge_index = knn_graph(data.x, k=k_value, batch=data.x_batch)  # k=16 neighbors\n",
    "        \n",
    "        \n",
    "        # Pass edge_index explicitly to the model\n",
    "        out = model(data.x, edge_index, data.x_batch)\n",
    "\n",
    "        values, counts = np.unique(data.x_batch.detach().cpu().numpy(), return_counts=True)\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        for e in range(len(counts)):\n",
    "                   \n",
    "\n",
    "            lower_edge = 0 if e == 0 else np.sum(counts[:e])\n",
    "            upper_edge = lower_edge + counts[e]\n",
    "\n",
    "\n",
    "            start_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 0]]\n",
    "            end_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 1]]\n",
    "            start_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 0]]\n",
    "            end_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 1]]\n",
    "\n",
    "            start_all = torch.cat((start_pos, start_neg), 0)\n",
    "            end_all = torch.cat((end_pos, end_neg), 0)\n",
    "\n",
    "            if len(losses) == 0:\n",
    "                losses.append(contrastive_loss(start_all, end_all, 0.3))\n",
    "            else:\n",
    "                losses.append(losses[-1] + contrastive_loss(start_all, end_all, 0.3))\n",
    "\n",
    "        loss = losses[-1]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_loader, model, device, k_value):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "\n",
    "    for data in tqdm.tqdm(test_loader):\n",
    "        counter += 1\n",
    "        data = data.to(device)\n",
    "\n",
    "\n",
    "        edge_index = knn_graph(data.x, k=k_value, batch=data.x_batch)  # k=16 neighbors\n",
    "        # Pass edge_index explicitly to the model\n",
    "        out = model(data.x, edge_index, data.x_batch)\n",
    "\n",
    "        values, counts = np.unique(data.x_batch.detach().cpu().numpy(), return_counts=True)\n",
    "\n",
    "        losses = []\n",
    "        for e in range(len(counts)):\n",
    "\n",
    "            lower_edge = 0 if e == 0 else np.sum(counts[:e])\n",
    "            upper_edge = lower_edge + counts[e]\n",
    "\n",
    "            start_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 0]]\n",
    "            end_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 1]]\n",
    "            start_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 0]]\n",
    "            end_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 1]]\n",
    "\n",
    "            start_all = torch.cat((start_pos, start_neg), 0)\n",
    "            end_all = torch.cat((end_pos, end_neg), 0)\n",
    "\n",
    "            if len(losses) == 0:\n",
    "                losses.append(contrastive_loss(start_all, end_all, 0.3))\n",
    "            else:\n",
    "                losses.append(losses[-1] + contrastive_loss(start_all, end_all, 0.3))\n",
    "\n",
    "        loss = losses[-1]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400ab0d",
   "metadata": {},
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "589d4b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [01:12<00:00,  2.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 0.013052317657727497, Validation Loss: 0.0026853354047282154\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                         | 126/194 [00:46<00:25,  2.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_410089/2652804379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Train and evaluate for this epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_410089/443733532.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, device, k_value)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 or (isinstance(idx, np.ndarray) and np.isscalar(idx))):\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_410089/1181408279.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mevent_EV3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_EV3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mevent_eV0x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_eV0x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mevent_eV0y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_eV0y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mevent_eV0z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_eV0z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mevent_sigma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_sigma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msliced\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \"\"\"\n\u001b[0;32m-> 1351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/_connect/_numpy.py\u001b[0m in \u001b[0;36mconvert_to_array\u001b[0;34m(layout, args, kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/operations/convert.py\u001b[0m in \u001b[0;36mto_numpy\u001b[0;34m(array, allow_missing)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumpyArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnplike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cupy.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/nplike.py\u001b[0m in \u001b[0;36mof\u001b[0;34m(default_cls, *arrays)\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_numpy_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0mnplikes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_cupy_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m             \u001b[0mnplikes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_jax_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/nplike.py\u001b[0m in \u001b[0;36mis_cupy_buffer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \"\"\"\n\u001b[0;32m--> 887\u001b[0;31m     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cupy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Load DataLoader with current batch_size\n",
    "train_loader = DataLoader(data_train, batch_size=BS, shuffle=True, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Train and evaluate the model for the specified number of epochs\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Store train and validation losses for all epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_dir = '/vols/cms/mm1221/hgcal/pion5New/Track/resultsEnd/GATinit/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "    # Train and evaluate for this epoch\n",
    "    train_loss = train(train_loader, model, optimizer,device, k)\n",
    "    val_loss = test(val_loader, model, device, k)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the best model if this epoch's validation loss is lower\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "\n",
    "    # Save intermediate state dictionaries\n",
    "    state_dicts = {'model': model.state_dict(), 'opt': optimizer.state_dict(), 'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch}.pt'))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "\n",
    "# Save training and validation loss curves\n",
    "loss_result_filename = (\n",
    "    'result.csv'\n",
    ")\n",
    "\n",
    "# Dynamically adjust the epoch range to match the length of train_losses and val_losses\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),  # Adjusted to the actual length of losses\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "\n",
    "# Save to a CSV file in the output directory\n",
    "results_df.to_csv(os.path.join(output_dir, loss_result_filename), index=False)\n",
    "\n",
    "print(f'Saved training and validation losses to {os.path.join(output_dir, loss_result_filename)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5729785",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c3b14",
   "metadata": {},
   "source": [
    "## Loading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = \"/vols/cms/mm1221/Data/100k/5pi/test/\"\n",
    "\n",
    "data_test = CCV1(testpath, max_events=10000, inp='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad36325",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint= torch.load('/vols/cms/mm1221/hgcal/pion5New/Track/resultsEnd/layer3k16hd128temp3feat6/best_model.pt',  map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)  \n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []  \n",
    "\n",
    "\n",
    "for i, data in enumerate(data_test):\n",
    "    edge_index = knn_graph(data.x, k=k)  \n",
    "    predictions = model(data.x, edge_index, 1)\n",
    "    all_predictions.append(predictions[0].detach().cpu().numpy())  \n",
    "\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7535da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "def HDBSCANClustering(all_predictions, \n",
    "                      min_cluster_size=5, \n",
    "                      min_samples=None, \n",
    "                      metric='euclidean', \n",
    "                      alpha=1.0,\n",
    "                      cluster_selection_method='eom',\n",
    "                      prediction_data=False,\n",
    "                      allow_single_cluster=True,\n",
    "                      core_dist_n_jobs=1,\n",
    "                      cluster_selection_epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Performs HDBSCAN clustering on a list of prediction arrays with more hyperparameter control.\n",
    "\n",
    "    Parameters:\n",
    "    - all_predictions: List of numpy arrays, each containing data points for an event.\n",
    "    - min_cluster_size: Minimum size of clusters.\n",
    "    - min_samples: Number of samples in a neighborhood for a point to be considered a core point.\n",
    "                   If None, it defaults to min_cluster_size.\n",
    "    - metric: Distance metric to use.\n",
    "    - alpha: Controls the balance between single linkage and average linkage clustering.\n",
    "    - cluster_selection_method: 'eom' (Excess of Mass) or 'leaf' for finer clusters.\n",
    "    - prediction_data: If True, allows later predictions on new data.\n",
    "    - allow_single_cluster: If True, allows a single large cluster when applicable.\n",
    "    - core_dist_n_jobs: Number of parallel jobs (-1 uses all cores).\n",
    "    - cluster_selection_epsilon: Threshold distance for cluster selection (default 0.0).\n",
    "\n",
    "    Returns:\n",
    "    - all_cluster_labels: NumPy array of cluster labels for all events.\n",
    "    \"\"\"\n",
    "    all_cluster_labels = []             \n",
    "\n",
    "    for i, pred in enumerate(all_predictions):\n",
    "        print(f\"Processing event {i+1}/{len(all_predictions)}...\")\n",
    "        \n",
    "        if len(pred) < 2:\n",
    "            # Assign all points to cluster 0 (since HDBSCAN uses -1 for noise)\n",
    "            cluster_labels = np.zeros(len(pred), dtype=int) \n",
    "        else:\n",
    "            # Initialize HDBSCAN with specified parameters\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples if min_samples is not None else min_cluster_size,\n",
    "                metric=metric,\n",
    "                alpha=alpha,\n",
    "                cluster_selection_method=cluster_selection_method,\n",
    "                prediction_data=prediction_data,\n",
    "                allow_single_cluster=allow_single_cluster,\n",
    "                core_dist_n_jobs=core_dist_n_jobs,\n",
    "                cluster_selection_epsilon=cluster_selection_epsilon\n",
    "            )\n",
    "            \n",
    "            # Perform clustering\n",
    "            cluster_labels = clusterer.fit_predict(pred)  \n",
    "        \n",
    "        all_cluster_labels.append(cluster_labels)\n",
    "    \n",
    "    # Convert the list of cluster labels to a NumPy array\n",
    "    all_cluster_labels = np.array(all_cluster_labels)\n",
    "    return all_cluster_labels\n",
    "\n",
    "all_cluster_labels = HDBSCANClustering(\n",
    "    all_predictions, \n",
    "    min_cluster_size=2,  # Ensures at least 3 points per cluster\n",
    "    metric='euclidean',  # Change distance metric\n",
    "    alpha=1.0,  # Increase single linkage weighting\n",
    "    cluster_selection_method='leaf',  # Allow finer clusters\n",
    "    prediction_data=False,  # Enable future prediction capability\n",
    "    allow_single_cluster=True,  # Allow one large cluster\n",
    "    core_dist_n_jobs=-1  # Use all available CPU cores\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_ind = []\n",
    "\n",
    "for event_idx, labels in enumerate(all_cluster_labels):\n",
    "    event_clusters = {} \n",
    "    \n",
    "    for cluster_idx, cluster_label in enumerate(labels):\n",
    "        if cluster_label not in event_clusters:\n",
    "            event_clusters[cluster_label] = []\n",
    "        event_clusters[cluster_label].extend(Track_ind[event_idx][cluster_idx])\n",
    "    \n",
    "    recon_ind.append([event_clusters[label] for label in sorted(event_clusters.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d84b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sim_to_reco_score(CaloParticle, energies_indices, ReconstructedTrackster, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the sim-to-reco score for a given CaloParticle and ReconstructedTrackster.\n",
    "    \n",
    "    Parameters:\n",
    "    - CaloParticle: array of Layer Clusters in the CaloParticle.\n",
    "    - Multiplicity: array of Multiplicity for layer clusters in CP\n",
    "    - energies_indices: array of energies associated with all LC (indexed by LC).\n",
    "    - ReconstructedTrackster: array of LC in the reconstructed Trackster.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_to_reco_score: the calculated sim-to-reco score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    if total_energy_caloparticle == 0:\n",
    "        return 1.0  # No energy in the CaloParticle implies perfect mismatch\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    i = 0\n",
    "    # Iterate over all DetIds in the CaloParticle\n",
    "    for det_id in CaloParticle:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in CaloParticle\n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1 if det_id in ReconstructedTrackster else 0.0\n",
    "        # Fraction of energy in the CaloParticle (fr_k^SC)\n",
    "        fr_sc_k = 1 / Multi[i]\n",
    "\n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_sc_k ** 2                # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_sc_k ** 2) * (energy_k ** 2)\n",
    "        i+=1\n",
    "\n",
    "    # Calculate score\n",
    "    sim_to_reco_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return sim_to_reco_score\n",
    "\n",
    "def calculate_reco_to_sim_score(ReconstructedTrackster, energies_indices, CaloParticle, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the reco-to-sim score for a given ReconstructedTrackster and CaloParticle.\n",
    "\n",
    "    Parameters:\n",
    "    - ReconstructedTrackster: array of DetIds in the ReconstructedTrackster.\n",
    "    - energies_indices: array of energies associated with all DetIds (indexed by DetId).\n",
    "    - CaloParticle: array of DetIds in the CaloParticle.\n",
    "\n",
    "    Returns:\n",
    "    - reco_to_sim_score: the calculated reco-to-sim score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    if total_energy_trackster == 0:\n",
    "        return 1.0  # No energy in the Trackster implies perfect mismatch\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    # Iterate over all DetIds in the ReconstructedTrackster\n",
    "    for det_id in ReconstructedTrackster:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in the Trackster\n",
    "        \n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1\n",
    "\n",
    "        #fr_sc_k = 1 if det_id in CaloParticle else 0.0\n",
    "        if det_id in CaloParticle:\n",
    "            index = np.where(CaloParticle == det_id)[0][0]  # Find the index\n",
    "            Multiplicity = Multi[index]\n",
    "            fr_sc_k = 1\n",
    "        else:\n",
    "            fr_sc_k = 0\n",
    "            \n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_tst_k ** 2               # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_tst_k ** 2) * (energy_k ** 2)\n",
    "\n",
    "    # Calculate score\n",
    "    reco_to_sim_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return reco_to_sim_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edec964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "def calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, multi, num_events = 100):\n",
    "    \"\"\"\n",
    "    Calculate sim-to-reco and reco-to-sim scores for all CaloParticle and ReconstructedTrackster combinations across all events.\n",
    "\n",
    "    Parameters:\n",
    "    - GT_ind: List of CaloParticle indices for all events.\n",
    "    - energies: List of energy arrays for all events.\n",
    "    - recon_ind: List of ReconstructedTrackster indices for all events.\n",
    "    - LC_x, LC_y, LC_z, LC_eta: Lists of x, y, z positions and eta values for all DetIds across events.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing scores and additional features for each CaloParticle-Trackster combination across all events.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Loop over all events with a progress bar\n",
    "    for event_index in range(num_events):\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_multi = multi[event_index]\n",
    "\n",
    "        # Extract layer cluster positions and eta for this event\n",
    "        event_x = np.array(LC_x[event_index])\n",
    "        event_y = np.array(LC_y[event_index])\n",
    "        event_z = np.array(LC_z[event_index])\n",
    "        event_eta = np.array(LC_eta[event_index])\n",
    "\n",
    "        # Compute barycenter for each CaloParticle\n",
    "        cp_barycenters = []\n",
    "        cp_avg_etas = []\n",
    "        for caloparticle in caloparticles:\n",
    "            # Compute barycenter (x, y, z)\n",
    "            \n",
    "            barycenter_x = np.mean([event_x[det_id] for det_id in caloparticle])\n",
    "            barycenter_y = np.mean([event_y[det_id] for det_id in caloparticle])\n",
    "            barycenter_z = np.mean([event_z[det_id] for det_id in caloparticle])\n",
    "            cp_barycenters.append(np.array([barycenter_x, barycenter_y, barycenter_z]))\n",
    "            \n",
    "            # Compute average eta\n",
    "            avg_eta = np.mean([event_eta[det_id] for det_id in caloparticle])\n",
    "            cp_avg_etas.append(avg_eta)\n",
    "\n",
    "        # Compute separation between two CaloParticles if at least two exist\n",
    "        if len(cp_barycenters) >= 2:\n",
    "            cp_separation = np.linalg.norm(cp_barycenters[0] - cp_barycenters[1])\n",
    "        else:\n",
    "            cp_separation = 0.0\n",
    "            \n",
    "        trackster_det_id_sets = [set(trackster) for trackster in tracksters]\n",
    "\n",
    "        # Loop over all CaloParticles\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            Calo_multi = event_multi[calo_idx]\n",
    "            calo_det_ids = set(calo_id for calo_id in caloparticle)\n",
    "            # Loop over all Tracksters\n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                # Calculate sim-to-reco score\n",
    "                trackster_det_ids = trackster_det_id_sets[trackster_idx]\n",
    "                shared_det_ids = calo_det_ids.intersection(trackster_det_ids)\n",
    "                \n",
    "                # Calculate shared_energy by summing energies of shared det_ids\n",
    "                shared_energy = np.sum(event_energies[list(shared_det_ids)]) if shared_det_ids else 0.0\n",
    "                \n",
    "                \n",
    "                sim_to_reco_score = calculate_sim_to_reco_score(caloparticle, event_energies, trackster, Calo_multi)\n",
    "                # Calculate reco-to-sim score\n",
    "                reco_to_sim_score = calculate_reco_to_sim_score(trackster, event_energies, caloparticle, Calo_multi)\n",
    "\n",
    "                # Calculate total energy for CaloParticle and Trackster\n",
    "                cp_energy_lc2 = event_energies[caloparticle] / Calo_multi\n",
    "                cp_energy = np.sum(cp_energy_lc2)\n",
    "                \n",
    "                trackster_energy = np.sum([event_energies[det_id] for det_id in trackster])\n",
    "\n",
    "                # Calculate energy difference ratio\n",
    "                energy_diff_ratio = (trackster_energy / cp_energy if cp_energy != 0 else None)\n",
    "\n",
    "                # Append results\n",
    "                all_results.append({\n",
    "                    \"event_index\": event_index,\n",
    "                    \"cp_id\": calo_idx,\n",
    "                    \"trackster_id\": trackster_idx,\n",
    "                    \"sim_to_reco_score\": sim_to_reco_score,\n",
    "                    \"reco_to_sim_score\": reco_to_sim_score,\n",
    "                    \"cp_energy\": cp_energy,\n",
    "                    \"trackster_energy\": trackster_energy,\n",
    "                    \"cp_avg_eta\": cp_avg_etas[calo_idx],\n",
    "                    \"cp_separation\": cp_separation,\n",
    "                    \"energy_ratio\": energy_diff_ratio,\n",
    "                    \"shared_energy\": shared_energy  # New column\n",
    "                })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL = calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 100)\n",
    "#df_TICL = calculate_all_event_scores(GT_ind, energies, MT_ind, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Print metrics\n",
    "\n",
    "def calculate_metrics(df, model_name):\n",
    "    # ----- Efficiency Calculation -----\n",
    "    # Step 1: Filter out rows where 'cp_id' is NaN\n",
    "    cp_valid = df.dropna(subset=['cp_id']).copy()\n",
    "\n",
    "    # Step 2: Group by 'event_index' and 'cp_id' to process each CaloParticle individually\n",
    "    cp_grouped = cp_valid.groupby(['event_index', 'cp_id'])\n",
    "\n",
    "    # Step 3: For each CaloParticle, check if any 'shared_energy' >= 50% of 'cp_energy'\n",
    "    def is_cp_associated(group):\n",
    "        cp_energy = group['cp_energy'].iloc[0]  # Assuming 'cp_energy' is consistent within the group\n",
    "        threshold = 0.5 * cp_energy\n",
    "        return (group['shared_energy'] >= threshold).any()\n",
    "\n",
    "    # Apply the association function to each group\n",
    "    cp_associated = cp_grouped.apply(is_cp_associated)\n",
    "\n",
    "    # Step 4: Calculate the number of associated CaloParticles and total CaloParticles\n",
    "    num_associated_cp = cp_associated.sum()\n",
    "    total_cp = cp_associated.count()\n",
    "    efficiency = num_associated_cp / total_cp if total_cp > 0 else 0\n",
    "\n",
    "    # ----- Purity Calculation -----\n",
    "    tst_valid = df.dropna(subset=['trackster_id']).copy()\n",
    "    tst_grouped = tst_valid.groupby(['event_index', 'trackster_id'])\n",
    "    tst_associated = tst_grouped['reco_to_sim_score'].min() < 0.2\n",
    "    num_associated_tst = tst_associated.sum()\n",
    "    total_tst = tst_associated.count()\n",
    "    purity = num_associated_tst / total_tst if total_tst > 0 else 0\n",
    "\n",
    "    # ----- Average Energy Ratio Calculation -----\n",
    "    low_score_mask = df['sim_to_reco_score'] < 0.2\n",
    "    low_score_events = df[low_score_mask]\n",
    "    if not low_score_events.empty:\n",
    "        avg_energy_ratio = (low_score_events['trackster_energy'] / low_score_events['cp_energy']).mean()\n",
    "    else:\n",
    "        avg_energy_ratio = 0\n",
    "\n",
    "    # Print results for the model\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Efficiency: {efficiency:.4f} ({num_associated_cp} associated CPs out of {total_cp} total CPs)\")\n",
    "    print(f\"Purity: {purity:.4f} ({num_associated_tst} associated Tracksters out of {total_tst} total Tracksters)\")\n",
    "    print(f\"Num tracksters ratio: {total_tst / total_cp if total_cp > 0 else 0:.4f}\")\n",
    "    print(f\"Average Energy Ratio: {avg_energy_ratio:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'efficiency': efficiency,\n",
    "        'purity': purity,\n",
    "        'avg_energy_ratio': avg_energy_ratio,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "our_model_metrics = calculate_metrics(df_CL, \"Our Model\")\n",
    "cern_model_metrics = calculate_metrics(df_TICL, \"CERN Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1e001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
