{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fd2ddc",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b6cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "from torch_geometric.nn import knn_graph\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2851a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "\n",
    "class CCV1(Dataset):\n",
    "    r'''\n",
    "    Loads trackster-level features and associations for positive/negative edge creation.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, inp='train'):\n",
    "        super(CCV1, self).__init__(root, transform)\n",
    "        self.inp = inp\n",
    "        self.max_events = max_events\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "        print(\"### Loading tracksters data\")\n",
    "\n",
    "\n",
    "        for path in tqdm.tqdm(self.raw_paths):\n",
    "            print(path)\n",
    "            \n",
    "            tracksters_path = find_highest_branch(path, 'tracksters')\n",
    "            associations_path = find_highest_branch(path, 'associations')\n",
    "            simtrack = find_highest_branch(path, 'simtrackstersCP')\n",
    "            # Load tracksters features in chunks\n",
    "            for array in uproot.iterate(\n",
    "                f\"{path}:{tracksters_path}\",\n",
    "                [\n",
    "                    \"time\", \"raw_energy\",\n",
    "                    \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \n",
    "                    \"barycenter_eta\", \"barycenter_phi\",\n",
    "                    \"EV1\", \"EV2\", \"EV3\",\n",
    "                    \"eVector0_x\", \"eVector0_y\", \"eVector0_z\",\n",
    "                    \"sigmaPCA1\", \"sigmaPCA2\", \"sigmaPCA3\"\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                tmp_time = array[\"time\"]\n",
    "                tmp_raw_energy = array[\"raw_energy\"]\n",
    "                tmp_bx = array[\"barycenter_x\"]\n",
    "                tmp_by = array[\"barycenter_y\"]\n",
    "                tmp_bz = array[\"barycenter_z\"]\n",
    "                tmp_beta = array[\"barycenter_eta\"]\n",
    "                tmp_bphi = array[\"barycenter_phi\"]\n",
    "                tmp_EV1 = array[\"EV1\"]\n",
    "                tmp_EV2 = array[\"EV2\"]\n",
    "                tmp_EV3 = array[\"EV3\"]\n",
    "                tmp_eV0x = array[\"eVector0_x\"]\n",
    "                tmp_eV0y = array[\"eVector0_y\"]\n",
    "                tmp_eV0z = array[\"eVector0_z\"]\n",
    "                tmp_sigma1 = array[\"sigmaPCA1\"]\n",
    "                tmp_sigma2 = array[\"sigmaPCA2\"]\n",
    "                tmp_sigma3 = array[\"sigmaPCA3\"]\n",
    "                \n",
    "                \n",
    "                vert_array = []\n",
    "                for vert_chunk in uproot.iterate(\n",
    "                    f\"{path}:{simtrack}\",\n",
    "                    [\"barycenter_x\"],\n",
    "                ):\n",
    "                    vert_array = vert_chunk[\"barycenter_x\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "\n",
    "                # Now load the associations for the same events/chunk\n",
    "                # 'tsCLUE3D_recoToSim_CP' gives association arrays like [[1,0],[0,1],...]\n",
    "                # Make sure we read from the same events\n",
    "                tmp_array = []\n",
    "                for assoc_chunk in uproot.iterate(\n",
    "                    f\"{path}:{associations_path}\",\n",
    "                    [\"tsCLUE3D_recoToSim_CP\"],\n",
    "                ):\n",
    "                    tmp_array = assoc_chunk[\"tsCLUE3D_recoToSim_CP\"]\n",
    "                    break  # Since we have a matching chunk, no need to continue\n",
    "                \n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in vert_array:\n",
    "                    if 1 <=len(e) <= 5:\n",
    "                        skim_mask.append(True)\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "\n",
    "\n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "                \n",
    "                skim_mask = []\n",
    "                for e in tmp_array:\n",
    "                    if 2 <= len(e):\n",
    "                        skim_mask.append(True)\n",
    "\n",
    "                    elif len(e) == 0:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                    else:\n",
    "                        skim_mask.append(False)\n",
    "\n",
    "                        \n",
    "                tmp_time = tmp_time[skim_mask]\n",
    "                tmp_raw_energy = tmp_raw_energy[skim_mask]\n",
    "                tmp_bx = tmp_bx[skim_mask]\n",
    "                tmp_by = tmp_by[skim_mask]\n",
    "                tmp_bz = tmp_bz[skim_mask]\n",
    "                tmp_beta = tmp_beta[skim_mask]\n",
    "                tmp_bphi = tmp_bphi[skim_mask]\n",
    "                tmp_EV1 = tmp_EV1[skim_mask]\n",
    "                tmp_EV2 = tmp_EV2[skim_mask]\n",
    "                tmp_EV3 = tmp_EV3[skim_mask]\n",
    "                tmp_eV0x = tmp_eV0x[skim_mask]\n",
    "                tmp_eV0y = tmp_eV0y[skim_mask]\n",
    "                tmp_eV0z = tmp_eV0z[skim_mask]\n",
    "                tmp_sigma1 = tmp_sigma1[skim_mask]\n",
    "                tmp_sigma2 = tmp_sigma2[skim_mask]\n",
    "                tmp_sigma3 = tmp_sigma3[skim_mask]\n",
    "                tmp_array = tmp_array[skim_mask]\n",
    "\n",
    "                \n",
    "                # Concatenate or initialize storage\n",
    "                if counter == 0:\n",
    "                    self.time = tmp_time\n",
    "                    self.raw_energy = tmp_raw_energy\n",
    "                    self.bx = tmp_bx\n",
    "                    self.by = tmp_by\n",
    "                    self.bz = tmp_bz\n",
    "                    self.beta = tmp_beta\n",
    "                    self.bphi = tmp_bphi\n",
    "                    self.EV1 = tmp_EV1\n",
    "                    self.EV2 = tmp_EV2\n",
    "                    self.EV3 = tmp_EV3\n",
    "                    self.eV0x = tmp_eV0x\n",
    "                    self.eV0y = tmp_eV0y\n",
    "                    self.eV0z = tmp_eV0z\n",
    "                    self.sigma1 = tmp_sigma1\n",
    "                    self.sigma2 = tmp_sigma2\n",
    "                    self.sigma3 = tmp_sigma3\n",
    "                    self.assoc = tmp_array\n",
    "                else:\n",
    "                    self.time = ak.concatenate((self.time, tmp_time))\n",
    "                    self.raw_energy = ak.concatenate((self.raw_energy, tmp_raw_energy))\n",
    "                    self.bx = ak.concatenate((self.bx, tmp_bx))\n",
    "                    self.by = ak.concatenate((self.by, tmp_by))\n",
    "                    self.bz = ak.concatenate((self.bz, tmp_bz))\n",
    "                    self.beta = ak.concatenate((self.beta, tmp_beta))\n",
    "                    self.bphi = ak.concatenate((self.bphi, tmp_bphi))\n",
    "                    self.EV1 = ak.concatenate((self.EV1, tmp_EV1))\n",
    "                    self.EV2 = ak.concatenate((self.EV2, tmp_EV2))\n",
    "                    self.EV3 = ak.concatenate((self.EV3, tmp_EV3))\n",
    "                    self.eV0x = ak.concatenate((self.eV0x, tmp_eV0x))\n",
    "                    self.eV0y = ak.concatenate((self.eV0y, tmp_eV0y))\n",
    "                    self.eV0z = ak.concatenate((self.eV0z, tmp_eV0z))\n",
    "                    self.sigma1 = ak.concatenate((self.sigma1, tmp_sigma1))\n",
    "                    self.sigma2 = ak.concatenate((self.sigma2, tmp_sigma2))\n",
    "                    self.sigma3 = ak.concatenate((self.sigma3, tmp_sigma3))\n",
    "                    self.assoc = ak.concatenate((self.assoc, tmp_array))\n",
    "\n",
    "                counter += len(tmp_bx)\n",
    "                if counter >= max_events:\n",
    "                    print(f\"Reached {max_events} events!\")\n",
    "                    break\n",
    "            if counter >= max_events:\n",
    "                break\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            f'Dataset not found. Please download it from {self.url} and move all '\n",
    "            f'*.root files to {self.raw_dir}')\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.time)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        def reconstruct_array(grouped_indices):\n",
    "            # Find the maximum index to determine the array length\n",
    "            max_index = max(max(indices) for indices in grouped_indices.values())\n",
    "\n",
    "            # Initialize an array with the correct size, filled with a placeholder (e.g., -1)\n",
    "            reconstructed = [-1] * (max_index + 1)\n",
    "\n",
    "            # Populate the array based on the dictionary\n",
    "            for value, indices in grouped_indices.items():\n",
    "                for idx in indices:\n",
    "                    reconstructed[idx] = value\n",
    "\n",
    "            return reconstructed\n",
    "        # Extract per-event arrays\n",
    "        event_time = self.time[idx]\n",
    "        event_raw_energy = self.raw_energy[idx]\n",
    "        event_bx = self.bx[idx]\n",
    "        event_by = self.by[idx]\n",
    "        event_bz = self.bz[idx]\n",
    "        event_beta = self.beta[idx]\n",
    "        event_bphi = self.bphi[idx]\n",
    "        event_EV1 = self.EV1[idx]\n",
    "        event_EV2 = self.EV2[idx]\n",
    "        event_EV3 = self.EV3[idx]\n",
    "        event_eV0x = self.eV0x[idx]\n",
    "        event_eV0y = self.eV0y[idx]\n",
    "        event_eV0z = self.eV0z[idx]\n",
    "        event_sigma1 = self.sigma1[idx]\n",
    "        event_sigma2 = self.sigma2[idx]\n",
    "        event_sigma3 = self.sigma3[idx]\n",
    "        event_assoc = self.assoc[idx]  # Each is now an array (e.g., [0, 1, 2]) indicating the pion group\n",
    "\n",
    "        # Convert each to numpy arrays if needed\n",
    "        event_time = np.array(event_time)\n",
    "        event_raw_energy = np.array(event_raw_energy)\n",
    "        event_bx = np.array(event_bx)\n",
    "        event_by = np.array(event_by)\n",
    "        event_bz = np.array(event_bz)\n",
    "        event_beta = np.array(event_beta)\n",
    "        event_bphi = np.array(event_bphi)\n",
    "        event_EV1 = np.array(event_EV1)\n",
    "        event_EV2 = np.array(event_EV2)\n",
    "        event_EV3 = np.array(event_EV3)\n",
    "        event_eV0x = np.array(event_eV0x)\n",
    "        event_eV0y = np.array(event_eV0y)\n",
    "        event_eV0z = np.array(event_eV0z)\n",
    "        event_sigma1 = np.array(event_sigma1)\n",
    "        event_sigma2 = np.array(event_sigma2)\n",
    "        event_sigma3 = np.array(event_sigma3)\n",
    "        event_assoc = np.array(event_assoc)  # e.g. [[0,1,2], [2,0,1], [1,0,2], ...]\n",
    "\n",
    "        # Combine features\n",
    "        flat_feats = np.column_stack((\n",
    "            event_bx, event_by, event_bz, event_raw_energy,\n",
    "            event_beta, event_bphi,\n",
    "            event_EV1, event_EV2, event_EV3,\n",
    "            event_eV0x, event_eV0y, event_eV0z,\n",
    "            event_sigma1, event_sigma2, event_sigma3\n",
    "        ))\n",
    "        x = torch.from_numpy(flat_feats).float()\n",
    "        assoc = event_assoc\n",
    "\n",
    "        total_tracksters = len(event_time)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple. Two tracksters belong\n",
    "        # to the same pion group if their association arrays (converted to tuples)\n",
    "        # match.\n",
    "        # --------------------------------------------------------------------\n",
    "        # Group tracksters by their association tuple\n",
    "         # Group tracksters by the first element of event_assoc\n",
    "        assoc_groups = {}\n",
    "        for i, assoc in enumerate(event_assoc):\n",
    "            key = assoc[0]  # Only use the first element as the key\n",
    "            if key not in assoc_groups:\n",
    "                assoc_groups[key] = []\n",
    "            assoc_groups[key].append(i)\n",
    "        assoc_array = reconstruct_array(assoc_groups)\n",
    "        pos_edges = []\n",
    "        neg_edges = []\n",
    "        # Ensure positive edges always connect to another trackster in the same group if possible\n",
    "        for i in range(total_tracksters):\n",
    "            key = event_assoc[i][0]  # Get first element as group identifier\n",
    "            same_group = assoc_groups[key]\n",
    "\n",
    "            # --- Positive edge ---\n",
    "            if len(same_group) > 1:\n",
    "                # Always select another trackster from the same group\n",
    "                pos_target = random.choice([j for j in same_group if j != i])\n",
    "            else:\n",
    "                # No other trackster in the group, form a self-loop\n",
    "                pos_target = i\n",
    "            pos_edges.append([i, pos_target])\n",
    "\n",
    "            # --- Negative edge ---\n",
    "            neg_candidates = [j for j in range(total_tracksters) if event_assoc[j][0] != key]\n",
    "            if neg_candidates:\n",
    "                neg_target = random.choice(neg_candidates)\n",
    "            else:\n",
    "                neg_target = i\n",
    "            neg_edges.append([i, neg_target])\n",
    "\n",
    "        x_pos_edge = torch.tensor(pos_edges, dtype=torch.long)\n",
    "        x_neg_edge = torch.tensor(neg_edges, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, x_pe=x_pos_edge, x_ne=x_neg_edge, assoc = assoc_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361a2ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/train/raw/train.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:42<00:00, 42.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/val/raw/2k5pi.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 1/2 [00:01<00:01,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/val/raw/8k5pi.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "ipath = \"/vols/cms/mm1221/Data/100k/5pi/train/\"\n",
    "vpath = \"/vols/cms/mm1221/Data/100k/5pi/val/\"\n",
    "\n",
    "data_train = CCV1(ipath, max_events=80000, inp = 'train')\n",
    "data_val = CCV1(vpath, max_events=10000, inp='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9097cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load explicitely, used for analysis and plots\n",
    "data_path = '/vols/cms/mm1221/Data/100k/5pi/test/raw/test.root'\n",
    "data_file = uproot.open(data_path)\n",
    "\n",
    "Track_ind = data_file['tracksters;2']['vertices_indexes'].array()\n",
    "GT_ind = data_file['simtrackstersCP;3']['vertices_indexes'].array()\n",
    "GT_mult = data_file['simtrackstersCP;3']['vertices_multiplicity'].array()\n",
    "GT_bc = data_file['simtrackstersCP;3']['barycenter_x'].array()\n",
    "energies = data_file['clusters;4']['energy'].array()\n",
    "LC_x = data_file['clusters;4']['position_x'].array()\n",
    "LC_y = data_file['clusters;4']['position_y'].array()\n",
    "LC_z = data_file['clusters;4']['position_z'].array()\n",
    "LC_eta = data_file['clusters;4']['position_eta'].array()\n",
    "MT_ind = data_file['trackstersMerged;2']['vertices_indexes'].array()\n",
    "\n",
    "#1.3 Filter so get rid of events with 0 calo particles\n",
    "skim_mask = []\n",
    "for e in GT_bc:\n",
    "    if 5 <= len(e) <=5 :\n",
    "        skim_mask.append(True)\n",
    "    else:\n",
    "        skim_mask.append(False)\n",
    "\n",
    "Track_ind = Track_ind[skim_mask]\n",
    "GT_ind = GT_ind[skim_mask]\n",
    "GT_mult = GT_mult[skim_mask]\n",
    "energies = energies[skim_mask]\n",
    "LC_x = LC_x[skim_mask]\n",
    "LC_y = LC_y[skim_mask]\n",
    "LC_z = LC_z[skim_mask]\n",
    "LC_eta = LC_eta[skim_mask]\n",
    "MT_ind = MT_ind[skim_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33374a88",
   "metadata": {},
   "source": [
    "# Initialise the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7844e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomStaticEdgeConv(nn.Module):\n",
    "    def __init__(self, nn_module):\n",
    "        super(CustomStaticEdgeConv, self).__init__()\n",
    "        self.nn_module = nn_module\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features of shape (N, F).\n",
    "            edge_index (torch.Tensor): Predefined edges [2, E], where E is the number of edges.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Node features after static edge aggregation.\n",
    "        \"\"\"\n",
    "        row, col = edge_index  # Extract row (source) and col (target) nodes\n",
    "        x_center = x[row]\n",
    "        x_neighbor = x[col]\n",
    "\n",
    "        # Compute edge features (relative)\n",
    "        edge_features = torch.cat([x_center, x_neighbor - x_center], dim=-1)\n",
    "        edge_features = self.nn_module(edge_features)\n",
    "\n",
    "        # Aggregate features back to nodes\n",
    "        num_nodes = x.size(0)\n",
    "        node_features = torch.zeros(num_nodes, edge_features.size(-1), device=x.device)\n",
    "        node_features.index_add_(0, row, edge_features)\n",
    "\n",
    "        # Normalization (Divide by node degrees)\n",
    "        counts = torch.bincount(row, minlength=num_nodes).clamp(min=1).view(-1, 1)\n",
    "        node_features = node_features / counts\n",
    "\n",
    "        return node_features\n",
    "\n",
    "\n",
    "class CustomGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=1, concat=True, dropout=0.6, alpha=0.4):\n",
    "        \"\"\"\n",
    "        Initializes the Custom GAT Layer.\n",
    "\n",
    "        Args:\n",
    "            in_dim (int): Input feature dimension.\n",
    "            out_dim (int): Output feature dimension per head.\n",
    "            heads (int): Number of attention heads.\n",
    "            concat (bool): Whether to concatenate the heads' output or average them.\n",
    "            dropout (float): Dropout rate on attention coefficients.\n",
    "            alpha (float): Negative slope for LeakyReLU.\n",
    "        \"\"\"\n",
    "        super(CustomGATLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "\n",
    "        # Linear transformation for node features\n",
    "        self.W = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "\n",
    "        # Attention mechanism: a vector for each head\n",
    "        self.a_src = nn.Parameter(torch.zeros(heads, out_dim))\n",
    "        self.a_tgt = nn.Parameter(torch.zeros(heads, out_dim))\n",
    "        nn.init.xavier_uniform_(self.a_src.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a_tgt.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Optional batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(heads * out_dim) if concat else nn.BatchNorm1d(out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node features of shape (N, in_dim).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated node features after attention-based aggregation.\n",
    "        \"\"\"\n",
    "        src, tgt = edge_index  # Source and target node indices\n",
    "        N = x.size(0)\n",
    "\n",
    "        # Apply linear transformation and reshape for multi-head attention\n",
    "        h = self.W(x)  # Shape: (N, heads * out_dim)\n",
    "        h = h.view(N, self.heads, self.out_dim)  # Shape: (N, heads, out_dim)\n",
    "\n",
    "        # Gather node features for each edge\n",
    "        h_src = h[src]  # Shape: (E, heads, out_dim)\n",
    "        h_tgt = h[tgt]  # Shape: (E, heads, out_dim)\n",
    "\n",
    "        # Compute attention coefficients using separate vectors for source and target\n",
    "        e_src = (h_src * self.a_src).sum(dim=-1)  # Shape: (E, heads)\n",
    "        e_tgt = (h_tgt * self.a_tgt).sum(dim=-1)  # Shape: (E, heads)\n",
    "        e = self.leakyrelu(e_src + e_tgt)  # Shape: (E, heads)\n",
    "\n",
    "        # Compute softmax normalization for attention coefficients\n",
    "        # To ensure numerical stability\n",
    "        e = e - e.max(dim=0, keepdim=True)[0]\n",
    "        alpha = torch.exp(e)  # Shape: (E, heads)\n",
    "\n",
    "        # Sum of attention coefficients for each target node and head\n",
    "        alpha_sum = torch.zeros(N, self.heads, device=x.device).scatter_add_(0, tgt.unsqueeze(-1).expand(-1, self.heads), alpha)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        alpha_sum = alpha_sum + 1e-16\n",
    "\n",
    "        # Normalize attention coefficients\n",
    "        alpha = alpha / alpha_sum[tgt]  # Shape: (E, heads)\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        # Weighted aggregation of source node features\n",
    "        h_prime = h_src * alpha.unsqueeze(-1)  # Shape: (E, heads, out_dim)\n",
    "\n",
    "        # Initialize output tensor and aggregate\n",
    "        out = torch.zeros(N, self.heads, self.out_dim, device=x.device)\n",
    "        out.scatter_add_(0, tgt.unsqueeze(-1).unsqueeze(-1).expand(-1, self.heads, self.out_dim), h_prime)  # Shape: (N, heads, out_dim)\n",
    "\n",
    "        # Concatenate or average the heads\n",
    "        if self.concat:\n",
    "            out = out.view(N, self.heads * self.out_dim)  # Shape: (N, heads*out_dim)\n",
    "        else:\n",
    "            out = out.mean(dim=1)  # Shape: (N, out_dim)\n",
    "\n",
    "        # Apply batch normalization\n",
    "        out = self.batch_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, num_layers=4, dropout=0.3, contrastive_dim=8, heads=4):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with alternating StaticEdgeConv and GAT layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of hidden layers.\n",
    "            num_layers (int): Total number of convolutional layers (both StaticEdgeConv and GAT).\n",
    "            dropout (float): Dropout rate.\n",
    "            contrastive_dim (int): Dimension of the contrastive output.\n",
    "            heads (int): Number of attention heads in GAT layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.contrastive_dim = contrastive_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        # Input encoder\n",
    "        self.lc_encode = nn.Sequential(\n",
    "            nn.Linear(15, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the network's convolutional layers, alternating between StaticEdgeConv and GAT\n",
    "        self.convs = nn.ModuleList()\n",
    "        for layer_idx in range(num_layers):\n",
    "            if layer_idx % 2 == 0:\n",
    "                # Even-indexed layers: StaticEdgeConv\n",
    "                conv = CustomStaticEdgeConv(\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                        nn.ELU(),\n",
    "                        nn.BatchNorm1d(hidden_dim),\n",
    "                        nn.Dropout(p=dropout)\n",
    "                    )\n",
    "                )\n",
    "                self.convs.append(conv)\n",
    "            else:\n",
    "                # Odd-indexed layers: GAT\n",
    "                gat = CustomGATLayer(\n",
    "                    in_dim=hidden_dim,\n",
    "                    out_dim=hidden_dim // heads if heads > 1 else hidden_dim,\n",
    "                    heads=heads,\n",
    "                    concat=True,\n",
    "                    dropout=0.6,\n",
    "                    alpha=0.4\n",
    "                )\n",
    "                self.convs.append(gat)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(16, contrastive_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features of shape (N, 15).\n",
    "            edge_index (torch.Tensor): Edge indices of shape (2, E).\n",
    "            batch (torch.Tensor): Batch vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features after processing.\n",
    "            torch.Tensor: Batch vector.\n",
    "        \"\"\"\n",
    "        # Input encoding\n",
    "        x_lc_enc = self.lc_encode(x)  # Shape: (N, hidden_dim)\n",
    "\n",
    "        # Apply convolutional layers with residual connections\n",
    "        feats = x_lc_enc\n",
    "        for idx, conv in enumerate(self.convs):\n",
    "            feats = conv(feats, edge_index) + feats  # Residual connection\n",
    "\n",
    "        # Final output\n",
    "        out = self.output(feats)\n",
    "        return out, batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60a508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with passed hyperparameters\n",
    "model = Net(\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    dropout=0.3,\n",
    "    contrastive_dim=64,\n",
    "    heads=16\n",
    ")\n",
    "\n",
    "k = 16\n",
    "BS = 64\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0caf30",
   "metadata": {},
   "source": [
    "# Define The Loss term and the Training + Val Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da053f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def contrastive_loss_random(embeddings, pos_indices, group_ids, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Contrastive loss using a randomly selected negative.\n",
    "    \"\"\"\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    group_ids = group_ids.long()\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        anchor = embeddings[i]\n",
    "        positive = embeddings[pos_indices[i]]\n",
    "        neg_mask = (group_ids != group_ids[i])\n",
    "        if neg_mask.sum() == 0:\n",
    "            continue\n",
    "        negatives = embeddings[neg_mask]\n",
    "        # Randomly sample one negative from the candidates.\n",
    "        rand_idx = torch.randint(0, negatives.size(0), (1,)).item()\n",
    "        neg_sample = negatives[rand_idx]\n",
    "        \n",
    "        pos_sim = F.cosine_similarity(anchor.unsqueeze(0), positive.unsqueeze(0))\n",
    "        neg_sim = F.cosine_similarity(anchor.unsqueeze(0), neg_sample.unsqueeze(0))\n",
    "        \n",
    "        loss = -torch.log(\n",
    "            torch.exp(pos_sim/temperature) / (torch.exp(pos_sim/temperature) + torch.exp(neg_sim/temperature))\n",
    "        )\n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "    return loss_sum / count if count > 0 else torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def contrastive_loss_hard(embeddings, pos_indices, group_ids, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Contrastive loss using hard negative mining.\n",
    "    \"\"\"\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    group_ids = group_ids.long()\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        anchor = embeddings[i]\n",
    "        positive = embeddings[pos_indices[i]]\n",
    "        neg_mask = (group_ids != group_ids[i])\n",
    "        if neg_mask.sum() == 0:\n",
    "            continue\n",
    "        negatives = embeddings[neg_mask]\n",
    "        # Hard negative: the candidate with maximum cosine similarity.\n",
    "        cos_sim = F.cosine_similarity(anchor.unsqueeze(0), negatives)\n",
    "        hard_neg_sim = cos_sim.max()\n",
    "        \n",
    "        pos_sim = F.cosine_similarity(anchor.unsqueeze(0), positive.unsqueeze(0))\n",
    "        \n",
    "        loss = -torch.log(\n",
    "            torch.exp(pos_sim/temperature) / (torch.exp(pos_sim/temperature) + torch.exp(hard_neg_sim/temperature))\n",
    "        )\n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "    return loss_sum / count if count > 0 else torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def contrastive_loss_curriculum(embeddings, pos_indices, group_ids, temperature=0.3, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Blends the random-negative loss and the hard-negative loss.\n",
    "    When alpha=0, uses only random negatives (easy scenario);\n",
    "    When alpha=1, uses only hard negatives.\n",
    "    \"\"\"\n",
    "    loss_random = contrastive_loss_random(embeddings, pos_indices, group_ids, temperature)\n",
    "    loss_hard = contrastive_loss_hard(embeddings, pos_indices, group_ids, temperature)\n",
    "    return (1 - alpha) * loss_random + alpha * loss_hard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c157d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################\n",
    "# OLD CONTRASTIVE LOSS & TRAIN/TEST FUNCTIONS\n",
    "##############################################\n",
    "\n",
    "def contrastive_loss(start_all, end_all, temperature=0.1):\n",
    "    # Normalize the start and end embeddings\n",
    "    z_start = F.normalize(start_all, dim=1)\n",
    "    z_end = F.normalize(end_all, dim=1)\n",
    "    # Split into positive and negative halves\n",
    "    half = int(len(z_start) / 2)\n",
    "    positives = torch.exp(F.cosine_similarity(z_start[:half], z_end[:half], dim=1))\n",
    "    negatives = torch.exp(F.cosine_similarity(z_start[half:], z_end[half:], dim=1))\n",
    "    nominator = positives / temperature\n",
    "    denominator = negatives\n",
    "    loss = torch.exp(-nominator.sum() / denominator.sum())\n",
    "    return loss\n",
    "\n",
    "def train_old(train_loader, model, optimizer, device, k_value):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in tqdm.tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        out = model(data.x, edge_index, data.x_batch)\n",
    "\n",
    "        # Get event partitioning information.\n",
    "        values, counts = np.unique(data.x_batch.detach().cpu().numpy(), return_counts=True)\n",
    "        losses = []\n",
    "        for e in range(len(counts)):\n",
    "            lower_edge = 0 if e == 0 else np.sum(counts[:e])\n",
    "            upper_edge = lower_edge + counts[e]\n",
    "\n",
    "            start_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 0]]\n",
    "            end_pos   = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 1]]\n",
    "            start_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 0]]\n",
    "            end_neg   = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 1]]\n",
    "\n",
    "            start_all = torch.cat((start_pos, start_neg), 0)\n",
    "            end_all   = torch.cat((end_pos, end_neg), 0)\n",
    "\n",
    "            if len(losses) == 0:\n",
    "                losses.append(contrastive_loss(start_all, end_all, 0.3))\n",
    "            else:\n",
    "                losses.append(losses[-1] + contrastive_loss(start_all, end_all, 0.3))\n",
    "\n",
    "        loss = losses[-1]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_old(test_loader, model, device, k_value):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for data in tqdm.tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        out = model(data.x, edge_index, data.x_batch)\n",
    "        values, counts = np.unique(data.x_batch.detach().cpu().numpy(), return_counts=True)\n",
    "        losses = []\n",
    "        for e in range(len(counts)):\n",
    "            lower_edge = 0 if e == 0 else np.sum(counts[:e])\n",
    "            upper_edge = lower_edge + counts[e]\n",
    "\n",
    "            start_pos = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 0]]\n",
    "            end_pos   = out[0][lower_edge:upper_edge][data.x_pe[lower_edge:upper_edge, 1]]\n",
    "            start_neg = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 0]]\n",
    "            end_neg   = out[0][lower_edge:upper_edge][data.x_ne[lower_edge:upper_edge, 1]]\n",
    "\n",
    "            start_all = torch.cat((start_pos, start_neg), 0)\n",
    "            end_all   = torch.cat((end_pos, end_neg), 0)\n",
    "\n",
    "            if len(losses) == 0:\n",
    "                losses.append(contrastive_loss(start_all, end_all, 0.3))\n",
    "            else:\n",
    "                losses.append(losses[-1] + contrastive_loss(start_all, end_all, 0.3))\n",
    "        loss = losses[-1]\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "##############################################\n",
    "# NEW (Curriculum) Loss & TRAIN/TEST FUNCTIONS\n",
    "##############################################\n",
    "\n",
    "def contrastive_loss_random(embeddings, pos_indices, group_ids, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Contrastive loss using a randomly selected negative.\n",
    "    \"\"\"\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    group_ids = group_ids.long()\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        anchor = embeddings[i]\n",
    "        positive = embeddings[pos_indices[i]]\n",
    "        neg_mask = (group_ids != group_ids[i])\n",
    "        if neg_mask.sum() == 0:\n",
    "            continue\n",
    "        negatives = embeddings[neg_mask]\n",
    "        # Randomly sample one negative.\n",
    "        rand_idx = torch.randint(0, negatives.size(0), (1,)).item()\n",
    "        neg_sample = negatives[rand_idx]\n",
    "        pos_sim = F.cosine_similarity(anchor.unsqueeze(0), positive.unsqueeze(0))\n",
    "        neg_sim = F.cosine_similarity(anchor.unsqueeze(0), neg_sample.unsqueeze(0))\n",
    "        loss = -torch.log(\n",
    "            torch.exp(pos_sim/temperature) / (torch.exp(pos_sim/temperature) + torch.exp(neg_sim/temperature))\n",
    "        )\n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "    return loss_sum / count if count > 0 else torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def contrastive_loss_hard(embeddings, pos_indices, group_ids, temperature=0.3):\n",
    "    \"\"\"\n",
    "    Contrastive loss using hard negative mining (loop version).\n",
    "    \"\"\"\n",
    "    loss_sum = 0.0\n",
    "    count = 0\n",
    "    group_ids = group_ids.long()\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        anchor = embeddings[i]\n",
    "        positive = embeddings[pos_indices[i]]\n",
    "        neg_mask = (group_ids != group_ids[i])\n",
    "        if neg_mask.sum() == 0:\n",
    "            continue\n",
    "        negatives = embeddings[neg_mask]\n",
    "        cos_sim = F.cosine_similarity(anchor.unsqueeze(0), negatives)\n",
    "        hard_neg_sim = cos_sim.max()\n",
    "        pos_sim = F.cosine_similarity(anchor.unsqueeze(0), positive.unsqueeze(0))\n",
    "        loss = -torch.log(\n",
    "            torch.exp(pos_sim/temperature) / (torch.exp(pos_sim/temperature) + torch.exp(hard_neg_sim/temperature))\n",
    "        )\n",
    "        loss_sum += loss\n",
    "        count += 1\n",
    "    return loss_sum / count if count > 0 else torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "def contrastive_loss_curriculum(embeddings, pos_indices, group_ids, temperature=0.3, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Blends the random-negative loss and the hard-negative loss.\n",
    "    When alpha=0: only random negatives; when alpha=1: only hard negatives.\n",
    "    \"\"\"\n",
    "    loss_random = contrastive_loss_random(embeddings, pos_indices, group_ids, temperature)\n",
    "    loss_hard = contrastive_loss_hard(embeddings, pos_indices, group_ids, temperature)\n",
    "    return (1 - alpha) * loss_random + alpha * loss_hard\n",
    "\n",
    "def train_new(train_loader, model, optimizer, device, k_value, alpha):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in tqdm.tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert data.assoc to tensor if needed.\n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "\n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        # Partition batch by event.\n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            loss_event = contrastive_loss_curriculum(event_embeddings, event_pos_indices, event_group_ids,\n",
    "                                                     temperature=0.3, alpha=alpha)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        loss = loss_event_total / len(counts)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_new(test_loader, model, device, k_value):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for data in tqdm.tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        if isinstance(data.assoc, list):\n",
    "            if isinstance(data.assoc[0], list):\n",
    "                assoc_tensor = torch.cat([torch.tensor(a, dtype=torch.int64, device=data.x.device)\n",
    "                                          for a in data.assoc])\n",
    "            else:\n",
    "                assoc_tensor = torch.tensor(data.assoc, device=data.x.device)\n",
    "        else:\n",
    "            assoc_tensor = data.assoc\n",
    "        \n",
    "        edge_index = knn_graph(data.x[:, :3], k=k_value, batch=data.x_batch)\n",
    "        embeddings, _ = model(data.x, edge_index, data.x_batch)\n",
    "        \n",
    "        batch_np = data.x_batch.detach().cpu().numpy()\n",
    "        _, counts = np.unique(batch_np, return_counts=True)\n",
    "        \n",
    "        loss_event_total = 0.0\n",
    "        start_idx = 0\n",
    "        for count in counts:\n",
    "            end_idx = start_idx + count\n",
    "            event_embeddings = embeddings[start_idx:end_idx]\n",
    "            event_group_ids = assoc_tensor[start_idx:end_idx]\n",
    "            event_pos_indices = data.x_pe[start_idx:end_idx, 1].view(-1)\n",
    "            # For testing, we can use the hard-negative loss.\n",
    "            loss_event = contrastive_loss_hard(event_embeddings, event_pos_indices, event_group_ids, temperature=0.3)\n",
    "            loss_event_total += loss_event\n",
    "            start_idx = end_idx\n",
    "        total_loss += loss_event_total / len(counts)\n",
    "    return total_loss / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d61a65",
   "metadata": {},
   "source": [
    "# Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fd56e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [01:49<00:00,  1.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Method: Train Loss: 0.0083, Val Loss: 0.0020\n",
      "Epoch 1/20 - Train Loss: 0.008330954226158543, Validation Loss: 0.002001957932555419\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [01:44<00:00,  1.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Method: Train Loss: 0.0019, Val Loss: 0.0009\n",
      "Epoch 2/20 - Train Loss: 0.0019222859552106676, Validation Loss: 0.0009232569384646476\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [09:17<00:00,  2.88s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:12<00:00,  2.04it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_921663/3482734383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Transition Method (alpha={alpha:.2f}): Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# For epochs 31-40, use the new method with hard negatives (alpha=1).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_meta\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Load DataLoader with current batch_size\n",
    "train_loader = DataLoader(data_train, batch_size=BS, shuffle=True, follow_batch=['x'])\n",
    "val_loader = DataLoader(data_val, batch_size=BS, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "# Train and evaluate the model for the specified number of epochs\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Store train and validation losses for all epochs\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "output_dir = '/vols/cms/mm1221/hgcal/pion5New/Track/resultsEndHard/SEGATxyz/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    # For epochs 1-20 use the old loss method.\n",
    "    if epoch < 2:\n",
    "        train_loss = train_old(train_loader, model, optimizer, device, k)\n",
    "        val_loss = test_old(val_loader, model, device, k)\n",
    "        print(f\"Old Method: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    # For epochs 21-30, gradually shift from random negatives (alpha=0) to hard negatives (alpha=1).\n",
    "    elif epoch < 4:\n",
    "        alpha = (epoch - 2) / 2.0   # linearly increase from 0 to 1\n",
    "        train_loss = train_new(train_loader, model, optimizer, device, k, alpha)\n",
    "        val_loss = test_new(val_loader, model, device, k)\n",
    "        print(f\"Transition Method (alpha={alpha:.2f}): Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    # For epochs 31-40, use the new method with hard negatives (alpha=1).\n",
    "    else:\n",
    "        alpha = 1.0\n",
    "        train_loss = train_new(train_loader, model, optimizer, device, k, alpha)\n",
    "        val_loss = test_new(val_loader, model, device, k)\n",
    "        print(f\"New Method (alpha={alpha}): Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the best model if this epoch's validation loss is lower\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model.pt'))\n",
    "\n",
    "    # Save intermediate state dictionaries\n",
    "    state_dicts = {'model': model.state_dict(), 'opt': optimizer.state_dict(), 'lr': scheduler.state_dict()}\n",
    "    torch.save(state_dicts, os.path.join(output_dir, f'epoch-{epoch}.pt'))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "\n",
    "\n",
    "# Save training and validation loss curves\n",
    "loss_result_filename = (\n",
    "    'result.csv'\n",
    ")\n",
    "\n",
    "# Dynamically adjust the epoch range to match the length of train_losses and val_losses\n",
    "results_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_losses) + 1)),  # Adjusted to the actual length of losses\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses\n",
    "})\n",
    "\n",
    "# Save to a CSV file in the output directory\n",
    "results_df.to_csv(os.path.join(output_dir, loss_result_filename), index=False)\n",
    "\n",
    "print(f'Saved training and validation losses to {os.path.join(output_dir, loss_result_filename)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c4be4",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5979a",
   "metadata": {},
   "source": [
    "## Loading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "280da9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/100k/5pi/test/raw/test.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.40s/it]\n"
     ]
    }
   ],
   "source": [
    "testpath = \"/vols/cms/mm1221/Data/100k/5pi/test/\"\n",
    "\n",
    "data_test = CCV1(testpath, max_events=10000, inp='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a962aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (lc_encode): Sequential(\n",
       "    (0): Linear(in_features=15, out_features=128, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): CustomStaticEdgeConv(\n",
       "      (nn_module): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): CustomGATLayer(\n",
       "      (W): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (leakyrelu): LeakyReLU(negative_slope=0.4)\n",
       "      (dropout): Dropout(p=0.6, inplace=False)\n",
       "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): CustomStaticEdgeConv(\n",
       "      (nn_module): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (1): ELU(alpha=1.0)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): CustomGATLayer(\n",
       "      (W): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (leakyrelu): LeakyReLU(negative_slope=0.4)\n",
       "      (dropout): Dropout(p=0.6, inplace=False)\n",
       "      (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=16, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint= torch.load('/vols/cms/mm1221/hgcal/pion5New/Track/StaticEdge/results/SEGAT/results_lr0.001_bs64_hd128_nl4_do0.3_k16_cd64/best_model.pt',  map_location=torch.device('cpu'))\n",
    "checkpoint= torch.load('/vols/cms/mm1221/hgcal/pion5New/Track/NegativeMining/resultsRandHard/best_model.pt',  map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint)  \n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7341948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244483/2736649685.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_predictions = np.array(all_predictions)\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []  \n",
    "\n",
    "\n",
    "for i, data in enumerate(data_test):\n",
    "    edge_index = knn_graph(data.x[:, :3], k=k)  \n",
    "    predictions = model(data.x, edge_index, 1)\n",
    "    all_predictions.append(predictions[0].detach().cpu().numpy())  \n",
    "    \n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f181c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 1/1495...\n",
      "Processing event 2/1495...\n",
      "Processing event 3/1495...\n",
      "Processing event 4/1495...\n",
      "Processing event 5/1495...\n",
      "Processing event 6/1495...\n",
      "Processing event 7/1495...\n",
      "Processing event 8/1495...\n",
      "Processing event 9/1495...\n",
      "Processing event 10/1495...\n",
      "Processing event 11/1495...\n",
      "Processing event 12/1495...\n",
      "Processing event 13/1495...\n",
      "Processing event 14/1495...\n",
      "Processing event 15/1495...\n",
      "Processing event 16/1495...\n",
      "Processing event 17/1495...\n",
      "Processing event 18/1495...\n",
      "Processing event 19/1495...\n",
      "Processing event 20/1495...\n",
      "Processing event 21/1495...\n",
      "Processing event 22/1495...\n",
      "Processing event 23/1495...\n",
      "Processing event 24/1495...\n",
      "Processing event 25/1495...\n",
      "Processing event 26/1495...\n",
      "Processing event 27/1495...\n",
      "Processing event 28/1495...\n",
      "Processing event 29/1495...\n",
      "Processing event 30/1495...\n",
      "Processing event 31/1495...\n",
      "Processing event 32/1495...\n",
      "Processing event 33/1495...\n",
      "Processing event 34/1495...\n",
      "Processing event 35/1495...\n",
      "Processing event 36/1495...\n",
      "Processing event 37/1495...\n",
      "Processing event 38/1495...\n",
      "Processing event 39/1495...\n",
      "Processing event 40/1495...\n",
      "Processing event 41/1495...\n",
      "Processing event 42/1495...\n",
      "Processing event 43/1495...\n",
      "Processing event 44/1495...\n",
      "Processing event 45/1495...\n",
      "Processing event 46/1495...\n",
      "Processing event 47/1495...\n",
      "Processing event 48/1495...\n",
      "Processing event 49/1495...\n",
      "Processing event 50/1495...\n",
      "Processing event 51/1495...\n",
      "Processing event 52/1495...\n",
      "Processing event 53/1495...\n",
      "Processing event 54/1495...\n",
      "Processing event 55/1495...\n",
      "Processing event 56/1495...\n",
      "Processing event 57/1495...\n",
      "Processing event 58/1495...\n",
      "Processing event 59/1495...\n",
      "Processing event 60/1495...\n",
      "Processing event 61/1495...\n",
      "Processing event 62/1495...\n",
      "Processing event 63/1495...\n",
      "Processing event 64/1495...\n",
      "Processing event 65/1495...\n",
      "Processing event 66/1495...\n",
      "Processing event 67/1495...\n",
      "Processing event 68/1495...\n",
      "Processing event 69/1495...\n",
      "Processing event 70/1495...\n",
      "Processing event 71/1495...\n",
      "Processing event 72/1495...\n",
      "Processing event 73/1495...\n",
      "Processing event 74/1495...\n",
      "Processing event 75/1495...\n",
      "Processing event 76/1495...\n",
      "Processing event 77/1495...\n",
      "Processing event 78/1495...\n",
      "Processing event 79/1495...\n",
      "Processing event 80/1495...\n",
      "Processing event 81/1495...\n",
      "Processing event 82/1495...\n",
      "Processing event 83/1495...\n",
      "Processing event 84/1495...\n",
      "Processing event 85/1495...\n",
      "Processing event 86/1495...\n",
      "Processing event 87/1495...\n",
      "Processing event 88/1495...\n",
      "Processing event 89/1495...\n",
      "Processing event 90/1495...\n",
      "Processing event 91/1495...\n",
      "Processing event 92/1495...\n",
      "Processing event 93/1495...\n",
      "Processing event 94/1495...\n",
      "Processing event 95/1495...\n",
      "Processing event 96/1495...\n",
      "Processing event 97/1495...\n",
      "Processing event 98/1495...\n",
      "Processing event 99/1495...\n",
      "Processing event 100/1495...\n",
      "Processing event 101/1495...\n",
      "Processing event 102/1495...\n",
      "Processing event 103/1495...\n",
      "Processing event 104/1495...\n",
      "Processing event 105/1495...\n",
      "Processing event 106/1495...\n",
      "Processing event 107/1495...\n",
      "Processing event 108/1495...\n",
      "Processing event 109/1495...\n",
      "Processing event 110/1495...\n",
      "Processing event 111/1495...\n",
      "Processing event 112/1495...\n",
      "Processing event 113/1495...\n",
      "Processing event 114/1495...\n",
      "Processing event 115/1495...\n",
      "Processing event 116/1495...\n",
      "Processing event 117/1495...\n",
      "Processing event 118/1495...\n",
      "Processing event 119/1495...\n",
      "Processing event 120/1495...\n",
      "Processing event 121/1495...\n",
      "Processing event 122/1495...\n",
      "Processing event 123/1495...\n",
      "Processing event 124/1495...\n",
      "Processing event 125/1495...\n",
      "Processing event 126/1495...\n",
      "Processing event 127/1495...\n",
      "Processing event 128/1495...\n",
      "Processing event 129/1495...\n",
      "Processing event 130/1495...\n",
      "Processing event 131/1495...\n",
      "Processing event 132/1495...\n",
      "Processing event 133/1495...\n",
      "Processing event 134/1495...\n",
      "Processing event 135/1495...\n",
      "Processing event 136/1495...\n",
      "Processing event 137/1495...\n",
      "Processing event 138/1495...\n",
      "Processing event 139/1495...\n",
      "Processing event 140/1495...\n",
      "Processing event 141/1495...\n",
      "Processing event 142/1495...\n",
      "Processing event 143/1495...\n",
      "Processing event 144/1495...\n",
      "Processing event 145/1495...\n",
      "Processing event 146/1495...\n",
      "Processing event 147/1495...\n",
      "Processing event 148/1495...\n",
      "Processing event 149/1495...\n",
      "Processing event 150/1495...\n",
      "Processing event 151/1495...\n",
      "Processing event 152/1495...\n",
      "Processing event 153/1495...\n",
      "Processing event 154/1495...\n",
      "Processing event 155/1495...\n",
      "Processing event 156/1495...\n",
      "Processing event 157/1495...\n",
      "Processing event 158/1495...\n",
      "Processing event 159/1495...\n",
      "Processing event 160/1495...\n",
      "Processing event 161/1495...\n",
      "Processing event 162/1495...\n",
      "Processing event 163/1495...\n",
      "Processing event 164/1495...\n",
      "Processing event 165/1495...\n",
      "Processing event 166/1495...\n",
      "Processing event 167/1495...\n",
      "Processing event 168/1495...\n",
      "Processing event 169/1495...\n",
      "Processing event 170/1495...\n",
      "Processing event 171/1495...\n",
      "Processing event 172/1495...\n",
      "Processing event 173/1495...\n",
      "Processing event 174/1495...\n",
      "Processing event 175/1495...\n",
      "Processing event 176/1495...\n",
      "Processing event 177/1495...\n",
      "Processing event 178/1495...\n",
      "Processing event 179/1495...\n",
      "Processing event 180/1495...\n",
      "Processing event 181/1495...\n",
      "Processing event 182/1495...\n",
      "Processing event 183/1495...\n",
      "Processing event 184/1495...\n",
      "Processing event 185/1495...\n",
      "Processing event 186/1495...\n",
      "Processing event 187/1495...\n",
      "Processing event 188/1495...\n",
      "Processing event 189/1495...\n",
      "Processing event 190/1495...\n",
      "Processing event 191/1495...\n",
      "Processing event 192/1495...\n",
      "Processing event 193/1495...\n",
      "Processing event 194/1495...\n",
      "Processing event 195/1495...\n",
      "Processing event 196/1495...\n",
      "Processing event 197/1495...\n",
      "Processing event 198/1495...\n",
      "Processing event 199/1495...\n",
      "Processing event 200/1495...\n",
      "Processing event 201/1495...\n",
      "Processing event 202/1495...\n",
      "Processing event 203/1495...\n",
      "Processing event 204/1495...\n",
      "Processing event 205/1495...\n",
      "Processing event 206/1495...\n",
      "Processing event 207/1495...\n",
      "Processing event 208/1495...\n",
      "Processing event 209/1495...\n",
      "Processing event 210/1495...\n",
      "Processing event 211/1495...\n",
      "Processing event 212/1495...\n",
      "Processing event 213/1495...\n",
      "Processing event 214/1495...\n",
      "Processing event 215/1495...\n",
      "Processing event 216/1495...\n",
      "Processing event 217/1495...\n",
      "Processing event 218/1495...\n",
      "Processing event 219/1495...\n",
      "Processing event 220/1495...\n",
      "Processing event 221/1495...\n",
      "Processing event 222/1495...\n",
      "Processing event 223/1495...\n",
      "Processing event 224/1495...\n",
      "Processing event 225/1495...\n",
      "Processing event 226/1495...\n",
      "Processing event 227/1495...\n",
      "Processing event 228/1495...\n",
      "Processing event 229/1495...\n",
      "Processing event 230/1495...\n",
      "Processing event 231/1495...\n",
      "Processing event 232/1495...\n",
      "Processing event 233/1495...\n",
      "Processing event 234/1495...\n",
      "Processing event 235/1495...\n",
      "Processing event 236/1495...\n",
      "Processing event 237/1495...\n",
      "Processing event 238/1495...\n",
      "Processing event 239/1495...\n",
      "Processing event 240/1495...\n",
      "Processing event 241/1495...\n",
      "Processing event 242/1495...\n",
      "Processing event 243/1495...\n",
      "Processing event 244/1495...\n",
      "Processing event 245/1495...\n",
      "Processing event 246/1495...\n",
      "Processing event 247/1495...\n",
      "Processing event 248/1495...\n",
      "Processing event 249/1495...\n",
      "Processing event 250/1495...\n",
      "Processing event 251/1495...\n",
      "Processing event 252/1495...\n",
      "Processing event 253/1495...\n",
      "Processing event 254/1495...\n",
      "Processing event 255/1495...\n",
      "Processing event 256/1495...\n",
      "Processing event 257/1495...\n",
      "Processing event 258/1495...\n",
      "Processing event 259/1495...\n",
      "Processing event 260/1495...\n",
      "Processing event 261/1495...\n",
      "Processing event 262/1495...\n",
      "Processing event 263/1495...\n",
      "Processing event 264/1495...\n",
      "Processing event 265/1495...\n",
      "Processing event 266/1495...\n",
      "Processing event 267/1495...\n",
      "Processing event 268/1495...\n",
      "Processing event 269/1495...\n",
      "Processing event 270/1495...\n",
      "Processing event 271/1495...\n",
      "Processing event 272/1495...\n",
      "Processing event 273/1495...\n",
      "Processing event 274/1495...\n",
      "Processing event 275/1495...\n",
      "Processing event 276/1495...\n",
      "Processing event 277/1495...\n",
      "Processing event 278/1495...\n",
      "Processing event 279/1495...\n",
      "Processing event 280/1495...\n",
      "Processing event 281/1495...\n",
      "Processing event 282/1495...\n",
      "Processing event 283/1495...\n",
      "Processing event 284/1495...\n",
      "Processing event 285/1495...\n",
      "Processing event 286/1495...\n",
      "Processing event 287/1495...\n",
      "Processing event 288/1495...\n",
      "Processing event 289/1495...\n",
      "Processing event 290/1495...\n",
      "Processing event 291/1495...\n",
      "Processing event 292/1495...\n",
      "Processing event 293/1495...\n",
      "Processing event 294/1495...\n",
      "Processing event 295/1495...\n",
      "Processing event 296/1495...\n",
      "Processing event 297/1495...\n",
      "Processing event 298/1495...\n",
      "Processing event 299/1495...\n",
      "Processing event 300/1495...\n",
      "Processing event 301/1495...\n",
      "Processing event 302/1495...\n",
      "Processing event 303/1495...\n",
      "Processing event 304/1495...\n",
      "Processing event 305/1495...\n",
      "Processing event 306/1495...\n",
      "Processing event 307/1495...\n",
      "Processing event 308/1495...\n",
      "Processing event 309/1495...\n",
      "Processing event 310/1495...\n",
      "Processing event 311/1495...\n",
      "Processing event 312/1495...\n",
      "Processing event 313/1495...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 314/1495...\n",
      "Processing event 315/1495...\n",
      "Processing event 316/1495...\n",
      "Processing event 317/1495...\n",
      "Processing event 318/1495...\n",
      "Processing event 319/1495...\n",
      "Processing event 320/1495...\n",
      "Processing event 321/1495...\n",
      "Processing event 322/1495...\n",
      "Processing event 323/1495...\n",
      "Processing event 324/1495...\n",
      "Processing event 325/1495...\n",
      "Processing event 326/1495...\n",
      "Processing event 327/1495...\n",
      "Processing event 328/1495...\n",
      "Processing event 329/1495...\n",
      "Processing event 330/1495...\n",
      "Processing event 331/1495...\n",
      "Processing event 332/1495...\n",
      "Processing event 333/1495...\n",
      "Processing event 334/1495...\n",
      "Processing event 335/1495...\n",
      "Processing event 336/1495...\n",
      "Processing event 337/1495...\n",
      "Processing event 338/1495...\n",
      "Processing event 339/1495...\n",
      "Processing event 340/1495...\n",
      "Processing event 341/1495...\n",
      "Processing event 342/1495...\n",
      "Processing event 343/1495...\n",
      "Processing event 344/1495...\n",
      "Processing event 345/1495...\n",
      "Processing event 346/1495...\n",
      "Processing event 347/1495...\n",
      "Processing event 348/1495...\n",
      "Processing event 349/1495...\n",
      "Processing event 350/1495...\n",
      "Processing event 351/1495...\n",
      "Processing event 352/1495...\n",
      "Processing event 353/1495...\n",
      "Processing event 354/1495...\n",
      "Processing event 355/1495...\n",
      "Processing event 356/1495...\n",
      "Processing event 357/1495...\n",
      "Processing event 358/1495...\n",
      "Processing event 359/1495...\n",
      "Processing event 360/1495...\n",
      "Processing event 361/1495...\n",
      "Processing event 362/1495...\n",
      "Processing event 363/1495...\n",
      "Processing event 364/1495...\n",
      "Processing event 365/1495...\n",
      "Processing event 366/1495...\n",
      "Processing event 367/1495...\n",
      "Processing event 368/1495...\n",
      "Processing event 369/1495...\n",
      "Processing event 370/1495...\n",
      "Processing event 371/1495...\n",
      "Processing event 372/1495...\n",
      "Processing event 373/1495...\n",
      "Processing event 374/1495...\n",
      "Processing event 375/1495...\n",
      "Processing event 376/1495...\n",
      "Processing event 377/1495...\n",
      "Processing event 378/1495...\n",
      "Processing event 379/1495...\n",
      "Processing event 380/1495...\n",
      "Processing event 381/1495...\n",
      "Processing event 382/1495...\n",
      "Processing event 383/1495...\n",
      "Processing event 384/1495...\n",
      "Processing event 385/1495...\n",
      "Processing event 386/1495...\n",
      "Processing event 387/1495...\n",
      "Processing event 388/1495...\n",
      "Processing event 389/1495...\n",
      "Processing event 390/1495...\n",
      "Processing event 391/1495...\n",
      "Processing event 392/1495...\n",
      "Processing event 393/1495...\n",
      "Processing event 394/1495...\n",
      "Processing event 395/1495...\n",
      "Processing event 396/1495...\n",
      "Processing event 397/1495...\n",
      "Processing event 398/1495...\n",
      "Processing event 399/1495...\n",
      "Processing event 400/1495...\n",
      "Processing event 401/1495...\n",
      "Processing event 402/1495...\n",
      "Processing event 403/1495...\n",
      "Processing event 404/1495...\n",
      "Processing event 405/1495...\n",
      "Processing event 406/1495...\n",
      "Processing event 407/1495...\n",
      "Processing event 408/1495...\n",
      "Processing event 409/1495...\n",
      "Processing event 410/1495...\n",
      "Processing event 411/1495...\n",
      "Processing event 412/1495...\n",
      "Processing event 413/1495...\n",
      "Processing event 414/1495...\n",
      "Processing event 415/1495...\n",
      "Processing event 416/1495...\n",
      "Processing event 417/1495...\n",
      "Processing event 418/1495...\n",
      "Processing event 419/1495...\n",
      "Processing event 420/1495...\n",
      "Processing event 421/1495...\n",
      "Processing event 422/1495...\n",
      "Processing event 423/1495...\n",
      "Processing event 424/1495...\n",
      "Processing event 425/1495...\n",
      "Processing event 426/1495...\n",
      "Processing event 427/1495...\n",
      "Processing event 428/1495...\n",
      "Processing event 429/1495...\n",
      "Processing event 430/1495...\n",
      "Processing event 431/1495...\n",
      "Processing event 432/1495...\n",
      "Processing event 433/1495...\n",
      "Processing event 434/1495...\n",
      "Processing event 435/1495...\n",
      "Processing event 436/1495...\n",
      "Processing event 437/1495...\n",
      "Processing event 438/1495...\n",
      "Processing event 439/1495...\n",
      "Processing event 440/1495...\n",
      "Processing event 441/1495...\n",
      "Processing event 442/1495...\n",
      "Processing event 443/1495...\n",
      "Processing event 444/1495...\n",
      "Processing event 445/1495...\n",
      "Processing event 446/1495...\n",
      "Processing event 447/1495...\n",
      "Processing event 448/1495...\n",
      "Processing event 449/1495...\n",
      "Processing event 450/1495...\n",
      "Processing event 451/1495...\n",
      "Processing event 452/1495...\n",
      "Processing event 453/1495...\n",
      "Processing event 454/1495...\n",
      "Processing event 455/1495...\n",
      "Processing event 456/1495...\n",
      "Processing event 457/1495...\n",
      "Processing event 458/1495...\n",
      "Processing event 459/1495...\n",
      "Processing event 460/1495...\n",
      "Processing event 461/1495...\n",
      "Processing event 462/1495...\n",
      "Processing event 463/1495...\n",
      "Processing event 464/1495...\n",
      "Processing event 465/1495...\n",
      "Processing event 466/1495...\n",
      "Processing event 467/1495...\n",
      "Processing event 468/1495...\n",
      "Processing event 469/1495...\n",
      "Processing event 470/1495...\n",
      "Processing event 471/1495...\n",
      "Processing event 472/1495...\n",
      "Processing event 473/1495...\n",
      "Processing event 474/1495...\n",
      "Processing event 475/1495...\n",
      "Processing event 476/1495...\n",
      "Processing event 477/1495...\n",
      "Processing event 478/1495...\n",
      "Processing event 479/1495...\n",
      "Processing event 480/1495...\n",
      "Processing event 481/1495...\n",
      "Processing event 482/1495...\n",
      "Processing event 483/1495...\n",
      "Processing event 484/1495...\n",
      "Processing event 485/1495...\n",
      "Processing event 486/1495...\n",
      "Processing event 487/1495...\n",
      "Processing event 488/1495...\n",
      "Processing event 489/1495...\n",
      "Processing event 490/1495...\n",
      "Processing event 491/1495...\n",
      "Processing event 492/1495...\n",
      "Processing event 493/1495...\n",
      "Processing event 494/1495...\n",
      "Processing event 495/1495...\n",
      "Processing event 496/1495...\n",
      "Processing event 497/1495...\n",
      "Processing event 498/1495...\n",
      "Processing event 499/1495...\n",
      "Processing event 500/1495...\n",
      "Processing event 501/1495...\n",
      "Processing event 502/1495...\n",
      "Processing event 503/1495...\n",
      "Processing event 504/1495...\n",
      "Processing event 505/1495...\n",
      "Processing event 506/1495...\n",
      "Processing event 507/1495...\n",
      "Processing event 508/1495...\n",
      "Processing event 509/1495...\n",
      "Processing event 510/1495...\n",
      "Processing event 511/1495...\n",
      "Processing event 512/1495...\n",
      "Processing event 513/1495...\n",
      "Processing event 514/1495...\n",
      "Processing event 515/1495...\n",
      "Processing event 516/1495...\n",
      "Processing event 517/1495...\n",
      "Processing event 518/1495...\n",
      "Processing event 519/1495...\n",
      "Processing event 520/1495...\n",
      "Processing event 521/1495...\n",
      "Processing event 522/1495...\n",
      "Processing event 523/1495...\n",
      "Processing event 524/1495...\n",
      "Processing event 525/1495...\n",
      "Processing event 526/1495...\n",
      "Processing event 527/1495...\n",
      "Processing event 528/1495...\n",
      "Processing event 529/1495...\n",
      "Processing event 530/1495...\n",
      "Processing event 531/1495...\n",
      "Processing event 532/1495...\n",
      "Processing event 533/1495...\n",
      "Processing event 534/1495...\n",
      "Processing event 535/1495...\n",
      "Processing event 536/1495...\n",
      "Processing event 537/1495...\n",
      "Processing event 538/1495...\n",
      "Processing event 539/1495...\n",
      "Processing event 540/1495...\n",
      "Processing event 541/1495...\n",
      "Processing event 542/1495...\n",
      "Processing event 543/1495...\n",
      "Processing event 544/1495...\n",
      "Processing event 545/1495...\n",
      "Processing event 546/1495...\n",
      "Processing event 547/1495...\n",
      "Processing event 548/1495...\n",
      "Processing event 549/1495...\n",
      "Processing event 550/1495...\n",
      "Processing event 551/1495...\n",
      "Processing event 552/1495...\n",
      "Processing event 553/1495...\n",
      "Processing event 554/1495...\n",
      "Processing event 555/1495...\n",
      "Processing event 556/1495...\n",
      "Processing event 557/1495...\n",
      "Processing event 558/1495...\n",
      "Processing event 559/1495...\n",
      "Processing event 560/1495...\n",
      "Processing event 561/1495...\n",
      "Processing event 562/1495...\n",
      "Processing event 563/1495...\n",
      "Processing event 564/1495...\n",
      "Processing event 565/1495...\n",
      "Processing event 566/1495...\n",
      "Processing event 567/1495...\n",
      "Processing event 568/1495...\n",
      "Processing event 569/1495...\n",
      "Processing event 570/1495...\n",
      "Processing event 571/1495...\n",
      "Processing event 572/1495...\n",
      "Processing event 573/1495...\n",
      "Processing event 574/1495...\n",
      "Processing event 575/1495...\n",
      "Processing event 576/1495...\n",
      "Processing event 577/1495...\n",
      "Processing event 578/1495...\n",
      "Processing event 579/1495...\n",
      "Processing event 580/1495...\n",
      "Processing event 581/1495...\n",
      "Processing event 582/1495...\n",
      "Processing event 583/1495...\n",
      "Processing event 584/1495...\n",
      "Processing event 585/1495...\n",
      "Processing event 586/1495...\n",
      "Processing event 587/1495...\n",
      "Processing event 588/1495...\n",
      "Processing event 589/1495...\n",
      "Processing event 590/1495...\n",
      "Processing event 591/1495...\n",
      "Processing event 592/1495...\n",
      "Processing event 593/1495...\n",
      "Processing event 594/1495...\n",
      "Processing event 595/1495...\n",
      "Processing event 596/1495...\n",
      "Processing event 597/1495...\n",
      "Processing event 598/1495...\n",
      "Processing event 599/1495...\n",
      "Processing event 600/1495...\n",
      "Processing event 601/1495...\n",
      "Processing event 602/1495...\n",
      "Processing event 603/1495...\n",
      "Processing event 604/1495...\n",
      "Processing event 605/1495...\n",
      "Processing event 606/1495...\n",
      "Processing event 607/1495...\n",
      "Processing event 608/1495...\n",
      "Processing event 609/1495...\n",
      "Processing event 610/1495...\n",
      "Processing event 611/1495...\n",
      "Processing event 612/1495...\n",
      "Processing event 613/1495...\n",
      "Processing event 614/1495...\n",
      "Processing event 615/1495...\n",
      "Processing event 616/1495...\n",
      "Processing event 617/1495...\n",
      "Processing event 618/1495...\n",
      "Processing event 619/1495...\n",
      "Processing event 620/1495...\n",
      "Processing event 621/1495...\n",
      "Processing event 622/1495...\n",
      "Processing event 623/1495...\n",
      "Processing event 624/1495...\n",
      "Processing event 625/1495...\n",
      "Processing event 626/1495...\n",
      "Processing event 627/1495...\n",
      "Processing event 628/1495...\n",
      "Processing event 629/1495...\n",
      "Processing event 630/1495...\n",
      "Processing event 631/1495...\n",
      "Processing event 632/1495...\n",
      "Processing event 633/1495...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 634/1495...\n",
      "Processing event 635/1495...\n",
      "Processing event 636/1495...\n",
      "Processing event 637/1495...\n",
      "Processing event 638/1495...\n",
      "Processing event 639/1495...\n",
      "Processing event 640/1495...\n",
      "Processing event 641/1495...\n",
      "Processing event 642/1495...\n",
      "Processing event 643/1495...\n",
      "Processing event 644/1495...\n",
      "Processing event 645/1495...\n",
      "Processing event 646/1495...\n",
      "Processing event 647/1495...\n",
      "Processing event 648/1495...\n",
      "Processing event 649/1495...\n",
      "Processing event 650/1495...\n",
      "Processing event 651/1495...\n",
      "Processing event 652/1495...\n",
      "Processing event 653/1495...\n",
      "Processing event 654/1495...\n",
      "Processing event 655/1495...\n",
      "Processing event 656/1495...\n",
      "Processing event 657/1495...\n",
      "Processing event 658/1495...\n",
      "Processing event 659/1495...\n",
      "Processing event 660/1495...\n",
      "Processing event 661/1495...\n",
      "Processing event 662/1495...\n",
      "Processing event 663/1495...\n",
      "Processing event 664/1495...\n",
      "Processing event 665/1495...\n",
      "Processing event 666/1495...\n",
      "Processing event 667/1495...\n",
      "Processing event 668/1495...\n",
      "Processing event 669/1495...\n",
      "Processing event 670/1495...\n",
      "Processing event 671/1495...\n",
      "Processing event 672/1495...\n",
      "Processing event 673/1495...\n",
      "Processing event 674/1495...\n",
      "Processing event 675/1495...\n",
      "Processing event 676/1495...\n",
      "Processing event 677/1495...\n",
      "Processing event 678/1495...\n",
      "Processing event 679/1495...\n",
      "Processing event 680/1495...\n",
      "Processing event 681/1495...\n",
      "Processing event 682/1495...\n",
      "Processing event 683/1495...\n",
      "Processing event 684/1495...\n",
      "Processing event 685/1495...\n",
      "Processing event 686/1495...\n",
      "Processing event 687/1495...\n",
      "Processing event 688/1495...\n",
      "Processing event 689/1495...\n",
      "Processing event 690/1495...\n",
      "Processing event 691/1495...\n",
      "Processing event 692/1495...\n",
      "Processing event 693/1495...\n",
      "Processing event 694/1495...\n",
      "Processing event 695/1495...\n",
      "Processing event 696/1495...\n",
      "Processing event 697/1495...\n",
      "Processing event 698/1495...\n",
      "Processing event 699/1495...\n",
      "Processing event 700/1495...\n",
      "Processing event 701/1495...\n",
      "Processing event 702/1495...\n",
      "Processing event 703/1495...\n",
      "Processing event 704/1495...\n",
      "Processing event 705/1495...\n",
      "Processing event 706/1495...\n",
      "Processing event 707/1495...\n",
      "Processing event 708/1495...\n",
      "Processing event 709/1495...\n",
      "Processing event 710/1495...\n",
      "Processing event 711/1495...\n",
      "Processing event 712/1495...\n",
      "Processing event 713/1495...\n",
      "Processing event 714/1495...\n",
      "Processing event 715/1495...\n",
      "Processing event 716/1495...\n",
      "Processing event 717/1495...\n",
      "Processing event 718/1495...\n",
      "Processing event 719/1495...\n",
      "Processing event 720/1495...\n",
      "Processing event 721/1495...\n",
      "Processing event 722/1495...\n",
      "Processing event 723/1495...\n",
      "Processing event 724/1495...\n",
      "Processing event 725/1495...\n",
      "Processing event 726/1495...\n",
      "Processing event 727/1495...\n",
      "Processing event 728/1495...\n",
      "Processing event 729/1495...\n",
      "Processing event 730/1495...\n",
      "Processing event 731/1495...\n",
      "Processing event 732/1495...\n",
      "Processing event 733/1495...\n",
      "Processing event 734/1495...\n",
      "Processing event 735/1495...\n",
      "Processing event 736/1495...\n",
      "Processing event 737/1495...\n",
      "Processing event 738/1495...\n",
      "Processing event 739/1495...\n",
      "Processing event 740/1495...\n",
      "Processing event 741/1495...\n",
      "Processing event 742/1495...\n",
      "Processing event 743/1495...\n",
      "Processing event 744/1495...\n",
      "Processing event 745/1495...\n",
      "Processing event 746/1495...\n",
      "Processing event 747/1495...\n",
      "Processing event 748/1495...\n",
      "Processing event 749/1495...\n",
      "Processing event 750/1495...\n",
      "Processing event 751/1495...\n",
      "Processing event 752/1495...\n",
      "Processing event 753/1495...\n",
      "Processing event 754/1495...\n",
      "Processing event 755/1495...\n",
      "Processing event 756/1495...\n",
      "Processing event 757/1495...\n",
      "Processing event 758/1495...\n",
      "Processing event 759/1495...\n",
      "Processing event 760/1495...\n",
      "Processing event 761/1495...\n",
      "Processing event 762/1495...\n",
      "Processing event 763/1495...\n",
      "Processing event 764/1495...\n",
      "Processing event 765/1495...\n",
      "Processing event 766/1495...\n",
      "Processing event 767/1495...\n",
      "Processing event 768/1495...\n",
      "Processing event 769/1495...\n",
      "Processing event 770/1495...\n",
      "Processing event 771/1495...\n",
      "Processing event 772/1495...\n",
      "Processing event 773/1495...\n",
      "Processing event 774/1495...\n",
      "Processing event 775/1495...\n",
      "Processing event 776/1495...\n",
      "Processing event 777/1495...\n",
      "Processing event 778/1495...\n",
      "Processing event 779/1495...\n",
      "Processing event 780/1495...\n",
      "Processing event 781/1495...\n",
      "Processing event 782/1495...\n",
      "Processing event 783/1495...\n",
      "Processing event 784/1495...\n",
      "Processing event 785/1495...\n",
      "Processing event 786/1495...\n",
      "Processing event 787/1495...\n",
      "Processing event 788/1495...\n",
      "Processing event 789/1495...\n",
      "Processing event 790/1495...\n",
      "Processing event 791/1495...\n",
      "Processing event 792/1495...\n",
      "Processing event 793/1495...\n",
      "Processing event 794/1495...\n",
      "Processing event 795/1495...\n",
      "Processing event 796/1495...\n",
      "Processing event 797/1495...\n",
      "Processing event 798/1495...\n",
      "Processing event 799/1495...\n",
      "Processing event 800/1495...\n",
      "Processing event 801/1495...\n",
      "Processing event 802/1495...\n",
      "Processing event 803/1495...\n",
      "Processing event 804/1495...\n",
      "Processing event 805/1495...\n",
      "Processing event 806/1495...\n",
      "Processing event 807/1495...\n",
      "Processing event 808/1495...\n",
      "Processing event 809/1495...\n",
      "Processing event 810/1495...\n",
      "Processing event 811/1495...\n",
      "Processing event 812/1495...\n",
      "Processing event 813/1495...\n",
      "Processing event 814/1495...\n",
      "Processing event 815/1495...\n",
      "Processing event 816/1495...\n",
      "Processing event 817/1495...\n",
      "Processing event 818/1495...\n",
      "Processing event 819/1495...\n",
      "Processing event 820/1495...\n",
      "Processing event 821/1495...\n",
      "Processing event 822/1495...\n",
      "Processing event 823/1495...\n",
      "Processing event 824/1495...\n",
      "Processing event 825/1495...\n",
      "Processing event 826/1495...\n",
      "Processing event 827/1495...\n",
      "Processing event 828/1495...\n",
      "Processing event 829/1495...\n",
      "Processing event 830/1495...\n",
      "Processing event 831/1495...\n",
      "Processing event 832/1495...\n",
      "Processing event 833/1495...\n",
      "Processing event 834/1495...\n",
      "Processing event 835/1495...\n",
      "Processing event 836/1495...\n",
      "Processing event 837/1495...\n",
      "Processing event 838/1495...\n",
      "Processing event 839/1495...\n",
      "Processing event 840/1495...\n",
      "Processing event 841/1495...\n",
      "Processing event 842/1495...\n",
      "Processing event 843/1495...\n",
      "Processing event 844/1495...\n",
      "Processing event 845/1495...\n",
      "Processing event 846/1495...\n",
      "Processing event 847/1495...\n",
      "Processing event 848/1495...\n",
      "Processing event 849/1495...\n",
      "Processing event 850/1495...\n",
      "Processing event 851/1495...\n",
      "Processing event 852/1495...\n",
      "Processing event 853/1495...\n",
      "Processing event 854/1495...\n",
      "Processing event 855/1495...\n",
      "Processing event 856/1495...\n",
      "Processing event 857/1495...\n",
      "Processing event 858/1495...\n",
      "Processing event 859/1495...\n",
      "Processing event 860/1495...\n",
      "Processing event 861/1495...\n",
      "Processing event 862/1495...\n",
      "Processing event 863/1495...\n",
      "Processing event 864/1495...\n",
      "Processing event 865/1495...\n",
      "Processing event 866/1495...\n",
      "Processing event 867/1495...\n",
      "Processing event 868/1495...\n",
      "Processing event 869/1495...\n",
      "Processing event 870/1495...\n",
      "Processing event 871/1495...\n",
      "Processing event 872/1495...\n",
      "Processing event 873/1495...\n",
      "Processing event 874/1495...\n",
      "Processing event 875/1495...\n",
      "Processing event 876/1495...\n",
      "Processing event 877/1495...\n",
      "Processing event 878/1495...\n",
      "Processing event 879/1495...\n",
      "Processing event 880/1495...\n",
      "Processing event 881/1495...\n",
      "Processing event 882/1495...\n",
      "Processing event 883/1495...\n",
      "Processing event 884/1495...\n",
      "Processing event 885/1495...\n",
      "Processing event 886/1495...\n",
      "Processing event 887/1495...\n",
      "Processing event 888/1495...\n",
      "Processing event 889/1495...\n",
      "Processing event 890/1495...\n",
      "Processing event 891/1495...\n",
      "Processing event 892/1495...\n",
      "Processing event 893/1495...\n",
      "Processing event 894/1495...\n",
      "Processing event 895/1495...\n",
      "Processing event 896/1495...\n",
      "Processing event 897/1495...\n",
      "Processing event 898/1495...\n",
      "Processing event 899/1495...\n",
      "Processing event 900/1495...\n",
      "Processing event 901/1495...\n",
      "Processing event 902/1495...\n",
      "Processing event 903/1495...\n",
      "Processing event 904/1495...\n",
      "Processing event 905/1495...\n",
      "Processing event 906/1495...\n",
      "Processing event 907/1495...\n",
      "Processing event 908/1495...\n",
      "Processing event 909/1495...\n",
      "Processing event 910/1495...\n",
      "Processing event 911/1495...\n",
      "Processing event 912/1495...\n",
      "Processing event 913/1495...\n",
      "Processing event 914/1495...\n",
      "Processing event 915/1495...\n",
      "Processing event 916/1495...\n",
      "Processing event 917/1495...\n",
      "Processing event 918/1495...\n",
      "Processing event 919/1495...\n",
      "Processing event 920/1495...\n",
      "Processing event 921/1495...\n",
      "Processing event 922/1495...\n",
      "Processing event 923/1495...\n",
      "Processing event 924/1495...\n",
      "Processing event 925/1495...\n",
      "Processing event 926/1495...\n",
      "Processing event 927/1495...\n",
      "Processing event 928/1495...\n",
      "Processing event 929/1495...\n",
      "Processing event 930/1495...\n",
      "Processing event 931/1495...\n",
      "Processing event 932/1495...\n",
      "Processing event 933/1495...\n",
      "Processing event 934/1495...\n",
      "Processing event 935/1495...\n",
      "Processing event 936/1495...\n",
      "Processing event 937/1495...\n",
      "Processing event 938/1495...\n",
      "Processing event 939/1495...\n",
      "Processing event 940/1495...\n",
      "Processing event 941/1495...\n",
      "Processing event 942/1495...\n",
      "Processing event 943/1495...\n",
      "Processing event 944/1495...\n",
      "Processing event 945/1495...\n",
      "Processing event 946/1495...\n",
      "Processing event 947/1495...\n",
      "Processing event 948/1495...\n",
      "Processing event 949/1495...\n",
      "Processing event 950/1495...\n",
      "Processing event 951/1495...\n",
      "Processing event 952/1495...\n",
      "Processing event 953/1495...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 954/1495...\n",
      "Processing event 955/1495...\n",
      "Processing event 956/1495...\n",
      "Processing event 957/1495...\n",
      "Processing event 958/1495...\n",
      "Processing event 959/1495...\n",
      "Processing event 960/1495...\n",
      "Processing event 961/1495...\n",
      "Processing event 962/1495...\n",
      "Processing event 963/1495...\n",
      "Processing event 964/1495...\n",
      "Processing event 965/1495...\n",
      "Processing event 966/1495...\n",
      "Processing event 967/1495...\n",
      "Processing event 968/1495...\n",
      "Processing event 969/1495...\n",
      "Processing event 970/1495...\n",
      "Processing event 971/1495...\n",
      "Processing event 972/1495...\n",
      "Processing event 973/1495...\n",
      "Processing event 974/1495...\n",
      "Processing event 975/1495...\n",
      "Processing event 976/1495...\n",
      "Processing event 977/1495...\n",
      "Processing event 978/1495...\n",
      "Processing event 979/1495...\n",
      "Processing event 980/1495...\n",
      "Processing event 981/1495...\n",
      "Processing event 982/1495...\n",
      "Processing event 983/1495...\n",
      "Processing event 984/1495...\n",
      "Processing event 985/1495...\n",
      "Processing event 986/1495...\n",
      "Processing event 987/1495...\n",
      "Processing event 988/1495...\n",
      "Processing event 989/1495...\n",
      "Processing event 990/1495...\n",
      "Processing event 991/1495...\n",
      "Processing event 992/1495...\n",
      "Processing event 993/1495...\n",
      "Processing event 994/1495...\n",
      "Processing event 995/1495...\n",
      "Processing event 996/1495...\n",
      "Processing event 997/1495...\n",
      "Processing event 998/1495...\n",
      "Processing event 999/1495...\n",
      "Processing event 1000/1495...\n",
      "Processing event 1001/1495...\n",
      "Processing event 1002/1495...\n",
      "Processing event 1003/1495...\n",
      "Processing event 1004/1495...\n",
      "Processing event 1005/1495...\n",
      "Processing event 1006/1495...\n",
      "Processing event 1007/1495...\n",
      "Processing event 1008/1495...\n",
      "Processing event 1009/1495...\n",
      "Processing event 1010/1495...\n",
      "Processing event 1011/1495...\n",
      "Processing event 1012/1495...\n",
      "Processing event 1013/1495...\n",
      "Processing event 1014/1495...\n",
      "Processing event 1015/1495...\n",
      "Processing event 1016/1495...\n",
      "Processing event 1017/1495...\n",
      "Processing event 1018/1495...\n",
      "Processing event 1019/1495...\n",
      "Processing event 1020/1495...\n",
      "Processing event 1021/1495...\n",
      "Processing event 1022/1495...\n",
      "Processing event 1023/1495...\n",
      "Processing event 1024/1495...\n",
      "Processing event 1025/1495...\n",
      "Processing event 1026/1495...\n",
      "Processing event 1027/1495...\n",
      "Processing event 1028/1495...\n",
      "Processing event 1029/1495...\n",
      "Processing event 1030/1495...\n",
      "Processing event 1031/1495...\n",
      "Processing event 1032/1495...\n",
      "Processing event 1033/1495...\n",
      "Processing event 1034/1495...\n",
      "Processing event 1035/1495...\n",
      "Processing event 1036/1495...\n",
      "Processing event 1037/1495...\n",
      "Processing event 1038/1495...\n",
      "Processing event 1039/1495...\n",
      "Processing event 1040/1495...\n",
      "Processing event 1041/1495...\n",
      "Processing event 1042/1495...\n",
      "Processing event 1043/1495...\n",
      "Processing event 1044/1495...\n",
      "Processing event 1045/1495...\n",
      "Processing event 1046/1495...\n",
      "Processing event 1047/1495...\n",
      "Processing event 1048/1495...\n",
      "Processing event 1049/1495...\n",
      "Processing event 1050/1495...\n",
      "Processing event 1051/1495...\n",
      "Processing event 1052/1495...\n",
      "Processing event 1053/1495...\n",
      "Processing event 1054/1495...\n",
      "Processing event 1055/1495...\n",
      "Processing event 1056/1495...\n",
      "Processing event 1057/1495...\n",
      "Processing event 1058/1495...\n",
      "Processing event 1059/1495...\n",
      "Processing event 1060/1495...\n",
      "Processing event 1061/1495...\n",
      "Processing event 1062/1495...\n",
      "Processing event 1063/1495...\n",
      "Processing event 1064/1495...\n",
      "Processing event 1065/1495...\n",
      "Processing event 1066/1495...\n",
      "Processing event 1067/1495...\n",
      "Processing event 1068/1495...\n",
      "Processing event 1069/1495...\n",
      "Processing event 1070/1495...\n",
      "Processing event 1071/1495...\n",
      "Processing event 1072/1495...\n",
      "Processing event 1073/1495...\n",
      "Processing event 1074/1495...\n",
      "Processing event 1075/1495...\n",
      "Processing event 1076/1495...\n",
      "Processing event 1077/1495...\n",
      "Processing event 1078/1495...\n",
      "Processing event 1079/1495...\n",
      "Processing event 1080/1495...\n",
      "Processing event 1081/1495...\n",
      "Processing event 1082/1495...\n",
      "Processing event 1083/1495...\n",
      "Processing event 1084/1495...\n",
      "Processing event 1085/1495...\n",
      "Processing event 1086/1495...\n",
      "Processing event 1087/1495...\n",
      "Processing event 1088/1495...\n",
      "Processing event 1089/1495...\n",
      "Processing event 1090/1495...\n",
      "Processing event 1091/1495...\n",
      "Processing event 1092/1495...\n",
      "Processing event 1093/1495...\n",
      "Processing event 1094/1495...\n",
      "Processing event 1095/1495...\n",
      "Processing event 1096/1495...\n",
      "Processing event 1097/1495...\n",
      "Processing event 1098/1495...\n",
      "Processing event 1099/1495...\n",
      "Processing event 1100/1495...\n",
      "Processing event 1101/1495...\n",
      "Processing event 1102/1495...\n",
      "Processing event 1103/1495...\n",
      "Processing event 1104/1495...\n",
      "Processing event 1105/1495...\n",
      "Processing event 1106/1495...\n",
      "Processing event 1107/1495...\n",
      "Processing event 1108/1495...\n",
      "Processing event 1109/1495...\n",
      "Processing event 1110/1495...\n",
      "Processing event 1111/1495...\n",
      "Processing event 1112/1495...\n",
      "Processing event 1113/1495...\n",
      "Processing event 1114/1495...\n",
      "Processing event 1115/1495...\n",
      "Processing event 1116/1495...\n",
      "Processing event 1117/1495...\n",
      "Processing event 1118/1495...\n",
      "Processing event 1119/1495...\n",
      "Processing event 1120/1495...\n",
      "Processing event 1121/1495...\n",
      "Processing event 1122/1495...\n",
      "Processing event 1123/1495...\n",
      "Processing event 1124/1495...\n",
      "Processing event 1125/1495...\n",
      "Processing event 1126/1495...\n",
      "Processing event 1127/1495...\n",
      "Processing event 1128/1495...\n",
      "Processing event 1129/1495...\n",
      "Processing event 1130/1495...\n",
      "Processing event 1131/1495...\n",
      "Processing event 1132/1495...\n",
      "Processing event 1133/1495...\n",
      "Processing event 1134/1495...\n",
      "Processing event 1135/1495...\n",
      "Processing event 1136/1495...\n",
      "Processing event 1137/1495...\n",
      "Processing event 1138/1495...\n",
      "Processing event 1139/1495...\n",
      "Processing event 1140/1495...\n",
      "Processing event 1141/1495...\n",
      "Processing event 1142/1495...\n",
      "Processing event 1143/1495...\n",
      "Processing event 1144/1495...\n",
      "Processing event 1145/1495...\n",
      "Processing event 1146/1495...\n",
      "Processing event 1147/1495...\n",
      "Processing event 1148/1495...\n",
      "Processing event 1149/1495...\n",
      "Processing event 1150/1495...\n",
      "Processing event 1151/1495...\n",
      "Processing event 1152/1495...\n",
      "Processing event 1153/1495...\n",
      "Processing event 1154/1495...\n",
      "Processing event 1155/1495...\n",
      "Processing event 1156/1495...\n",
      "Processing event 1157/1495...\n",
      "Processing event 1158/1495...\n",
      "Processing event 1159/1495...\n",
      "Processing event 1160/1495...\n",
      "Processing event 1161/1495...\n",
      "Processing event 1162/1495...\n",
      "Processing event 1163/1495...\n",
      "Processing event 1164/1495...\n",
      "Processing event 1165/1495...\n",
      "Processing event 1166/1495...\n",
      "Processing event 1167/1495...\n",
      "Processing event 1168/1495...\n",
      "Processing event 1169/1495...\n",
      "Processing event 1170/1495...\n",
      "Processing event 1171/1495...\n",
      "Processing event 1172/1495...\n",
      "Processing event 1173/1495...\n",
      "Processing event 1174/1495...\n",
      "Processing event 1175/1495...\n",
      "Processing event 1176/1495...\n",
      "Processing event 1177/1495...\n",
      "Processing event 1178/1495...\n",
      "Processing event 1179/1495...\n",
      "Processing event 1180/1495...\n",
      "Processing event 1181/1495...\n",
      "Processing event 1182/1495...\n",
      "Processing event 1183/1495...\n",
      "Processing event 1184/1495...\n",
      "Processing event 1185/1495...\n",
      "Processing event 1186/1495...\n",
      "Processing event 1187/1495...\n",
      "Processing event 1188/1495...\n",
      "Processing event 1189/1495...\n",
      "Processing event 1190/1495...\n",
      "Processing event 1191/1495...\n",
      "Processing event 1192/1495...\n",
      "Processing event 1193/1495...\n",
      "Processing event 1194/1495...\n",
      "Processing event 1195/1495...\n",
      "Processing event 1196/1495...\n",
      "Processing event 1197/1495...\n",
      "Processing event 1198/1495...\n",
      "Processing event 1199/1495...\n",
      "Processing event 1200/1495...\n",
      "Processing event 1201/1495...\n",
      "Processing event 1202/1495...\n",
      "Processing event 1203/1495...\n",
      "Processing event 1204/1495...\n",
      "Processing event 1205/1495...\n",
      "Processing event 1206/1495...\n",
      "Processing event 1207/1495...\n",
      "Processing event 1208/1495...\n",
      "Processing event 1209/1495...\n",
      "Processing event 1210/1495...\n",
      "Processing event 1211/1495...\n",
      "Processing event 1212/1495...\n",
      "Processing event 1213/1495...\n",
      "Processing event 1214/1495...\n",
      "Processing event 1215/1495...\n",
      "Processing event 1216/1495...\n",
      "Processing event 1217/1495...\n",
      "Processing event 1218/1495...\n",
      "Processing event 1219/1495...\n",
      "Processing event 1220/1495...\n",
      "Processing event 1221/1495...\n",
      "Processing event 1222/1495...\n",
      "Processing event 1223/1495...\n",
      "Processing event 1224/1495...\n",
      "Processing event 1225/1495...\n",
      "Processing event 1226/1495...\n",
      "Processing event 1227/1495...\n",
      "Processing event 1228/1495...\n",
      "Processing event 1229/1495...\n",
      "Processing event 1230/1495...\n",
      "Processing event 1231/1495...\n",
      "Processing event 1232/1495...\n",
      "Processing event 1233/1495...\n",
      "Processing event 1234/1495...\n",
      "Processing event 1235/1495...\n",
      "Processing event 1236/1495...\n",
      "Processing event 1237/1495...\n",
      "Processing event 1238/1495...\n",
      "Processing event 1239/1495...\n",
      "Processing event 1240/1495...\n",
      "Processing event 1241/1495...\n",
      "Processing event 1242/1495...\n",
      "Processing event 1243/1495...\n",
      "Processing event 1244/1495...\n",
      "Processing event 1245/1495...\n",
      "Processing event 1246/1495...\n",
      "Processing event 1247/1495...\n",
      "Processing event 1248/1495...\n",
      "Processing event 1249/1495...\n",
      "Processing event 1250/1495...\n",
      "Processing event 1251/1495...\n",
      "Processing event 1252/1495...\n",
      "Processing event 1253/1495...\n",
      "Processing event 1254/1495...\n",
      "Processing event 1255/1495...\n",
      "Processing event 1256/1495...\n",
      "Processing event 1257/1495...\n",
      "Processing event 1258/1495...\n",
      "Processing event 1259/1495...\n",
      "Processing event 1260/1495...\n",
      "Processing event 1261/1495...\n",
      "Processing event 1262/1495...\n",
      "Processing event 1263/1495...\n",
      "Processing event 1264/1495...\n",
      "Processing event 1265/1495...\n",
      "Processing event 1266/1495...\n",
      "Processing event 1267/1495...\n",
      "Processing event 1268/1495...\n",
      "Processing event 1269/1495...\n",
      "Processing event 1270/1495...\n",
      "Processing event 1271/1495...\n",
      "Processing event 1272/1495...\n",
      "Processing event 1273/1495...\n",
      "Processing event 1274/1495...\n",
      "Processing event 1275/1495...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 1276/1495...\n",
      "Processing event 1277/1495...\n",
      "Processing event 1278/1495...\n",
      "Processing event 1279/1495...\n",
      "Processing event 1280/1495...\n",
      "Processing event 1281/1495...\n",
      "Processing event 1282/1495...\n",
      "Processing event 1283/1495...\n",
      "Processing event 1284/1495...\n",
      "Processing event 1285/1495...\n",
      "Processing event 1286/1495...\n",
      "Processing event 1287/1495...\n",
      "Processing event 1288/1495...\n",
      "Processing event 1289/1495...\n",
      "Processing event 1290/1495...\n",
      "Processing event 1291/1495...\n",
      "Processing event 1292/1495...\n",
      "Processing event 1293/1495...\n",
      "Processing event 1294/1495...\n",
      "Processing event 1295/1495...\n",
      "Processing event 1296/1495...\n",
      "Processing event 1297/1495...\n",
      "Processing event 1298/1495...\n",
      "Processing event 1299/1495...\n",
      "Processing event 1300/1495...\n",
      "Processing event 1301/1495...\n",
      "Processing event 1302/1495...\n",
      "Processing event 1303/1495...\n",
      "Processing event 1304/1495...\n",
      "Processing event 1305/1495...\n",
      "Processing event 1306/1495...\n",
      "Processing event 1307/1495...\n",
      "Processing event 1308/1495...\n",
      "Processing event 1309/1495...\n",
      "Processing event 1310/1495...\n",
      "Processing event 1311/1495...\n",
      "Processing event 1312/1495...\n",
      "Processing event 1313/1495...\n",
      "Processing event 1314/1495...\n",
      "Processing event 1315/1495...\n",
      "Processing event 1316/1495...\n",
      "Processing event 1317/1495...\n",
      "Processing event 1318/1495...\n",
      "Processing event 1319/1495...\n",
      "Processing event 1320/1495...\n",
      "Processing event 1321/1495...\n",
      "Processing event 1322/1495...\n",
      "Processing event 1323/1495...\n",
      "Processing event 1324/1495...\n",
      "Processing event 1325/1495...\n",
      "Processing event 1326/1495...\n",
      "Processing event 1327/1495...\n",
      "Processing event 1328/1495...\n",
      "Processing event 1329/1495...\n",
      "Processing event 1330/1495...\n",
      "Processing event 1331/1495...\n",
      "Processing event 1332/1495...\n",
      "Processing event 1333/1495...\n",
      "Processing event 1334/1495...\n",
      "Processing event 1335/1495...\n",
      "Processing event 1336/1495...\n",
      "Processing event 1337/1495...\n",
      "Processing event 1338/1495...\n",
      "Processing event 1339/1495...\n",
      "Processing event 1340/1495...\n",
      "Processing event 1341/1495...\n",
      "Processing event 1342/1495...\n",
      "Processing event 1343/1495...\n",
      "Processing event 1344/1495...\n",
      "Processing event 1345/1495...\n",
      "Processing event 1346/1495...\n",
      "Processing event 1347/1495...\n",
      "Processing event 1348/1495...\n",
      "Processing event 1349/1495...\n",
      "Processing event 1350/1495...\n",
      "Processing event 1351/1495...\n",
      "Processing event 1352/1495...\n",
      "Processing event 1353/1495...\n",
      "Processing event 1354/1495...\n",
      "Processing event 1355/1495...\n",
      "Processing event 1356/1495...\n",
      "Processing event 1357/1495...\n",
      "Processing event 1358/1495...\n",
      "Processing event 1359/1495...\n",
      "Processing event 1360/1495...\n",
      "Processing event 1361/1495...\n",
      "Processing event 1362/1495...\n",
      "Processing event 1363/1495...\n",
      "Processing event 1364/1495...\n",
      "Processing event 1365/1495...\n",
      "Processing event 1366/1495...\n",
      "Processing event 1367/1495...\n",
      "Processing event 1368/1495...\n",
      "Processing event 1369/1495...\n",
      "Processing event 1370/1495...\n",
      "Processing event 1371/1495...\n",
      "Processing event 1372/1495...\n",
      "Processing event 1373/1495...\n",
      "Processing event 1374/1495...\n",
      "Processing event 1375/1495...\n",
      "Processing event 1376/1495...\n",
      "Processing event 1377/1495...\n",
      "Processing event 1378/1495...\n",
      "Processing event 1379/1495...\n",
      "Processing event 1380/1495...\n",
      "Processing event 1381/1495...\n",
      "Processing event 1382/1495...\n",
      "Processing event 1383/1495...\n",
      "Processing event 1384/1495...\n",
      "Processing event 1385/1495...\n",
      "Processing event 1386/1495...\n",
      "Processing event 1387/1495...\n",
      "Processing event 1388/1495...\n",
      "Processing event 1389/1495...\n",
      "Processing event 1390/1495...\n",
      "Processing event 1391/1495...\n",
      "Processing event 1392/1495...\n",
      "Processing event 1393/1495...\n",
      "Processing event 1394/1495...\n",
      "Processing event 1395/1495...\n",
      "Processing event 1396/1495...\n",
      "Processing event 1397/1495...\n",
      "Processing event 1398/1495...\n",
      "Processing event 1399/1495...\n",
      "Processing event 1400/1495...\n",
      "Processing event 1401/1495...\n",
      "Processing event 1402/1495...\n",
      "Processing event 1403/1495...\n",
      "Processing event 1404/1495...\n",
      "Processing event 1405/1495...\n",
      "Processing event 1406/1495...\n",
      "Processing event 1407/1495...\n",
      "Processing event 1408/1495...\n",
      "Processing event 1409/1495...\n",
      "Processing event 1410/1495...\n",
      "Processing event 1411/1495...\n",
      "Processing event 1412/1495...\n",
      "Processing event 1413/1495...\n",
      "Processing event 1414/1495...\n",
      "Processing event 1415/1495...\n",
      "Processing event 1416/1495...\n",
      "Processing event 1417/1495...\n",
      "Processing event 1418/1495...\n",
      "Processing event 1419/1495...\n",
      "Processing event 1420/1495...\n",
      "Processing event 1421/1495...\n",
      "Processing event 1422/1495...\n",
      "Processing event 1423/1495...\n",
      "Processing event 1424/1495...\n",
      "Processing event 1425/1495...\n",
      "Processing event 1426/1495...\n",
      "Processing event 1427/1495...\n",
      "Processing event 1428/1495...\n",
      "Processing event 1429/1495...\n",
      "Processing event 1430/1495...\n",
      "Processing event 1431/1495...\n",
      "Processing event 1432/1495...\n",
      "Processing event 1433/1495...\n",
      "Processing event 1434/1495...\n",
      "Processing event 1435/1495...\n",
      "Processing event 1436/1495...\n",
      "Processing event 1437/1495...\n",
      "Processing event 1438/1495...\n",
      "Processing event 1439/1495...\n",
      "Processing event 1440/1495...\n",
      "Processing event 1441/1495...\n",
      "Processing event 1442/1495...\n",
      "Processing event 1443/1495...\n",
      "Processing event 1444/1495...\n",
      "Processing event 1445/1495...\n",
      "Processing event 1446/1495...\n",
      "Processing event 1447/1495...\n",
      "Processing event 1448/1495...\n",
      "Processing event 1449/1495...\n",
      "Processing event 1450/1495...\n",
      "Processing event 1451/1495...\n",
      "Processing event 1452/1495...\n",
      "Processing event 1453/1495...\n",
      "Processing event 1454/1495...\n",
      "Processing event 1455/1495...\n",
      "Processing event 1456/1495...\n",
      "Processing event 1457/1495...\n",
      "Processing event 1458/1495...\n",
      "Processing event 1459/1495...\n",
      "Processing event 1460/1495...\n",
      "Processing event 1461/1495...\n",
      "Processing event 1462/1495...\n",
      "Processing event 1463/1495...\n",
      "Processing event 1464/1495...\n",
      "Processing event 1465/1495...\n",
      "Processing event 1466/1495...\n",
      "Processing event 1467/1495...\n",
      "Processing event 1468/1495...\n",
      "Processing event 1469/1495...\n",
      "Processing event 1470/1495...\n",
      "Processing event 1471/1495...\n",
      "Processing event 1472/1495...\n",
      "Processing event 1473/1495...\n",
      "Processing event 1474/1495...\n",
      "Processing event 1475/1495...\n",
      "Processing event 1476/1495...\n",
      "Processing event 1477/1495...\n",
      "Processing event 1478/1495...\n",
      "Processing event 1479/1495...\n",
      "Processing event 1480/1495...\n",
      "Processing event 1481/1495...\n",
      "Processing event 1482/1495...\n",
      "Processing event 1483/1495...\n",
      "Processing event 1484/1495...\n",
      "Processing event 1485/1495...\n",
      "Processing event 1486/1495...\n",
      "Processing event 1487/1495...\n",
      "Processing event 1488/1495...\n",
      "Processing event 1489/1495...\n",
      "Processing event 1490/1495...\n",
      "Processing event 1491/1495...\n",
      "Processing event 1492/1495...\n",
      "Processing event 1493/1495...\n",
      "Processing event 1494/1495...\n",
      "Processing event 1495/1495...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1244483/1411266961.py:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  all_cluster_labels = np.array(all_cluster_labels)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "def HDBSCANClustering(all_predictions, \n",
    "                      min_cluster_size=5, \n",
    "                      min_samples=None, \n",
    "                      metric='euclidean', \n",
    "                      alpha=1.0,\n",
    "                      cluster_selection_method='eom',\n",
    "                      prediction_data=False,\n",
    "                      allow_single_cluster=True,\n",
    "                      core_dist_n_jobs=1,\n",
    "                      cluster_selection_epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Performs HDBSCAN clustering on a list of prediction arrays with more hyperparameter control.\n",
    "\n",
    "    Parameters:\n",
    "    - all_predictions: List of numpy arrays, each containing data points for an event.\n",
    "    - min_cluster_size: Minimum size of clusters.\n",
    "    - min_samples: Number of samples in a neighborhood for a point to be considered a core point.\n",
    "                   If None, it defaults to min_cluster_size.\n",
    "    - metric: Distance metric to use.\n",
    "    - alpha: Controls the balance between single linkage and average linkage clustering.\n",
    "    - cluster_selection_method: 'eom' (Excess of Mass) or 'leaf' for finer clusters.\n",
    "    - prediction_data: If True, allows later predictions on new data.\n",
    "    - allow_single_cluster: If True, allows a single large cluster when applicable.\n",
    "    - core_dist_n_jobs: Number of parallel jobs (-1 uses all cores).\n",
    "    - cluster_selection_epsilon: Threshold distance for cluster selection (default 0.0).\n",
    "\n",
    "    Returns:\n",
    "    - all_cluster_labels: NumPy array of cluster labels for all events.\n",
    "    \"\"\"\n",
    "    all_cluster_labels = []             \n",
    "\n",
    "    for i, pred in enumerate(all_predictions):\n",
    "        print(f\"Processing event {i+1}/{len(all_predictions)}...\")\n",
    "        \n",
    "        if len(pred) < 2:\n",
    "            # Assign all points to cluster 0 (since HDBSCAN uses -1 for noise)\n",
    "            cluster_labels = np.zeros(len(pred), dtype=int) \n",
    "        else:\n",
    "            # Initialize HDBSCAN with specified parameters\n",
    "            clusterer = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples if min_samples is not None else min_cluster_size,\n",
    "                metric=metric,\n",
    "                alpha=alpha,\n",
    "                cluster_selection_method=cluster_selection_method,\n",
    "                prediction_data=prediction_data,\n",
    "                allow_single_cluster=allow_single_cluster,\n",
    "                core_dist_n_jobs=core_dist_n_jobs,\n",
    "                cluster_selection_epsilon=cluster_selection_epsilon\n",
    "            )\n",
    "            \n",
    "            # Perform clustering\n",
    "            cluster_labels = clusterer.fit_predict(pred)  \n",
    "        \n",
    "        all_cluster_labels.append(cluster_labels)\n",
    "    \n",
    "    # Convert the list of cluster labels to a NumPy array\n",
    "    all_cluster_labels = np.array(all_cluster_labels)\n",
    "    return all_cluster_labels\n",
    "\n",
    "all_cluster_labels = HDBSCANClustering(\n",
    "    all_predictions, \n",
    "    min_cluster_size=2,  # Ensures at least 3 points per cluster\n",
    "    metric='euclidean',  # Change distance metric\n",
    "    alpha=1.0,  # Increase single linkage weighting\n",
    "    cluster_selection_method='leaf',  # Allow finer clusters\n",
    "    prediction_data=False,  # Enable future prediction capability\n",
    "    allow_single_cluster=True,  # Allow one large cluster\n",
    "    core_dist_n_jobs=-1  # Use all available CPU cores\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "653ec5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_ind = []\n",
    "\n",
    "for event_idx, labels in enumerate(all_cluster_labels):\n",
    "    event_clusters = {} \n",
    "    \n",
    "    for cluster_idx, cluster_label in enumerate(labels):\n",
    "        if cluster_label not in event_clusters:\n",
    "            event_clusters[cluster_label] = []\n",
    "        event_clusters[cluster_label].extend(Track_ind[event_idx][cluster_idx])\n",
    "    \n",
    "    recon_ind.append([event_clusters[label] for label in sorted(event_clusters.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf2eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sim_to_reco_score(CaloParticle, energies_indices, ReconstructedTrackster, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the sim-to-reco score for a given CaloParticle and ReconstructedTrackster.\n",
    "    \n",
    "    Parameters:\n",
    "    - CaloParticle: array of Layer Clusters in the CaloParticle.\n",
    "    - Multiplicity: array of Multiplicity for layer clusters in CP\n",
    "    - energies_indices: array of energies associated with all LC (indexed by LC).\n",
    "    - ReconstructedTrackster: array of LC in the reconstructed Trackster.\n",
    "    \n",
    "    Returns:\n",
    "    - sim_to_reco_score: the calculated sim-to-reco score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    if total_energy_caloparticle == 0:\n",
    "        return 1.0  # No energy in the CaloParticle implies perfect mismatch\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    i = 0\n",
    "    # Iterate over all DetIds in the CaloParticle\n",
    "    for det_id in CaloParticle:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in CaloParticle\n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1 if det_id in ReconstructedTrackster else 0.0\n",
    "        # Fraction of energy in the CaloParticle (fr_k^SC)\n",
    "        fr_sc_k = 1 / Multi[i]\n",
    "\n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_sc_k ** 2                # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_sc_k ** 2) * (energy_k ** 2)\n",
    "        i+=1\n",
    "\n",
    "    # Calculate score\n",
    "    sim_to_reco_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return sim_to_reco_score\n",
    "\n",
    "def calculate_reco_to_sim_score(ReconstructedTrackster, energies_indices, CaloParticle, Multi):\n",
    "    \"\"\"\n",
    "    Calculate the reco-to-sim score for a given ReconstructedTrackster and CaloParticle.\n",
    "\n",
    "    Parameters:\n",
    "    - ReconstructedTrackster: array of DetIds in the ReconstructedTrackster.\n",
    "    - energies_indices: array of energies associated with all DetIds (indexed by DetId).\n",
    "    - CaloParticle: array of DetIds in the CaloParticle.\n",
    "\n",
    "    Returns:\n",
    "    - reco_to_sim_score: the calculated reco-to-sim score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    # Calculate total energy of the ReconstructedTrackster\n",
    "    total_energy_trackster = sum(energies_indices[det_id] for det_id in ReconstructedTrackster)\n",
    "    if total_energy_trackster == 0:\n",
    "        return 1.0  # No energy in the Trackster implies perfect mismatch\n",
    "\n",
    "    energy_caloparticle_lc = energies_indices[CaloParticle] / Multi\n",
    "    total_energy_caloparticle = sum(energy_caloparticle_lc)\n",
    "    # Iterate over all DetIds in the ReconstructedTrackster\n",
    "    for det_id in ReconstructedTrackster:\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in the Trackster\n",
    "        \n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1\n",
    "\n",
    "        #fr_sc_k = 1 if det_id in CaloParticle else 0.0\n",
    "        if det_id in CaloParticle:\n",
    "            index = np.where(CaloParticle == det_id)[0][0]  # Find the index\n",
    "            Multiplicity = Multi[index]\n",
    "            fr_sc_k = 1\n",
    "        else:\n",
    "            fr_sc_k = 0\n",
    "            \n",
    "        # Update numerator using the min function\n",
    "        numerator += min(\n",
    "            (fr_tst_k - fr_sc_k) ** 2,  # First term in the min function\n",
    "            fr_tst_k ** 2               # Second term in the min function\n",
    "        ) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_tst_k ** 2) * (energy_k ** 2)\n",
    "\n",
    "    # Calculate score\n",
    "    reco_to_sim_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return reco_to_sim_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6186544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "def calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, multi, num_events = 100):\n",
    "    \"\"\"\n",
    "    Calculate sim-to-reco and reco-to-sim scores for all CaloParticle and ReconstructedTrackster combinations across all events.\n",
    "\n",
    "    Parameters:\n",
    "    - GT_ind: List of CaloParticle indices for all events.\n",
    "    - energies: List of energy arrays for all events.\n",
    "    - recon_ind: List of ReconstructedTrackster indices for all events.\n",
    "    - LC_x, LC_y, LC_z, LC_eta: Lists of x, y, z positions and eta values for all DetIds across events.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing scores and additional features for each CaloParticle-Trackster combination across all events.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Loop over all events with a progress bar\n",
    "    for event_index in range(num_events):\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_multi = multi[event_index]\n",
    "\n",
    "        # Extract layer cluster positions and eta for this event\n",
    "        event_x = np.array(LC_x[event_index])\n",
    "        event_y = np.array(LC_y[event_index])\n",
    "        event_z = np.array(LC_z[event_index])\n",
    "        event_eta = np.array(LC_eta[event_index])\n",
    "\n",
    "        # Compute barycenter for each CaloParticle\n",
    "        cp_barycenters = []\n",
    "        cp_avg_etas = []\n",
    "        for caloparticle in caloparticles:\n",
    "            # Compute barycenter (x, y, z)\n",
    "            \n",
    "            barycenter_x = np.mean([event_x[det_id] for det_id in caloparticle])\n",
    "            barycenter_y = np.mean([event_y[det_id] for det_id in caloparticle])\n",
    "            barycenter_z = np.mean([event_z[det_id] for det_id in caloparticle])\n",
    "            cp_barycenters.append(np.array([barycenter_x, barycenter_y, barycenter_z]))\n",
    "            \n",
    "            # Compute average eta\n",
    "            avg_eta = np.mean([event_eta[det_id] for det_id in caloparticle])\n",
    "            cp_avg_etas.append(avg_eta)\n",
    "\n",
    "        # Compute separation between two CaloParticles if at least two exist\n",
    "        if len(cp_barycenters) >= 2:\n",
    "            cp_separation = np.linalg.norm(cp_barycenters[0] - cp_barycenters[1])\n",
    "        else:\n",
    "            cp_separation = 0.0\n",
    "            \n",
    "        trackster_det_id_sets = [set(trackster) for trackster in tracksters]\n",
    "\n",
    "        # Loop over all CaloParticles\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            Calo_multi = event_multi[calo_idx]\n",
    "            calo_det_ids = set(calo_id for calo_id in caloparticle)\n",
    "            # Loop over all Tracksters\n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                # Calculate sim-to-reco score\n",
    "                trackster_det_ids = trackster_det_id_sets[trackster_idx]\n",
    "                shared_det_ids = calo_det_ids.intersection(trackster_det_ids)\n",
    "                \n",
    "                # Calculate shared_energy by summing energies of shared det_ids\n",
    "                shared_energy = np.sum(event_energies[list(shared_det_ids)]) if shared_det_ids else 0.0\n",
    "                \n",
    "                \n",
    "                sim_to_reco_score = calculate_sim_to_reco_score(caloparticle, event_energies, trackster, Calo_multi)\n",
    "                # Calculate reco-to-sim score\n",
    "                reco_to_sim_score = calculate_reco_to_sim_score(trackster, event_energies, caloparticle, Calo_multi)\n",
    "\n",
    "                # Calculate total energy for CaloParticle and Trackster\n",
    "                cp_energy_lc2 = event_energies[caloparticle] / Calo_multi\n",
    "                cp_energy = np.sum(cp_energy_lc2)\n",
    "                \n",
    "                trackster_energy = np.sum([event_energies[det_id] for det_id in trackster])\n",
    "\n",
    "                # Calculate energy difference ratio\n",
    "                energy_diff_ratio = (trackster_energy / cp_energy if cp_energy != 0 else None)\n",
    "\n",
    "                # Append results\n",
    "                all_results.append({\n",
    "                    \"event_index\": event_index,\n",
    "                    \"cp_id\": calo_idx,\n",
    "                    \"trackster_id\": trackster_idx,\n",
    "                    \"sim_to_reco_score\": sim_to_reco_score,\n",
    "                    \"reco_to_sim_score\": reco_to_sim_score,\n",
    "                    \"cp_energy\": cp_energy,\n",
    "                    \"trackster_energy\": trackster_energy,\n",
    "                    \"cp_avg_eta\": cp_avg_etas[calo_idx],\n",
    "                    \"cp_separation\": cp_separation,\n",
    "                    \"energy_ratio\": energy_diff_ratio,\n",
    "                    \"shared_energy\": shared_energy  # New column\n",
    "                })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f59c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL = calculate_all_event_scores(GT_ind, energies, recon_ind, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 100)\n",
    "#df_TICL = calculate_all_event_scores(GT_ind, energies, MT_ind, LC_x, LC_y, LC_z, LC_eta, GT_mult, num_events = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75774df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Our Model\n",
      "Efficiency: 0.7620 (381 associated CPs out of 500 total CPs)\n",
      "Purity: 0.9213 (515 associated Tracksters out of 559 total Tracksters)\n",
      "Num tracksters ratio: 1.1180\n",
      "Average Energy Ratio: 1.1556\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_TICL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1244483/4040651649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mour_model_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_CL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Our Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mcern_model_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_TICL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CERN Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_TICL' is not defined"
     ]
    }
   ],
   "source": [
    "#5: Print metrics\n",
    "\n",
    "def calculate_metrics(df, model_name):\n",
    "    # ----- Efficiency Calculation -----\n",
    "    # Step 1: Filter out rows where 'cp_id' is NaN\n",
    "    cp_valid = df.dropna(subset=['cp_id']).copy()\n",
    "\n",
    "    # Step 2: Group by 'event_index' and 'cp_id' to process each CaloParticle individually\n",
    "    cp_grouped = cp_valid.groupby(['event_index', 'cp_id'])\n",
    "\n",
    "    # Step 3: For each CaloParticle, check if any 'shared_energy' >= 50% of 'cp_energy'\n",
    "    def is_cp_associated(group):\n",
    "        cp_energy = group['cp_energy'].iloc[0]  # Assuming 'cp_energy' is consistent within the group\n",
    "        threshold = 0.5 * cp_energy\n",
    "        return (group['shared_energy'] >= threshold).any()\n",
    "\n",
    "    # Apply the association function to each group\n",
    "    cp_associated = cp_grouped.apply(is_cp_associated)\n",
    "\n",
    "    # Step 4: Calculate the number of associated CaloParticles and total CaloParticles\n",
    "    num_associated_cp = cp_associated.sum()\n",
    "    total_cp = cp_associated.count()\n",
    "    efficiency = num_associated_cp / total_cp if total_cp > 0 else 0\n",
    "\n",
    "    # ----- Purity Calculation -----\n",
    "    tst_valid = df.dropna(subset=['trackster_id']).copy()\n",
    "    tst_grouped = tst_valid.groupby(['event_index', 'trackster_id'])\n",
    "    tst_associated = tst_grouped['reco_to_sim_score'].min() < 0.2\n",
    "    num_associated_tst = tst_associated.sum()\n",
    "    total_tst = tst_associated.count()\n",
    "    purity = num_associated_tst / total_tst if total_tst > 0 else 0\n",
    "\n",
    "    # ----- Average Energy Ratio Calculation -----\n",
    "    low_score_mask = df['sim_to_reco_score'] < 0.2\n",
    "    low_score_events = df[low_score_mask]\n",
    "    if not low_score_events.empty:\n",
    "        avg_energy_ratio = (low_score_events['trackster_energy'] / low_score_events['cp_energy']).mean()\n",
    "    else:\n",
    "        avg_energy_ratio = 0\n",
    "\n",
    "    # Print results for the model\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Efficiency: {efficiency:.4f} ({num_associated_cp} associated CPs out of {total_cp} total CPs)\")\n",
    "    print(f\"Purity: {purity:.4f} ({num_associated_tst} associated Tracksters out of {total_tst} total Tracksters)\")\n",
    "    print(f\"Num tracksters ratio: {total_tst / total_cp if total_cp > 0 else 0:.4f}\")\n",
    "    print(f\"Average Energy Ratio: {avg_energy_ratio:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'efficiency': efficiency,\n",
    "        'purity': purity,\n",
    "        'avg_energy_ratio': avg_energy_ratio,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "our_model_metrics = calculate_metrics(df_CL, \"Our Model\")\n",
    "cern_model_metrics = calculate_metrics(df_TICL, \"CERN Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9292101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
