{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bbc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0: imports\n",
    "\n",
    "import uproot \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from dataAnalyse import CCV1\n",
    "from torch_geometric.data import DataLoader \n",
    "from models import Net_SEC, Net_GAT, Net_SECGAT, Net_Trans\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791eb3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Loading tracksters data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vols/cms/mm1221/Data/mix/test/raw/test.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:45<00:00, 45.99s/it]\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#1: Load Data + Model\n",
    "#1.1: Load Data Through the dataloader - used for predictions\n",
    "testpath = \"/vols/cms/mm1221/Data/mix/test/\"  \n",
    "#testpath = \"/vols/cms/mm1221/Data/100k/5pi/test/\"\n",
    "data_test = CCV1(testpath, max_events=15000, inp = 'test')\n",
    "test_loader = DataLoader(data_test, batch_size=1, shuffle=False, follow_batch=['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b31596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      ".\n",
      "...\n",
      "..\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Also load explicitely, used for analysis and plots\n",
    "data_path = '/vols/cms/mm1221/Data/mix/test/raw/test.root'\n",
    "#data_path = '/vols/cms/mm1221/Data/100k/5pi/test/raw/test.root' \n",
    "data_file = uproot.open(data_path)\n",
    "\n",
    "ass = data_file['associations']['tsCLUE3D_recoToSim_CP'].array()\n",
    "print(\"...\")\n",
    "scores = data_file['associations']['tsCLUE3D_recoToSim_CP_score'].array()\n",
    "Track_ind = data_file['tracksters']['vertices_indexes'].array()\n",
    "print('.')\n",
    "GT_ind = data_file['simtrackstersCP']['vertices_indexes'].array()\n",
    "GT_mult = data_file['simtrackstersCP']['vertices_multiplicity'].array()\n",
    "GT_bc = data_file['simtrackstersCP']['barycenter_x'].array()\n",
    "energies = data_file['clusters']['energy'].array()\n",
    "print(\"...\")\n",
    "LC_x = data_file['clusters']['position_x'].array()\n",
    "LC_y = data_file['clusters']['position_y'].array()\n",
    "LC_z = data_file['clusters']['position_z'].array()\n",
    "print('..')\n",
    "LC_eta = data_file['clusters']['position_eta'].array()\n",
    "MT_ind = data_file['trackstersMerged']['vertices_indexes'].array()\n",
    "print(\"...\")\n",
    "\n",
    "\n",
    "#1.3 Filter so get rid of events with 0 calo particles\n",
    "t_bx = data_file['tracksters']['barycenter_x'].array()\n",
    "print(\"...\")\n",
    "\n",
    "skim_mask = []\n",
    "for e in t_bx:\n",
    "    if len(e) == 0:\n",
    "        skim_mask.append(False)\n",
    "    else:\n",
    "        skim_mask.append(True)\n",
    "\n",
    "Track_ind = Track_ind[skim_mask]\n",
    "GT_ind = GT_ind[skim_mask]\n",
    "GT_mult = GT_mult[skim_mask]\n",
    "energies = energies[skim_mask]\n",
    "LC_x = LC_x[skim_mask]\n",
    "LC_y = LC_y[skim_mask]\n",
    "LC_z = LC_z[skim_mask]\n",
    "LC_eta = LC_eta[skim_mask]\n",
    "MT_ind = MT_ind[skim_mask]\n",
    "ass = ass[skim_mask]\n",
    "scores=scores[skim_mask]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "78689927",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_mult = data_file['trackstersMerged']['vertices_multiplicity'].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810c3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgid = data_file['simtrackstersCP']['pdgID'].array()\n",
    "reg_en = data_file['simtrackstersCP']['regressed_energy'].array()\n",
    "MT_mult = data_file['trackstersMerged']['vertices_multiplicity'].array()\n",
    "skim_mask = []\n",
    "for e in t_bx:\n",
    "    if len(e) == 0:\n",
    "        skim_mask.append(False)\n",
    "    else:\n",
    "        skim_mask.append(True)\n",
    "        \n",
    "pgid = pgid[skim_mask]\n",
    "reg_en = reg_en[skim_mask]\n",
    "MT_mult = MT_mult[skim_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "805b30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_Trans(\n",
       "  (lc_encode): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "  )\n",
       "  (transformer_layers): ModuleList(\n",
       "    (0-3): 4 x GraphTransformerLayer(\n",
       "      (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (out_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (att_dropout): Dropout(p=0.3, inplace=False)\n",
       "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.3, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import Net_split\n",
    "# load the model \n",
    "#model = Net(\n",
    "    #hidden_dim=128,\n",
    "    #num_layers=4,\n",
    "    #dropout=0.3,\n",
    "    #contrastive_dim=16\n",
    "#)\n",
    "model = Net_Trans(\n",
    "    hidden_dim=128,num_layers=4, dropout=0.3, contrastive_dim=16, num_heads=4\n",
    ")\n",
    "\n",
    "checkpoint= torch.load('/vols/cms/mm1221/hgcal/Mixed/Track/Fraction/runs_trans/epoch-255.pt',  map_location=torch.device('cpu'))\n",
    "#checkpoint= torch.load('/vols/cms/mm1221/hgcal/elec5New/Track/NegativeMining/resultsSECNeg/epoch-228.pt',  map_location=torch.device('cpu'))\n",
    "#checkpoint= torch.load('/vols/cms/mm1221/hgcal/pion5New/Track/NegativeMining/resultsSECNeg/best_model.pt',  map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])  \n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0e18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "# Create new lists to store the filtered results\n",
    "# This makes sure GT_ind, MT_ind, Recon_ind have the same indices\n",
    "filtered_GT_ind = []\n",
    "filtered_GT_mult = []\n",
    "filtered_MT_ind = []\n",
    "\n",
    "\n",
    "for event_idx, track_indices in enumerate(Track_ind):\n",
    "    # Flatten the current event's track indices and convert to a set\n",
    "    track_flat = set(ak.flatten(track_indices).tolist())  # Ensure it contains only integers\n",
    "    \n",
    "    # Filter GT_ind and GT_mult for the current event, preserving structure\n",
    "    event_GT_ind = GT_ind[event_idx]\n",
    "    event_GT_mult = GT_mult[event_idx]\n",
    "    filtered_event_GT_ind = []\n",
    "    filtered_event_GT_mult = []\n",
    "    for sublist_ind, sublist_mult in zip(event_GT_ind, event_GT_mult):\n",
    "        filtered_sublist_ind = [idx for idx in sublist_ind if idx in track_flat]\n",
    "        filtered_sublist_mult = [mult for idx, mult in zip(sublist_ind, sublist_mult) if idx in track_flat]\n",
    "        filtered_event_GT_ind.append(filtered_sublist_ind)\n",
    "        filtered_event_GT_mult.append(filtered_sublist_mult)\n",
    "\n",
    "    # Filter MT_ind for the current event, preserving structure\n",
    "    event_MT_ind = MT_ind[event_idx]\n",
    "    filtered_event_MT_ind = []\n",
    "    for sublist in event_MT_ind:\n",
    "        filtered_sublist = [idx for idx in sublist if idx in track_flat]\n",
    "        filtered_event_MT_ind.append(filtered_sublist)\n",
    "\n",
    "    # Append filtered results\n",
    "    filtered_GT_ind.append(filtered_event_GT_ind)\n",
    "    filtered_GT_mult.append(filtered_event_GT_mult)\n",
    "    filtered_MT_ind.append(filtered_event_MT_ind)\n",
    "\n",
    "# Convert the filtered results back to awkward arrays\n",
    "GT_ind_filt = ak.Array(filtered_GT_ind)\n",
    "GT_mult_filt = ak.Array(filtered_GT_mult)\n",
    "MT_ind_filt = ak.Array(filtered_MT_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2193ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def Aggloremative(all_predictions,  threshold = 0.7, metric = 'cosine', linkage = 'average'):\n",
    "    all_cluster_labels = []             \n",
    "\n",
    "    for i, pred in enumerate(all_predictions):\n",
    "\n",
    "        if len(pred) < 2:\n",
    "            cluster_labels = np.ones(len(pred), dtype=int) \n",
    "        else:\n",
    "            agglomerative = AgglomerativeClustering(\n",
    "                n_clusters=None,                 \n",
    "                distance_threshold=threshold,\n",
    "                linkage=linkage,\n",
    "                metric=metric,\n",
    "                compute_distances=True\n",
    "            )\n",
    "            cluster_labels = agglomerative.fit_predict(pred) \n",
    "\n",
    "        all_cluster_labels.append(cluster_labels)\n",
    "\n",
    "    all_cluster_labels = np.array(all_cluster_labels)\n",
    "    return all_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "33ee64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inference time: 0.011404897876366407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#3: Make Predictions + Cluster -> Calculate the inference time\n",
    "#3.1: Make Predictions\n",
    "\n",
    "import math\n",
    "\n",
    "all_predictions = []  \n",
    "total_times = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, data in enumerate(data_test):\n",
    "    \n",
    "    if i>500:\n",
    "        break\n",
    "\n",
    "    \n",
    "    edge_index = knn_graph(data.x, k=32)  \n",
    "    predictions = model(data.x, edge_index, 1)\n",
    "    all_predictions.append(predictions[0].detach().cpu().numpy())  \n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "#3.2: Cluster using threshold found in Script A\n",
    "\n",
    "all_cluster_labels = Aggloremative(all_predictions, threshold = 0.2)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "#3.3: Calculate average inference time\n",
    "\n",
    "time_diff = end_time - start_time\n",
    "inference_time = time_diff/len(all_cluster_labels)\n",
    "print(\"average inference time:\", inference_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56599c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_centroids_from_high_similarity(embeddings, sim_threshold=0.9999):\n",
    "    \"\"\"\n",
    "    Groups embeddings that are extremely close (cosine similarity >= sim_threshold)\n",
    "    into connected components, then returns the mean (centroid) of each group.\n",
    "    \n",
    "    embeddings: np.ndarray of shape (N, D)\n",
    "    sim_threshold: float (close to 1.0). Two embeddings are in the same group if\n",
    "                   their similarity >= sim_threshold.\n",
    "    \n",
    "    Returns:\n",
    "      centroids: np.ndarray of shape (num_centroids, D)\n",
    "    \"\"\"\n",
    "    if embeddings.shape[0] == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Compute cosine similarity between all pairs.\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    # Adjacency mask for \"almost identical\" embeddings\n",
    "    adjacency = (sim_matrix >= sim_threshold)\n",
    "    \n",
    "    visited = np.zeros(embeddings.shape[0], dtype=bool)\n",
    "    centroids = []\n",
    "    \n",
    "    # Find connected components in 'adjacency'\n",
    "    for start_node in range(embeddings.shape[0]):\n",
    "        if not visited[start_node]:\n",
    "            # BFS or DFS to collect all nodes connected to 'start_node'\n",
    "            queue = deque([start_node])\n",
    "            group_indices = []\n",
    "            visited[start_node] = True\n",
    "            \n",
    "            while queue:\n",
    "                node = queue.popleft()\n",
    "                group_indices.append(node)\n",
    "                # For every node that is not visited but is adjacent, add to queue\n",
    "                neighbors = np.where(adjacency[node])[0]\n",
    "                for nbr in neighbors:\n",
    "                    if not visited[nbr]:\n",
    "                        visited[nbr] = True\n",
    "                        queue.append(nbr)\n",
    "            \n",
    "            # Compute centroid of this group\n",
    "            group_embeddings = embeddings[group_indices]\n",
    "            centroid = group_embeddings.mean(axis=0)\n",
    "            centroids.append(centroid)\n",
    "    \n",
    "    return np.array(centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "549eedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_embeddings_to_centroids(embeddings, centroids, assign_threshold=0.9):\n",
    "    \"\"\"\n",
    "    For each embedding, compute the cosine similarity to each centroid.\n",
    "    - If exactly one centroid is above assign_threshold, assign fully to that centroid (one-hot).\n",
    "    - Otherwise, create a soft assignment across all centroids, weighted by similarity >= 0.0.\n",
    "    \n",
    "    embeddings: np.ndarray of shape (N, D)\n",
    "    centroids: np.ndarray of shape (C, D)\n",
    "    assign_threshold: float, e.g. 0.9. If there's exactly one centroid above this sim,\n",
    "                      we do a one-hot assignment.\n",
    "    \n",
    "    Returns:\n",
    "      assignments: np.ndarray of shape (N, C), with each row summing to 1\n",
    "    \"\"\"\n",
    "    N = embeddings.shape[0]\n",
    "    C = centroids.shape[0]\n",
    "    if C == 0 or N == 0:\n",
    "        return np.zeros((N, C))\n",
    "\n",
    "    # Compute cosine similarity shape: (N, C)\n",
    "    sims = cosine_similarity(embeddings, centroids)\n",
    "    \n",
    "    # We'll build the assignment matrix row by row.\n",
    "    assignments = np.zeros((N, C), dtype=float)\n",
    "    \n",
    "    for i in range(N):\n",
    "        row = sims[i]  # similarities to each centroid\n",
    "        # Find how many exceed assign_threshold\n",
    "        above_thresh = np.where(row >= assign_threshold)[0]\n",
    "        \n",
    "        if len(above_thresh) == 1:\n",
    "            # Exactly one centroid is \"very close\" => full assignment to that one\n",
    "            assignments[i, above_thresh[0]] = 1.0\n",
    "        else:\n",
    "            # Soft assignment across all centroids (or maybe just among top ones).\n",
    "            # We'll turn similarities into a positive weighting and then normalize.\n",
    "            # But we first ensure negatives don't cause trouble:\n",
    "            # If you want to ignore negative sims, you can clamp them to zero.\n",
    "            row_clamped = np.clip(row, 0.0, None)\n",
    "            if np.sum(row_clamped) == 0.0:\n",
    "                # fallback if all sims were negative\n",
    "                # pick the best centroid\n",
    "                best_idx = np.argmax(row)\n",
    "                assignments[i, best_idx] = 1.0\n",
    "            else:\n",
    "                # Weighted by nonnegative similarities\n",
    "                assignments[i, :] = row_clamped / np.sum(row_clamped)\n",
    "    \n",
    "    return assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5f1f979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 0.013670637222107299\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import deque\n",
    "\n",
    "# 1) Make Predictions\n",
    "all_predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, data in enumerate(data_test):\n",
    "    if i > 500:\n",
    "        break\n",
    "    \n",
    "    edge_index = knn_graph(data.x, k=90)  \n",
    "    predictions = model(data.x, edge_index, 1)\n",
    "    # predictions[0] are the embeddings for this event\n",
    "    all_predictions.append(predictions[0].detach().cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions, dtype=object)  # keep them ragged if needed\n",
    "\n",
    "# 2) For each event, build centroids and compute final assignments\n",
    "all_centroids_list = []\n",
    "all_assignments_list = []\n",
    "\n",
    "for event_embeddings in all_predictions:\n",
    "    # 2A. Build centroids\n",
    "    centroids = build_centroids_from_high_similarity(event_embeddings, sim_threshold=0.80)\n",
    "    # 2B. Assign each embedding to centroids\n",
    "    assignments = assign_embeddings_to_centroids(event_embeddings, centroids, assign_threshold=0.85)\n",
    "    \n",
    "    all_centroids_list.append(centroids)\n",
    "    all_assignments_list.append(assignments)\n",
    "\n",
    "end_time = time.time()\n",
    "time_diff = end_time - start_time\n",
    "inference_time = time_diff / len(all_predictions)\n",
    "print(f\"Average inference time: {inference_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b6cc796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.09e-05, 1, 1, 1, 1, 1, 1], [0.121, 0.634, ... 1, 1], [0, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2af9ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.01153597 0.37527496 0.44563651 0.0567202  0.11083233\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.        ]\n",
      " [0.         0.         0.12867112 0.21640216 0.39396787 0.21686704\n",
      "  0.04409179]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(all_assignments_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "42b5838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6803\n",
      "Split accuracy: 0.4036\n",
      "Not-split accuracy: 0.7390\n",
      "Total nodes: 10596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_splitting_accuracy(all_soft_assignments, all_scores, score_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Computes how often the model's 'split' prediction matches the 'split' ground truth,\n",
    "    reporting overall accuracy as well as separate accuracies for split and not-split nodes.\n",
    "    \n",
    "    A node is considered 'truly split' if it has >=2 scores above `score_threshold`.\n",
    "    A node is considered 'predicted split' if it has >=2 soft assignment entries > 0.0.\n",
    "    \n",
    "    Parameters:\n",
    "      - all_soft_assignments: list of length E, each element is (N_i, K) soft assignments\n",
    "      - all_scores: list of length E, each element is (N_i, S) ground-truth score vectors\n",
    "      - score_threshold: float, threshold above which a score is considered significant\n",
    "    \n",
    "    Returns:\n",
    "      - overall_accuracy: fraction of nodes where predicted split matches ground-truth split\n",
    "      - split_accuracy: fraction of truly split nodes correctly identified\n",
    "      - not_split_accuracy: fraction of truly not-split nodes correctly identified\n",
    "      - total_nodes: total number of nodes across all events\n",
    "    \"\"\"\n",
    "    # Counters\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    split_correct = 0\n",
    "    split_total = 0\n",
    "    \n",
    "    not_split_correct = 0\n",
    "    not_split_total = 0\n",
    "\n",
    "    # Loop over events\n",
    "    for soft_assignments_event, scores_event in zip(all_soft_assignments, all_scores):\n",
    "        # soft_assignments_event: shape (N, K)\n",
    "        # scores_event: shape (N, S)\n",
    "        num_nodes = soft_assignments_event.shape[0]\n",
    "        \n",
    "        for i in range(num_nodes):\n",
    "            # Ground truth: is node i split?\n",
    "            gt_split = (np.sum(scores_event[i] > score_threshold) >= 2)\n",
    "            \n",
    "            # Predicted: is node i split?\n",
    "            row = soft_assignments_event[i]\n",
    "            pred_split = (np.sum(row > 0.0) >= 2)\n",
    "            \n",
    "            # Overall correctness\n",
    "            if gt_split == pred_split:\n",
    "                total_correct += 1\n",
    "            total_nodes += 1\n",
    "            \n",
    "            # Split-specific counters\n",
    "            if gt_split:\n",
    "                split_total += 1\n",
    "                if pred_split == gt_split:\n",
    "                    split_correct += 1\n",
    "            else:\n",
    "                not_split_total += 1\n",
    "                if pred_split == gt_split:\n",
    "                    not_split_correct += 1\n",
    "\n",
    "    overall_accuracy = total_correct / total_nodes if total_nodes > 0 else 0.0\n",
    "    split_accuracy = split_correct / split_total if split_total > 0 else 0.0\n",
    "    not_split_accuracy = not_split_correct / not_split_total if not_split_total > 0 else 0.0\n",
    "    \n",
    "    return overall_accuracy, split_accuracy, not_split_accuracy, total_nodes\n",
    "\n",
    "# Example usage:\n",
    "# Suppose you have `all_scores` loaded in the same structure as `all_soft_assignments`.\n",
    "accuracy_overall, accuracy_split, accuracy_not_split, total_nodes = compute_splitting_accuracy(\n",
    "     all_assignments_list, 1-scores, score_threshold=0.2\n",
    "     )\n",
    "print(f\"Overall accuracy: {accuracy_overall:.4f}\")\n",
    "print(f\"Split accuracy: {accuracy_split:.4f}\")\n",
    "print(f\"Not-split accuracy: {accuracy_not_split:.4f}\")\n",
    "print(f\"Total nodes: {total_nodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d258c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:03<00:00, 56.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Positive Edge Cosine Similarity: 0.7731\n",
      "Mean Negative Edge Cosine Similarity: 0.0484\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to compute cosine similarities using sklearn's cosine_similarity.\n",
    "def compute_cosine_similarities(embeddings, edge_indices, skip_self=True):\n",
    "    \"\"\"\n",
    "    Given embeddings (NumPy array of shape [N, D]) and edge_indices (iterable of edges, each edge being a list or array),\n",
    "    compute the cosine similarity for each edge using sklearn.metrics.pairwise.cosine_similarity.\n",
    "    If skip_self is True, edges where source == target are skipped.\n",
    "    Returns a NumPy array of similarities.\n",
    "    \"\"\"\n",
    "    sims = []\n",
    "    for edge in edge_indices:\n",
    "        # Convert edge to a list if it's a tensor.\n",
    "        edge = edge.tolist() if isinstance(edge, torch.Tensor) else edge\n",
    "        src, tgt = edge\n",
    "        if skip_self and src == tgt:\n",
    "            continue\n",
    "        # Compute cosine similarity for the two embeddings.\n",
    "        sim = cosine_similarity(embeddings[[src, tgt], :])[0, 1]\n",
    "        sims.append(sim)\n",
    "    return np.array(sims)\n",
    "\n",
    "num_events = 200  # Process the first 100 events\n",
    "\n",
    "all_pos_sims = []\n",
    "all_neg_sims = []\n",
    "\n",
    "for i in tqdm(range(num_events), desc=\"Processing events\"):\n",
    "    # Convert predictions for event i to a tensor and normalize them.\n",
    "    pred_tensor = torch.tensor(all_predictions[i], dtype=torch.float32)\n",
    "    pred_norm = F.normalize(pred_tensor, p=2, dim=1)\n",
    "    # Convert the normalized embeddings to a NumPy array.\n",
    "    embeddings = pred_norm.cpu().numpy()\n",
    "    \n",
    "    # Get positive and negative edge indices for the event.\n",
    "    pos_edge_indices = data_test[i].x_pe\n",
    "    neg_edge_indices = data_test[i].x_ne\n",
    "    # If edge indices are tensors, convert them to NumPy arrays.\n",
    "    if torch.is_tensor(pos_edge_indices):\n",
    "        pos_edge_indices = pos_edge_indices.cpu().numpy()\n",
    "    if torch.is_tensor(neg_edge_indices):\n",
    "        neg_edge_indices = neg_edge_indices.cpu().numpy()\n",
    "    \n",
    "    # Compute cosine similarities for positive and negative edges.\n",
    "    pos_sims = compute_cosine_similarities(embeddings, pos_edge_indices, skip_self=True)\n",
    "    neg_sims = compute_cosine_similarities(embeddings, neg_edge_indices, skip_self=True)\n",
    "    \n",
    "    all_pos_sims.extend(pos_sims.tolist())\n",
    "    all_neg_sims.extend(neg_sims.tolist())\n",
    "\n",
    "all_pos_sims = np.array(all_pos_sims)\n",
    "all_neg_sims = np.array(all_neg_sims)\n",
    "\n",
    "mean_pos_sim = np.mean(all_pos_sims)\n",
    "mean_neg_sim = np.mean(all_neg_sims)\n",
    "\n",
    "print(f\"Mean Positive Edge Cosine Similarity: {mean_pos_sim:.4f}\")\n",
    "print(f\"Mean Negative Edge Cosine Similarity: {mean_neg_sim:.4f}\")\n",
    "\n",
    "# Plot histograms for positive and negative cosine similarities.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(all_pos_sims, bins=50, alpha=0.7, label=\"Positive Pairs\")\n",
    "plt.hist(all_neg_sims, bins=50, alpha=0.7, label=\"Negative Pairs\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Cosine Similarity Distribution (First 100 Events)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48d04cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_index(ReconstructedTrackster, det_id):\n",
    "    \"\"\"\n",
    "    Returns the index of det_id in ReconstructedTrackster.\n",
    "    If ReconstructedTrackster is a NumPy array, uses np.where.\n",
    "    If it is a list (or supports .index()), uses .index().\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    if isinstance(ReconstructedTrackster, np.ndarray):\n",
    "        indices = np.where(ReconstructedTrackster == det_id)[0]\n",
    "        return indices[0] if indices.size > 0 else None\n",
    "    elif hasattr(ReconstructedTrackster, 'index'):\n",
    "        try:\n",
    "            return ReconstructedTrackster.index(det_id)\n",
    "        except ValueError:\n",
    "            return None\n",
    "    else:\n",
    "        # Fallback: try converting to list and then use .index()\n",
    "        try:\n",
    "            reco_list = list(ReconstructedTrackster)\n",
    "            return reco_list.index(det_id)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def calculate_sim_to_reco_score(CaloParticle, energies_indices, ReconstructedTrackster, track_mult, calo_mult):\n",
    "    \"\"\"\n",
    "    Calculate the sim-to-reco score for a given CaloParticle and ReconstructedTrackster.\n",
    "    \n",
    "    Parameters:\n",
    "      - CaloParticle: array-like, list or array of layer clusters in the CaloParticle.\n",
    "      - energies_indices: array-like, mapping of energies by DetId.\n",
    "      - ReconstructedTrackster: array-like (NumPy array or list) of DetIds in the reconstructed Trackster.\n",
    "      - track_mult: array-like of multiplicities for the trackster nodes.\n",
    "      - calo_mult: array-like of multiplicities for the CaloParticle.\n",
    "    \n",
    "    Returns:\n",
    "      - sim_to_reco_score: the calculated sim-to-reco score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    # Iterate over all DetIds in the CaloParticle\n",
    "    for i, det_id in enumerate(CaloParticle):\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in CaloParticle\n",
    "\n",
    "        # Determine the fraction of energy in the Trackster\n",
    "        if det_id in ReconstructedTrackster:\n",
    "            index = find_index(ReconstructedTrackster, det_id)\n",
    "            if index is not None:\n",
    "                fr_tst_k = 1.0 / track_mult[index]\n",
    "            else:\n",
    "                fr_tst_k = 0.0\n",
    "        else:\n",
    "            fr_tst_k = 0.0  # det_id not found in ReconstructedTrackster\n",
    "\n",
    "        # Fraction of energy in the CaloParticle\n",
    "        fr_sc_k = 1.0 / calo_mult[i]\n",
    "\n",
    "        # Update numerator using the min function\n",
    "        numerator += min((fr_sc_k - fr_tst_k) ** 2, fr_sc_k ** 2) * (energy_k ** 2)\n",
    "        # Update denominator\n",
    "        denominator += (fr_sc_k ** 2) * (energy_k ** 2)\n",
    "\n",
    "    # Calculate score\n",
    "    sim_to_reco_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return sim_to_reco_score\n",
    "\n",
    "\n",
    "def calculate_reco_to_sim_score_and_sharedE(ReconstructedTrackster, energies_indices, CaloParticle, track_mult, calo_mult):\n",
    "    \"\"\"\n",
    "    Calculate the reco-to-sim score for a given ReconstructedTrackster and CaloParticle.\n",
    "\n",
    "    Parameters:\n",
    "    - ReconstructedTrackster: array of DetIds in the ReconstructedTrackster.\n",
    "    - energies_indices: array of energies associated with all DetIds (indexed by DetId).\n",
    "    - CaloParticle: array of DetIds in the CaloParticle.\n",
    "\n",
    "    Returns:\n",
    "    - reco_to_sim_score: the calculated reco-to-sim score.\n",
    "    \"\"\"\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    sharedEnergy = 0.0\n",
    "\n",
    "    # Iterate over all DetIds in the ReconstructedTrackster\n",
    "    for i, det_id in enumerate(ReconstructedTrackster):\n",
    "        energy_k = energies_indices[det_id]  # Energy for the current DetId in the Trackster\n",
    "        \n",
    "        # Fraction of energy in the Trackster (fr_k^TST)\n",
    "        fr_tst_k = 1 / track_mult[i]\n",
    "\n",
    "        #Fraction of energy in the caloparticle\n",
    "        if det_id in CaloParticle:\n",
    "            index = np.where(CaloParticle == det_id)[0][0]\n",
    "            fr_sc_k = 1 / calo_mult[index]\n",
    "            \n",
    "        else:\n",
    "            fr_sc_k = 0 # binary function also for CaloParticle\n",
    "            \n",
    "        # Update numerator using the min function\n",
    "        numerator += ((fr_tst_k - fr_sc_k) ** 2) * (energy_k ** 2)\n",
    "\n",
    "        # Update denominator\n",
    "        denominator += (fr_tst_k ** 2) * (energy_k ** 2)\n",
    "        \n",
    "        #shared_energy calculation\n",
    "        recosharedEnergy = energy_k * fr_tst_k\n",
    "        simsharedEnergy = energy_k * fr_sc_k\n",
    "        sharedEnergy += min(simsharedEnergy,recosharedEnergy)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Calculate score\n",
    "    reco_to_sim_score = numerator / denominator if denominator != 0 else 1.0\n",
    "    return reco_to_sim_score, sharedEnergy\n",
    "\n",
    "\n",
    "def calculate_all_event_scores(GT_ind, GT_mult, GT_regE, energies, recon_ind, recon_mult, num_events = 100):\n",
    "    \"\"\"\n",
    "    Calculate sim-to-reco and reco-to-sim scores for all CaloParticle and ReconstructedTrackster combinations across all events.\n",
    "\n",
    "    Parameters:\n",
    "    - GT_ind: List of CaloParticle indices for all events.\n",
    "    - energies: List of energy arrays for all events.\n",
    "    - recon_ind: List of ReconstructedTrackster indices for all events.\n",
    "    - LC_x, LC_y, LC_z, LC_eta: Lists of x, y, z positions and eta values for all DetIds across events.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing scores and additional features for each CaloParticle-Trackster combination across all events.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Loop over all events with a progress bar\n",
    "    for event_index in tqdm(range(num_events)):\n",
    "        caloparticles = GT_ind[event_index]  # Indices for all CaloParticles in the event\n",
    "        tracksters = recon_ind[event_index]  # Indices for all ReconstructedTracksters in the event\n",
    "        event_energies = energies[event_index]  # Energies for this event\n",
    "        event_GT_mult = GT_mult[event_index]\n",
    "        event_recon_mult = recon_mult[event_index]\n",
    "        event_GT_regE = GT_regE[event_index]\n",
    "\n",
    "        \n",
    "        # Loop over all CaloParticles\n",
    "        for calo_idx, caloparticle in enumerate(caloparticles):\n",
    "            calo_mult = event_GT_mult[calo_idx]\n",
    "            cp_raw_energy_lc = event_energies[caloparticle] / calo_mult\n",
    "            cp_raw_energy = np.sum(cp_raw_energy_lc)\n",
    "            cp_regressed_energy = event_GT_regE[calo_idx]\n",
    "            \n",
    "            for trackster_idx, trackster in enumerate(tracksters):\n",
    "                track_mult = event_recon_mult[trackster_idx]\n",
    "                \n",
    "                # Calculate sim-to-reco score\n",
    "                sim_to_reco_score = calculate_sim_to_reco_score(caloparticle, event_energies, trackster, track_mult, calo_mult)\n",
    "                \n",
    "                # Calculate reco-to-sim score\n",
    "                reco_to_sim_score, shared_energy = calculate_reco_to_sim_score_and_sharedE(trackster, event_energies, caloparticle, track_mult, calo_mult)\n",
    "                # Calculate trackster energy\n",
    "                trackster_energy_lc = event_energies[trackster] / track_mult\n",
    "                trackster_energy = np.sum(trackster_energy_lc)\n",
    "\n",
    "                # Append results\n",
    "                all_results.append({\n",
    "                    \"event_index\": event_index,\n",
    "                    \"cp_id\": calo_idx,\n",
    "                    \"trackster_id\": trackster_idx,\n",
    "                    \"reco_to_sim_score\": reco_to_sim_score,\n",
    "                    \"cp_raw_energy\": cp_raw_energy,\n",
    "                    \"cp_regressed_energy\": cp_regressed_energy,\n",
    "                    \"trackster_energy\": trackster_energy,\n",
    "                    \"shared_energy\": shared_energy\n",
    "                })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f34b330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Calculate Scores and create DF for our model and TICL\n",
    "\n",
    "#4.1: Turn the cluster labels into our reconstructed tracksters\n",
    "\n",
    "recon_ind = []\n",
    "\n",
    "for event_idx, labels in enumerate(all_cluster_labels):\n",
    "\n",
    "    event_clusters = {} \n",
    "    \n",
    "    for cluster_idx, cluster_label in enumerate(labels):\n",
    "        if cluster_label not in event_clusters:\n",
    "            event_clusters[cluster_label] = []\n",
    "        event_clusters[cluster_label].extend(Track_ind[event_idx][cluster_idx])\n",
    "    \n",
    "    recon_ind.append([event_clusters[label] for label in sorted(event_clusters.keys())])\n",
    "\n",
    "#4.2 Make DF from our model and CERN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "eedff87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_ind = []   # Hard assignment: each cluster gets a list of track indices.\n",
    "recon_mult = []  # Soft assignment: each cluster gets a list of multiplicity values (1/fraction).\n",
    "\n",
    "# Loop over each event.\n",
    "for event_idx, soft_assignments in enumerate(all_soft_assignments):\n",
    "    # Initialize dictionaries for this event.\n",
    "    # Keys are cluster indices; values are lists.\n",
    "    event_clusters_ind = {}\n",
    "    event_clusters_mult = {}\n",
    "    \n",
    "    # Loop over each trackster node (assume soft_assignments is [num_nodes, num_clusters]).\n",
    "    for node_idx, frac_vector in enumerate(soft_assignments):\n",
    "        # For each cluster in the soft assignment vector:\n",
    "        for cluster_j, fraction in enumerate(frac_vector):\n",
    "            # Only consider contributions above zero (assuming thresholding has been applied)\n",
    "            if fraction > 0.0:\n",
    "                # Initialize the cluster if needed.\n",
    "                if cluster_j not in event_clusters_ind:\n",
    "                    event_clusters_ind[cluster_j] = []\n",
    "                    event_clusters_mult[cluster_j] = []\n",
    "                # For each track index corresponding to this node, add the index and its multiplicity.\n",
    "                for track_idx in Track_ind[event_idx][node_idx]:\n",
    "                    event_clusters_ind[cluster_j].append(track_idx)\n",
    "                    event_clusters_mult[cluster_j].append(1.0 / fraction)\n",
    "    \n",
    "    # For consistency, sort clusters by their keys.\n",
    "    sorted_ind = [event_clusters_ind[k] for k in sorted(event_clusters_ind.keys())]\n",
    "    sorted_mult = [event_clusters_mult[k] for k in sorted(event_clusters_mult.keys())]\n",
    "    \n",
    "    recon_ind.append(sorted_ind)\n",
    "    recon_mult.append(sorted_mult)\n",
    "recon_ind = np.array(recon_ind)\n",
    "recon_mult = np.array(recon_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d321a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_mult = [\n",
    "    [ [1 for _ in cluster] for cluster in event ]\n",
    "    for event in MT_ind\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3337c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MT_mult = np.array(MT_mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ec4125c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [00:10<00:00,  4.67it/s]\n"
     ]
    }
   ],
   "source": [
    "df_CL = calculate_all_event_scores(GT_ind, GT_mult,reg_en, energies, recon_ind, recon_mult, num_events = 50)\n",
    "#df_TICL = calculate_all_event_scores(GT_ind, GT_mult,reg_en, energies, MT_ind, MT_mult, num_events = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "bb275291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CL['pgid'] = df_CL.apply(lambda row: pgid[int(row['event_index'])][int(row['cp_id'])], axis=1)\n",
    "df_TICL['pgid'] = df_TICL.apply(lambda row: pgid[int(row['event_index'])][int(row['cp_id'])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7e5d0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Our Model\n",
      "Efficiency:  0.7518 (106 / 141)\n",
      "Purity:      0.9270 (127 / 137)\n",
      "Num tracksters ratio: 0.9716\n",
      "\n",
      "Model: CERN Model\n",
      "Efficiency:  0.7724 (112 / 145)\n",
      "Purity:      0.9733 (255 / 262)\n",
      "Num tracksters ratio: 1.8069\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(df, model_name):\n",
    "    # Filter to only include rows with pgid == 211.\n",
    "    #df = df[df['pgid'] == 211].copy()\n",
    "    \n",
    "    # ----- Efficiency Calculation -----\n",
    "    # Step 1: Filter out rows where 'cp_id' is NaN\n",
    "    cp_valid = df.dropna(subset=['cp_id']).copy()\n",
    "\n",
    "    # Step 2: Group by 'event_index' and 'cp_id' to process each CaloParticle individually\n",
    "    cp_grouped = cp_valid.groupby(['event_index', 'cp_id'])\n",
    "\n",
    "    # Step 3: For each CaloParticle, check if any 'shared_energy' >= 50% of 'cp_energy'\n",
    "    def is_cp_associated(group):\n",
    "        cp_energy = group['cp_raw_energy'].iloc[0]  # Assuming 'cp_energy' is consistent within the group\n",
    "        threshold = 0.5 * cp_energy\n",
    "        return (group['shared_energy'] >= threshold).any()\n",
    "\n",
    "    # Apply the association function to each group\n",
    "    cp_associated = cp_grouped.apply(is_cp_associated)\n",
    "\n",
    "    # Step 4: Calculate the number of associated CaloParticles and total CaloParticles\n",
    "    num_associated_cp = cp_associated.sum()\n",
    "    total_cp = cp_associated.count()\n",
    "    efficiency = num_associated_cp / total_cp if total_cp > 0 else 0\n",
    "\n",
    "    # ----- Purity Calculation -----\n",
    "    tst_valid = df.dropna(subset=['trackster_id']).copy()\n",
    "    tst_grouped = tst_valid.groupby(['event_index', 'trackster_id'])\n",
    "\n",
    "    # A Trackster is \"associated\" if reco_to_sim_score < 0.2 for at least one CP\n",
    "    tst_associated = tst_grouped['reco_to_sim_score'].min() < 0.2\n",
    "    num_associated_tst = tst_associated.sum()\n",
    "    total_tst = tst_associated.count()\n",
    "    purity = num_associated_tst / total_tst if total_tst > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "    # ----- Merge Rate -----\n",
    "    # Number of reconstructed objects (Tracksters) that have been associated (Reco-to-Sim < 0.2)\n",
    "    # to multiple simulated objects, divided by the total number of reconstructed objects.\n",
    "    tst_grouped_merge = tst_valid.groupby(['event_index', 'trackster_id'])\n",
    "\n",
    "    def get_associated_cps_tst(group):\n",
    "        # Return the unique CP IDs for which reco_to_sim_score < 0.2\n",
    "        return group[group['reco_to_sim_score'] < 0.2]['cp_id'].unique()\n",
    "\n",
    "    tst_associated_cps = tst_grouped_merge.apply(get_associated_cps_tst)\n",
    "\n",
    "    # A Trackster is \"merged\" if it's associated with more than one CP\n",
    "    merge_count = (tst_associated_cps.apply(lambda x: len(x) > 1)).sum()\n",
    "    total_tst_merge = len(tst_associated_cps)\n",
    "    merge_rate = merge_count / total_tst_merge if total_tst_merge > 0 else 0\n",
    "\n",
    "    # Print results for the model\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Efficiency:  {efficiency:.4f} ({num_associated_cp} / {total_cp})\")\n",
    "    print(f\"Purity:      {purity:.4f} ({num_associated_tst} / {total_tst})\")\n",
    "\n",
    "    print(f\"Num tracksters ratio: {total_tst / total_cp if total_cp > 0 else 0:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'efficiency': efficiency,\n",
    "        'purity': purity\n",
    "\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "our_model_metrics = calculate_metrics(df_CL, \"Our Model\")\n",
    "cern_model_metrics = calculate_metrics(df_TICL, \"CERN Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1cf9015d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pgid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pgid'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_201341/3336230995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Filter to only include rows with a pgid of 211 or -211.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf_CL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_CL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_CL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pgid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdf_TICL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_TICL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_TICL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pgid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pgid'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Load Your DataFrames\n",
    "# ---------------------------\n",
    "# Replace the following with your actual data loading mechanism.\n",
    "# For example:\n",
    "# df_CL = pd.read_csv('df_CL.csv')\n",
    "# df_TICL = pd.read_csv('df_TICL.csv')\n",
    "# (If you have df_CL_pion, you can ignore its processing for now.)\n",
    "\n",
    "# ---------------------------\n",
    "# Preprocessing for df_CL and df_TICL\n",
    "# ---------------------------\n",
    "for df in [df_CL, df_TICL]:\n",
    "    df['cp_id'] = pd.to_numeric(df['cp_id'], errors='coerce')\n",
    "    df['shared_energy'] = pd.to_numeric(df['shared_energy'], errors='coerce')\n",
    "    df['cp_regressed_energy'] = pd.to_numeric(df['cp_regressed_energy'], errors='coerce')\n",
    "\n",
    "# Filter to only include rows with a pgid of 211 or -211.\n",
    "df_CL2 = df_CL[df_CL['pgid'].isin([211, -211])]\n",
    "df_TICL2 = df_TICL[df_TICL['pgid'].isin([211, -211])]\n",
    "\n",
    "# ---------------------------\n",
    "# (Optional) Remove df_CL_pion processing if not needed\n",
    "# ---------------------------\n",
    "# df_CL_pion['cp_id'] = pd.to_numeric(df_CL_pion['cp_id'], errors='coerce')\n",
    "# df_CL_pion['shared_energy'] = pd.to_numeric(df_CL_pion['shared_energy'], errors='coerce')\n",
    "# df_CL_pion['cp_energy'] = pd.to_numeric(df_CL_pion['cp_energy'], errors='coerce')\n",
    "# df_CL_pion2 = df_CL_pion[df_CL_pion['pgid'].isin([211, -211])]\n",
    "\n",
    "# ---------------------------\n",
    "# Define a Function to Prepare CaloParticle Data\n",
    "# ---------------------------\n",
    "def prepare_cp_data(df):\n",
    "    \"\"\"\n",
    "    Group the DataFrame by ['event_index', 'cp_id'] so that each caloparticle is counted once.\n",
    "    For each group:\n",
    "      - Take the first cp_energy.\n",
    "      - Take the maximum shared_energy to check if any shared_energy >= 50% of cp_energy.\n",
    "      - Mark the caloparticle as 'reconstructed' if any shared_energy >= 50% of cp_energy.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(['event_index', 'cp_id']).agg({\n",
    "        'cp_regressed_energy': 'first',\n",
    "        'shared_energy': 'max'\n",
    "    }).reset_index()\n",
    "    grouped['reco'] = (grouped['shared_energy'] >= 0.5 * grouped['cp_regressed_energy']).astype(int)\n",
    "    return grouped\n",
    "\n",
    "# Prepare the caloparticle data for each DataFrame.\n",
    "df_CL_cp = prepare_cp_data(df_CL2)\n",
    "df_TICL_cp = prepare_cp_data(df_TICL2)\n",
    "# (No processing for df_CL_pion since we are adding manual data.)\n",
    "\n",
    "# ---------------------------\n",
    "# Define Energy Bins\n",
    "# ---------------------------\n",
    "# Use the range of cp_energy from df_CL_cp to define bins.\n",
    "min_energy = df_CL_cp['cp_regressed_energy'].min()\n",
    "max_energy = 200\n",
    "n_bins = 10\n",
    "energy_bins = np.linspace(min_energy, max_energy, n_bins + 1)\n",
    "\n",
    "# Assign each caloparticle to an energy bin.\n",
    "df_CL_cp['energy_bin'] = pd.cut(df_CL_cp['cp_regressed_energy'], bins=energy_bins, labels=False, include_lowest=True)\n",
    "df_TICL_cp['energy_bin'] = pd.cut(df_TICL_cp['cp_regressed_energy'], bins=energy_bins, labels=False, include_lowest=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregate Efficiency Data\n",
    "# ---------------------------\n",
    "def aggregate_efficiency(df):\n",
    "    \"\"\"\n",
    "    For each energy bin, calculate:\n",
    "      - The total number of caloparticles in the bin.\n",
    "      - The number of reconstructed caloparticles.\n",
    "      - Efficiency = (reconstructed caloparticles) / (total number).\n",
    "      - An error estimate based on binomial statistics.\n",
    "    \"\"\"\n",
    "    agg = df.groupby('energy_bin').agg(\n",
    "        total_cp=('cp_regressed_energy', 'count'),\n",
    "        reco_cp=('reco', 'sum')\n",
    "    ).reset_index()\n",
    "    agg['efficiency'] = agg['reco_cp'] / agg['total_cp']\n",
    "    agg['eff_error'] = np.sqrt(agg['efficiency'] * (1 - agg['efficiency']) / agg['total_cp'])\n",
    "    return agg\n",
    "\n",
    "agg_CL = aggregate_efficiency(df_CL_cp)\n",
    "agg_TICL = aggregate_efficiency(df_TICL_cp)\n",
    "# (No aggregation for df_CL_pion is performed.)\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Efficiency vs Energy with Histogram and Ratio Plot\n",
    "# ---------------------------\n",
    "# Compute bin centers for plotting: average of adjacent bin edges.\n",
    "bin_centers = (energy_bins[:-1] + energy_bins[1:]) / 2\n",
    "bar_width = energy_bins[1] - energy_bins[0]\n",
    "\n",
    "# Calculate efficiency ratio (Our Model / TICL) and its error.\n",
    "eff_ratio = agg_CL['efficiency'] / agg_TICL['efficiency']\n",
    "eff_ratio_error = eff_ratio * np.sqrt(\n",
    "    (agg_CL['eff_error'] / agg_CL['efficiency'])**2 + \n",
    "    (agg_TICL['eff_error'] / agg_TICL['efficiency'])**2\n",
    ")\n",
    "\n",
    "# Create a figure with two subplots: one for efficiency curves and histogram, and one for the ratio.\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
    "\n",
    "# ----- Efficiency Plot -----\n",
    "ax1.errorbar(bin_centers, agg_CL['efficiency'], yerr=agg_CL['eff_error'],\n",
    "             marker='o', linestyle='-', color='blue', label='Our Model - Mixed')\n",
    "ax1.errorbar(bin_centers, agg_TICL['efficiency'], yerr=agg_TICL['eff_error'],\n",
    "             marker='o', linestyle='-', color='green', label='TICL')\n",
    "\n",
    "# Add a new manual curve.\n",
    "# Replace these arrays with your manual data for each energy bin.\n",
    "manual_efficiency = np.array([0.38, 0.59, 0.77, 0.86, 0.9, 0.94, 0.95, 0.96, 0.95, 0.945])\n",
    "manual_eff_error = np.array([0.0] * len(manual_efficiency))\n",
    "ax1.errorbar(bin_centers, manual_efficiency, yerr=manual_eff_error,\n",
    "             marker='x', fmt='x', linestyle='--', color='black', label='Our Model - Pion')\n",
    "\n",
    "ax1.set_ylabel('Efficiency', fontsize=14)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.legend(loc='lower right', fontsize=14)\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a secondary y-axis to show the number of CaloParticles as a histogram.\n",
    "ax1_hist = ax1.twinx()\n",
    "ax1_hist.bar(bin_centers, agg_CL['total_cp'], width=bar_width, color='lightblue', alpha=0.4)\n",
    "ax1_hist.set_ylabel('Number of CaloParticles', fontsize=14)\n",
    "ax1_hist.set_ylim(0, agg_CL['total_cp'].max() * 1.2)\n",
    "\n",
    "# ----- Ratio Plot -----\n",
    "ax2.errorbar(bin_centers, eff_ratio, yerr=eff_ratio_error, fmt='x', color='blue', markersize=10)\n",
    "ax2.axhline(1.0, color='black', linestyle='-', linewidth=1)\n",
    "ax2.set_xlabel('CaloParticle Energy [GeV]', fontsize=14)\n",
    "ax2.set_ylabel('Ratio', fontsize=12)\n",
    "ax2.set_ylim(0.0, 2.00)\n",
    "\n",
    "# Update Title to Reflect the Efficiency Study\n",
    "plt.suptitle(r'Efficiency vs CaloParticle Energy: Pion', fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf7828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
