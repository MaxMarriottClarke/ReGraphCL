{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7790a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0: imports\n",
    "\n",
    "import uproot \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import DataLoader \n",
    "from imports.models import Net_DE, Net_GAT, Net_Trans\n",
    "from imports.model import Net\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import time\n",
    "from imports.Agglomerative import Aggloremative\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cfe991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import uproot\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import awkward as ak\n",
    "import random\n",
    "\n",
    "def find_highest_branch(path, base_name):\n",
    "    with uproot.open(path) as f:\n",
    "        # Find keys that exactly match the base_name (not containing other variations)\n",
    "        branches = [k for k in f.keys() if k.startswith(base_name + ';')]\n",
    "        \n",
    "        # Sort and select the highest-numbered branch\n",
    "        sorted_branches = sorted(branches, key=lambda x: int(x.split(';')[-1]))\n",
    "        return sorted_branches[-1] if sorted_branches else None\n",
    "    \n",
    "    \n",
    "def remove_duplicates(A,B):    \n",
    "    all_masks = []\n",
    "    for event_idx, event in enumerate(A):\n",
    "        flat_A = np.array(ak.flatten(A[event_idx]))\n",
    "        flat_B = np.array(ak.flatten(B[event_idx]))\n",
    "        \n",
    "        # Initialize a mask to keep track of which values to keep\n",
    "        mask = np.zeros_like(flat_A, dtype=bool)\n",
    "\n",
    "        # Iterate over the unique elements in A\n",
    "        for elem in np.unique(flat_A):\n",
    "            # Get the indices where the element occurs in A\n",
    "            indices = np.where(flat_A == elem)[0]\n",
    "\n",
    "            # If there's more than one occurrence, keep the one with the max B value\n",
    "            if len(indices) > 1:\n",
    "                max_index = indices[np.argmax(flat_B[indices])]\n",
    "                mask[max_index] = True\n",
    "            else:\n",
    "                # If there's only one occurrence, keep it\n",
    "                mask[indices[0]] = True\n",
    "\n",
    "        unflattened_mask = ak.unflatten(mask, ak.num(A[event_idx]))\n",
    "        all_masks.append(unflattened_mask)\n",
    "        \n",
    "    return ak.Array(all_masks)\n",
    "\n",
    "class CCV3(Dataset):\n",
    "    r'''\n",
    "        input: layer clusters\n",
    "        Loads the data stored in the 'simtrackstersCP (test)' and 'clusters (test)' tree of the ticlNtuplizer.\n",
    "    '''\n",
    "\n",
    "    url = '/dummy/'\n",
    "\n",
    "    def __init__(self, root, transform=None, max_events=1e8, n_particles=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          root (str): Root directory containing the .root files.\n",
    "          transform: Optional transform.\n",
    "          max_events (int): Maximum events to load.\n",
    "          n_particles (int or None): If provided, only events with exactly this number of particles are kept.\n",
    "                                     Otherwise, events with at least 1 particle are loaded.\n",
    "        \"\"\"\n",
    "        super(CCV3, self).__init__(root, transform)\n",
    "        self.step_size = 100\n",
    "        self.max_events = max_events\n",
    "        self.n_particles = n_particles\n",
    "        self.fill_data(max_events)\n",
    "\n",
    "    def fill_data(self, max_events):\n",
    "        counter = 0\n",
    "\n",
    "        print(\"### Loading data\")\n",
    "        for fi, path in enumerate(tqdm.tqdm(self.raw_paths)):  ## only one root file so is there a need for a for loop?\n",
    "\n",
    "            #if fi > 2:\n",
    "                #break\n",
    "            cluster_path = find_highest_branch(path, 'clusters')\n",
    "            sim_path = find_highest_branch(path, 'simtrackstersCP')\n",
    "            trackster_path = find_highest_branch(path, 'tracksters')\n",
    "\n",
    "            crosstree =  uproot.open(path)[cluster_path]\n",
    "            tracktree = uproot.open(path)[trackster_path]\n",
    "            crosscounter = 0\n",
    "            for array in uproot.iterate(f\"{path}:{sim_path}\", [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n",
    "            \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \"regressed_energy\"], step_size=self.step_size):\n",
    "\n",
    "                tmp_stsCP_vertices_x = array['vertices_x']\n",
    "                tmp_stsCP_vertices_y = array['vertices_y']\n",
    "                tmp_stsCP_vertices_z = array['vertices_z']\n",
    "                tmp_stsCP_vertices_energy = array['vertices_energy']\n",
    "                tmp_stsCP_vertices_time = array['vertices_time']\n",
    "                tmp_stsCP_vertices_indexes = array['vertices_indexes']\n",
    "                tmp_stsCP_barycenter_x = array['barycenter_x']\n",
    "                tmp_stsCP_barycenter_y = array['barycenter_y']\n",
    "                tmp_stsCP_barycenter_z = array['barycenter_z']\n",
    "                tmp_stsCP_vertices_multiplicity = array['vertices_multiplicity']\n",
    "                tmp_stsCP_regressed_energy = array['regressed_energy']\n",
    "\n",
    "                self.step_size = min(self.step_size, len(tmp_stsCP_vertices_x))\n",
    "\n",
    "\n",
    "                # Code block for reading from 'clusters' tree\n",
    "                tmp_all_vertices_layer_id = crosstree['cluster_layer_id'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                #tmp_all_vertices_radius = crosstree['cluster_radius'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_noh = crosstree['cluster_number_of_hits'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_eta = crosstree['position_eta'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_phi = crosstree['position_phi'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_energy = crosstree['energy'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_x = crosstree['position_x'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_y = crosstree['position_y'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_all_vertices_z = crosstree['position_z'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)                \n",
    "\n",
    "                layer_id_list = []\n",
    "                radius_list = []\n",
    "                noh_list = []\n",
    "                eta_list = []\n",
    "                phi_list = []\n",
    "                for evt_row in range(len(tmp_all_vertices_noh)):\n",
    "                    #print(\"Event no: %i\"%evt_row)\n",
    "                    #print(\"There are %i particles in this event\"%len(tmp_stsCP_vertices_indexes[evt_row]))\n",
    "                    layer_id_list_one_event = []\n",
    "                    #radius_list_one_event = []\n",
    "                    noh_list_one_event = []\n",
    "                    eta_list_one_event = []\n",
    "                    phi_list_one_event = []\n",
    "                    for particle in range(len(tmp_stsCP_vertices_indexes[evt_row])):\n",
    "                        #print(\"Particle no: %i\"%particle)\n",
    "                        #print(\"A\")\n",
    "                        #print(np.array(tmp_all_vertices_radius[evt_row]).shape)\n",
    "                        #print(\"B\")\n",
    "                        #print(np.array(tmp_stsCP_vertices_indexes[evt_row][particle]).shape)\n",
    "                        #print(\"C\")\n",
    "                        tmp_stsCP_vertices_layer_id_one_particle = tmp_all_vertices_layer_id[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #tmp_stsCP_vertices_radius_one_particle = tmp_all_vertices_radius[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_noh_one_particle = tmp_all_vertices_noh[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_eta_one_particle = tmp_all_vertices_eta[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        tmp_stsCP_vertices_phi_one_particle = tmp_all_vertices_phi[evt_row][tmp_stsCP_vertices_indexes[evt_row][particle]]\n",
    "                        #print(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        layer_id_list_one_event.append(tmp_stsCP_vertices_layer_id_one_particle)\n",
    "                        #radius_list_one_event.append(tmp_stsCP_vertices_radius_one_particle)\n",
    "                        noh_list_one_event.append(tmp_stsCP_vertices_noh_one_particle)\n",
    "                        eta_list_one_event.append(tmp_stsCP_vertices_eta_one_particle)\n",
    "                        phi_list_one_event.append(tmp_stsCP_vertices_phi_one_particle)\n",
    "                    layer_id_list.append(layer_id_list_one_event)\n",
    "                   # radius_list.append(radius_list_one_event)\n",
    "                    noh_list.append(noh_list_one_event)\n",
    "                    eta_list.append(eta_list_one_event)\n",
    "                    phi_list.append(phi_list_one_event)\n",
    "                tmp_stsCP_vertices_layer_id = ak.Array(layer_id_list)                \n",
    "                #tmp_stsCP_vertices_radius = ak.Array(radius_list)                \n",
    "                tmp_stsCP_vertices_noh = ak.Array(noh_list)                \n",
    "                tmp_stsCP_vertices_eta = ak.Array(eta_list)                \n",
    "                tmp_stsCP_vertices_phi = ak.Array(phi_list)\n",
    "\n",
    "                \"\"\"\n",
    "                energyPercent = 1/tmp_stsCP_vertices_multiplicity\n",
    "                skim_mask_energyPercent = energyPercent > 0.5\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_energyPercent]\n",
    "                \"\"\"\n",
    "\n",
    "                # Apply filter noh > 1 for the LCs\n",
    "                skim_mask_noh = tmp_stsCP_vertices_noh > 1\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_noh]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_noh]\n",
    "                #tmp_stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask_noh]\n",
    "                tmp_stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity[skim_mask_noh] #<---\n",
    "                \n",
    "                # Remove duplicates by only allowing the caloparticle that contributed the most energy to a LC to actually contribute.\n",
    "                # This is so we can define a ground truth\n",
    "                \n",
    "                energyPercent = 1/tmp_stsCP_vertices_multiplicity\n",
    "                skim_mask_energyPercent = remove_duplicates(tmp_stsCP_vertices_indexes, energyPercent)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask_energyPercent]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask_energyPercent]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask_energyPercent]\n",
    "                #tmp_stsCP_vertices_indexes_unmasked = tmp_stsCP_vertices_indexes]\n",
    "\n",
    "                \n",
    "                # Code block for reading from 'tracksters' tree\n",
    "                tmp_ts_vertices_indexes = tracktree['vertices_indexes'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_ts_vertices_energy = tracktree['vertices_energy'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                tmp_ts_vertices_multiplicity = tracktree['vertices_multiplicity'].array(entry_start=crosscounter*self.step_size, entry_stop=(crosscounter+1)*self.step_size)\n",
    "                crosscounter += 1\n",
    "\n",
    "\n",
    "                # === MODIFIED FILTER SECTION ===\n",
    "                # Instead of filtering events with at least 1 particle, we now allow for specifying an exact number.\n",
    "                skim_mask = []\n",
    "                for e in tmp_stsCP_vertices_x:\n",
    "                    if self.n_particles is not None:\n",
    "                        if len(e) == self.n_particles:\n",
    "                            skim_mask.append(True)\n",
    "                        else:\n",
    "                            skim_mask.append(False)\n",
    "                    else:\n",
    "                        if 1 <= len(e):\n",
    "                            skim_mask.append(True)\n",
    "                        else:\n",
    "                            skim_mask.append(False)\n",
    "                tmp_stsCP_vertices_x = tmp_stsCP_vertices_x[skim_mask]\n",
    "                tmp_stsCP_vertices_y = tmp_stsCP_vertices_y[skim_mask]\n",
    "                tmp_stsCP_vertices_z = tmp_stsCP_vertices_z[skim_mask]\n",
    "                tmp_stsCP_vertices_energy = tmp_stsCP_vertices_energy[skim_mask]\n",
    "                tmp_stsCP_vertices_time = tmp_stsCP_vertices_time[skim_mask]\n",
    "                tmp_stsCP_vertices_indexes = tmp_stsCP_vertices_indexes[skim_mask]  ## additional filtering\n",
    "                tmp_stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id[skim_mask]\n",
    "                #tmp_stsCP_vertices_radius = tmp_stsCP_vertices_radius[skim_mask]\n",
    "                tmp_stsCP_vertices_noh = tmp_stsCP_vertices_noh[skim_mask]\n",
    "                tmp_stsCP_vertices_eta = tmp_stsCP_vertices_eta[skim_mask]\n",
    "                tmp_stsCP_vertices_phi = tmp_stsCP_vertices_phi[skim_mask]\n",
    "                tmp_stsCP_barycenter_x = tmp_stsCP_barycenter_x[skim_mask]  # <--- additional filtering\n",
    "                tmp_stsCP_barycenter_y = tmp_stsCP_barycenter_y[skim_mask]  # <---- additional filtering\n",
    "                tmp_stsCP_barycenter_z = tmp_stsCP_barycenter_z[skim_mask]  # <----- additional filtering\n",
    "                tmp_ts_vertices_indexes = tmp_ts_vertices_indexes[skim_mask]\n",
    "                tmp_ts_vertices_energy = tmp_ts_vertices_energy[skim_mask]\n",
    "                tmp_all_vertices_energy = tmp_all_vertices_energy[skim_mask]\n",
    "                # =================================\n",
    "\n",
    "                if counter == 0:\n",
    "                    self.stsCP_vertices_x = tmp_stsCP_vertices_x\n",
    "                    self.stsCP_vertices_y = tmp_stsCP_vertices_y\n",
    "                    self.stsCP_vertices_z = tmp_stsCP_vertices_z\n",
    "                    self.stsCP_vertices_energy = tmp_stsCP_vertices_energy\n",
    "                    self.stsCP_vertices_time = tmp_stsCP_vertices_time\n",
    "                    self.stsCP_vertices_layer_id = tmp_stsCP_vertices_layer_id\n",
    "                    #self.stsCP_vertices_radius = tmp_stsCP_vertices_radius\n",
    "                    self.stsCP_vertices_noh = tmp_stsCP_vertices_noh\n",
    "                    self.stsCP_vertices_eta = tmp_stsCP_vertices_eta\n",
    "                    self.stsCP_vertices_phi = tmp_stsCP_vertices_phi\n",
    "                    self.stsCP_vertices_indexes = tmp_stsCP_vertices_indexes\n",
    "                    self.stsCP_barycenter_x = tmp_stsCP_barycenter_x\n",
    "                    self.stsCP_barycenter_y = tmp_stsCP_barycenter_y\n",
    "                    self.stsCP_barycenter_z = tmp_stsCP_barycenter_z\n",
    "                    self.stsCP_vertices_multiplicity = tmp_stsCP_vertices_multiplicity\n",
    "                    self.stsCP_regressed_energy = tmp_stsCP_regressed_energy\n",
    "                    self.ts_vertices_indexes = tmp_ts_vertices_indexes\n",
    "                    self.ts_vertices_energy = tmp_ts_vertices_energy\n",
    "                    self.ts_vertices_multiplicity = tmp_ts_vertices_multiplicity\n",
    "                    self.all_vertices_energy = tmp_all_vertices_energy\n",
    "                    self.all_vertices_layer_id = tmp_all_vertices_layer_id\n",
    "                    self.all_vertices_x = tmp_all_vertices_x\n",
    "                    self.all_vertices_y = tmp_all_vertices_y\n",
    "                    self.all_vertices_z = tmp_all_vertices_z\n",
    "                else:\n",
    "                    self.stsCP_vertices_x = ak.concatenate((self.stsCP_vertices_x, tmp_stsCP_vertices_x))\n",
    "                    self.stsCP_vertices_y = ak.concatenate((self.stsCP_vertices_y, tmp_stsCP_vertices_y))\n",
    "                    self.stsCP_vertices_z = ak.concatenate((self.stsCP_vertices_z, tmp_stsCP_vertices_z))\n",
    "                    self.stsCP_vertices_energy = ak.concatenate((self.stsCP_vertices_energy, tmp_stsCP_vertices_energy))\n",
    "                    self.stsCP_vertices_time = ak.concatenate((self.stsCP_vertices_time, tmp_stsCP_vertices_time))\n",
    "                    self.stsCP_vertices_layer_id = ak.concatenate((self.stsCP_vertices_layer_id, tmp_stsCP_vertices_layer_id))\n",
    "                    #self.stsCP_vertices_radius = ak.concatenate((self.stsCP_vertices_radius, tmp_stsCP_vertices_radius))\n",
    "                    self.stsCP_vertices_noh = ak.concatenate((self.stsCP_vertices_noh, tmp_stsCP_vertices_noh))\n",
    "                    self.stsCP_vertices_eta = ak.concatenate((self.stsCP_vertices_eta, tmp_stsCP_vertices_eta))\n",
    "                    self.stsCP_vertices_phi = ak.concatenate((self.stsCP_vertices_phi, tmp_stsCP_vertices_phi))\n",
    "                    self.stsCP_vertices_indexes = ak.concatenate((self.stsCP_vertices_indexes, tmp_stsCP_vertices_indexes))\n",
    "                    self.stsCP_barycenter_x = ak.concatenate((self.stsCP_barycenter_x, tmp_stsCP_barycenter_x))\n",
    "                    self.stsCP_barycenter_y = ak.concatenate((self.stsCP_barycenter_y, tmp_stsCP_barycenter_y))\n",
    "                    self.stsCP_barycenter_z = ak.concatenate((self.stsCP_barycenter_z, tmp_stsCP_barycenter_z))\n",
    "                    self.stsCP_vertices_multiplicity = ak.concatenate((self.stsCP_vertices_multiplicity, tmp_stsCP_vertices_multiplicity))\n",
    "                    self.stsCP_regressed_energy = ak.concatenate((self.stsCP_regressed_energy, tmp_stsCP_regressed_energy))\n",
    "                    self.ts_vertices_indexes = ak.concatenate((self.ts_vertices_indexes, tmp_ts_vertices_indexes))\n",
    "                    self.ts_vertices_energy = ak.concatenate((self.ts_vertices_energy, tmp_ts_vertices_energy))\n",
    "                    self.ts_vertices_multiplicity = ak.concatenate((self.ts_vertices_multiplicity, tmp_ts_vertices_multiplicity))\n",
    "                    self.all_vertices_energy = ak.concatenate((self.all_vertices_energy, tmp_all_vertices_energy))\n",
    "                    self.all_vertices_layer_id = ak.concatenate((self.all_vertices_layer_id, tmp_all_vertices_layer_id))\n",
    "                    self.all_vertices_x = ak.concatenate((self.all_vertices_x, tmp_all_vertices_x))\n",
    "                    self.all_vertices_y = ak.concatenate((self.all_vertices_y, tmp_all_vertices_y))\n",
    "                    self.all_vertices_z = ak.concatenate((self.all_vertices_z, tmp_all_vertices_z))\n",
    "\n",
    "                #print(len(self.stsCP_vertices_x))\n",
    "                counter += 1\n",
    "                if len(self.stsCP_vertices_x) > max_events:\n",
    "                    print(f\"Reached {max_events}!\")\n",
    "                    break\n",
    "            if len(self.stsCP_vertices_x) > max_events:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        raise RuntimeError(\n",
    "            'Dataset not found. Please download it from {} and move all '\n",
    "            '*.z files to {}'.format(self.url, self.raw_dir))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.stsCP_vertices_x)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_files = sorted(glob.glob(osp.join(self.raw_dir, '*.root')))\n",
    "\n",
    "        #raw_files = [osp.join(self.raw_dir, 'step3_NTUPLE.root')]\n",
    "\n",
    "        return raw_files\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        # flatten so that we don't know what caloparticle the LC came from\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        \n",
    "        lc_id = self.stsCP_vertices_indexes[idx]\n",
    "        flat_lc_id = np.expand_dims(np.array(ak.flatten(lc_id)), axis=1)\n",
    "        lc_x = self.stsCP_vertices_x[idx]\n",
    "        flat_lc_x = np.expand_dims(np.array(ak.flatten(lc_x)), axis=1)\n",
    "        lc_y = self.stsCP_vertices_y[idx]\n",
    "        flat_lc_y = np.expand_dims(np.array(ak.flatten(lc_y)), axis=1)\n",
    "        lc_z = self.stsCP_vertices_z[idx]\n",
    "        flat_lc_z = np.expand_dims(np.array(ak.flatten(lc_z)), axis=1)\n",
    "        lc_e = self.stsCP_vertices_energy[idx]\n",
    "        flat_lc_e = np.expand_dims(np.array(ak.flatten(lc_e)), axis=1)     \n",
    "        lc_t = self.stsCP_vertices_time[idx]\n",
    "        flat_lc_t = np.expand_dims(np.array(ak.flatten(lc_t)), axis=1)  \n",
    "        lc_layer_id = self.stsCP_vertices_layer_id[idx]\n",
    "        flat_lc_layer_id = np.expand_dims(np.array(ak.flatten(lc_layer_id)), axis=1)  \n",
    "        lc_noh = self.stsCP_vertices_noh[idx]\n",
    "        flat_lc_noh = np.expand_dims(np.array(ak.flatten(lc_noh)), axis=1)  \n",
    "        lc_eta = self.stsCP_vertices_eta[idx]\n",
    "        flat_lc_eta = np.expand_dims(np.array(ak.flatten(lc_eta)), axis=1)  \n",
    "        lc_phi = self.stsCP_vertices_phi[idx]\n",
    "        flat_lc_phi = np.expand_dims(np.array(ak.flatten(lc_phi)), axis=1)\n",
    "        \n",
    "        flat_lc_feats = np.concatenate((flat_lc_x, flat_lc_y, flat_lc_z, flat_lc_e,\n",
    "                                        flat_lc_layer_id, flat_lc_noh, flat_lc_eta, flat_lc_phi), axis=-1)        \n",
    "        \n",
    "        result = np.concatenate([np.full(len(subarr), i) for i, subarr in enumerate(lc_x)])\n",
    "        result_list = result.tolist() \n",
    "   \n",
    "        x = torch.from_numpy(flat_lc_feats).float()\n",
    "        x_counts = lc_x\n",
    "        return Data(x=x, x_counts=x_counts, idx=flat_lc_id, assoc=result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e6d5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_DE(\n",
       "  (lc_encode): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ELU(alpha=1.0)\n",
       "  )\n",
       "  (edgeconv_layers): ModuleList(\n",
       "    (0-2): 3 x DynamicEdgeConv(nn=Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    ), k=64)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imports.model import Net\n",
    "model = Net_DE(128,3, dropout=0.3, contrastive_dim=16, k=64)\n",
    "#checkpoint= torch.load('/vols/cms/mm1221/hgcal/Mixed/LC/Fraction/runs/DEC/hd128nl3cd128k72/best_model.pt',  map_location=torch.device('cpu'))\n",
    "checkpoint= torch.load('/vols/cms/er421/hgcal/code/code/Mixed/LC/Full/results/hd128nl3cd16k64/epoch-100.pt',  map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model'])  \n",
    "model.eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpath = '/vols/cms/mm1221/Data/mix/test/'\n",
    "# Load test data\n",
    "data_test = CCV3(testpath, max_events=500)\n",
    "test_loader = DataLoader(data_test, batch_size=1, shuffle=False, follow_batch=['x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c956eff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with n_particles = 1\n",
      "  Run 1/3 ...\n",
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                        | 0/1 [07:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached 500!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 2/3 ...\n",
      "### Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                        | 0/1 [05:38<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1920366/3501349325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Run {run+1}/{num_runs} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Load test data with exactly cp calo particles per event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1920366/1855396627.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, max_events, n_particles)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_particles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1920366/1855396627.py\u001b[0m in \u001b[0;36mfill_data\u001b[0;34m(self, max_events)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mtracktree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrackster_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcrosscounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             for array in uproot.iterate(f\"{path}:{sim_path}\", [\"vertices_x\", \"vertices_y\", \"vertices_z\", \n\u001b[0m\u001b[1;32m    100\u001b[0m             \"vertices_energy\", \"vertices_multiplicity\", \"vertices_time\", \"vertices_indexes\", \"barycenter_x\", \"barycenter_y\", \"barycenter_z\", \"regressed_energy\"], step_size=self.step_size):\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(files, expressions, cut, filter_name, filter_typename, filter_branch, aliases, language, step_size, decompression_executor, interpretation_executor, library, how, report, custom_classes, allow_missing, **options)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mhasbranches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                     for item in hasbranches.iterate(\n\u001b[0m\u001b[1;32m    191\u001b[0m                         \u001b[0mexpressions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpressions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                         \u001b[0mcut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self, expressions, cut, filter_name, filter_typename, filter_branch, aliases, language, entry_start, entry_stop, step_size, decompression_executor, interpretation_executor, library, how, report)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m                 \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m                 _ranges_or_baskets_to_arrays(\n\u001b[0m\u001b[1;32m   1489\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m                     \u001b[0mranges_or_baskets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36m_ranges_or_baskets_to_arrays\u001b[0;34m(hasbranches, ranges_or_baskets, branchid_interpretation, entry_start, entry_stop, decompression_executor, interpretation_executor, library, arrays, update_ranges_or_baskets)\u001b[0m\n\u001b[1;32m   3485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3486\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muproot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTBasket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel_TBasket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3487\u001b[0;31m             \u001b[0minterpretation_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_to_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/source/futures.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, task, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mImmediately\u001b[0m \u001b[0mruns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrivialFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/behaviors/TBranch.py\u001b[0m in \u001b[0;36mbasket_to_array\u001b[0;34m(basket)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasket_arrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbranchid_num_baskets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m                 arrays[branch.cache_key] = interpretation.final_array(\n\u001b[0m\u001b[1;32m   3464\u001b[0m                     \u001b[0mbasket_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m                     \u001b[0mentry_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/interpretation/objects.py\u001b[0m in \u001b[0;36mfinal_array\u001b[0;34m(self, basket_arrays, entry_start, entry_stop, entry_offsets, library, branch)\u001b[0m\n\u001b[1;32m    223\u001b[0m         )\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         self.hook_after_final_array(\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/uproot/interpretation/library.py\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(self, array, branch, interpretation, entry_start, entry_stop)\u001b[0m\n\u001b[1;32m    576\u001b[0m                 ) from err\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             unlabeled = awkward.from_iter(\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0m_object_to_awkward_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105a_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/operations/convert.py\u001b[0m in \u001b[0;36mfrom_iter\u001b[0;34m(iterable, highlevel, behavior, allow_record, initial, resize)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromiter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/releases/Python/3.9.12-9a1bc/x86_64-el9-gcc11-opt/lib/python3.9/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# ASSUMPTION: Your model, CCV3, Aggloremative function, etc. are already defined.\n",
    "# For example:\n",
    "# model = ...  # your already-loaded or defined model\n",
    "#\n",
    "# testpath should point to your test data directory.\n",
    "testpath = '/vols/cms/mm1221/Data/mix/test/'\n",
    "\n",
    "# Set the number of runs for each CP value to compute error bars\n",
    "num_runs = 3  # you can adjust this number as needed\n",
    "cp_values = list(range(1, 11))  # CP's from 1 to 10 inclusive\n",
    "\n",
    "# Lists to store the performance metrics for each CP value.\n",
    "avg_inference_times = []  # mean inference time per event [s]\n",
    "std_inference_times = []  # standard deviation of inference time per event\n",
    "avg_peak_memory = []      # mean peak memory usage [MB]\n",
    "std_peak_memory = []      # standard deviation of peak memory usage\n",
    "\n",
    "# Loop through each CP setting:\n",
    "for cp in cp_values:\n",
    "    print(f\"Testing with n_particles = {cp}\")\n",
    "    inference_times = []\n",
    "    peak_memories = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"  Run {run+1}/{num_runs} ...\")\n",
    "        # Load test data with exactly cp calo particles per event.\n",
    "        data_test = CCV3(testpath, max_events=100, n_particles=cp)\n",
    "        test_loader = DataLoader(data_test, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "        # Start measuring memory with tracemalloc.\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "\n",
    "        all_predictions = []\n",
    "        # Loop over test events to run inference.\n",
    "        for i, data in enumerate(test_loader):\n",
    "            if i>\n",
    "            # You can add any additional processing if needed.\n",
    "            predictions = model(data.x, data.x_batch)\n",
    "            all_predictions.append(predictions[0].detach().cpu().numpy())\n",
    "        all_cluster_labels = Aggloremative(all_predictions, threshold=0.175)\n",
    "\n",
    "        end_time = time.time()\n",
    "        # Get current and peak memory usage (in bytes).\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        # Calculate average inference time per event.\n",
    "        total_time = end_time - start_time\n",
    "        avg_time = total_time / len(test_loader)\n",
    "        inference_times.append(avg_time)\n",
    "\n",
    "        # Convert peak memory (bytes) to megabytes.\n",
    "        peak_memory_mb = peak / (1024 * 1024)\n",
    "        peak_memories.append(peak_memory_mb)\n",
    "\n",
    "    # Compute the mean and standard deviation of the collected runs.\n",
    "    avg_inference_times.append(np.mean(inference_times))\n",
    "    std_inference_times.append(np.std(inference_times))\n",
    "    avg_peak_memory.append(np.mean(peak_memories))\n",
    "    std_peak_memory.append(np.std(peak_memories))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Now plot the results with dual y axes.\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "color_time = 'tab:blue'\n",
    "ax1.set_xlabel('Number of Calo Particles')\n",
    "ax1.set_ylabel('Average Inference Time (s)', color=color_time)\n",
    "ax1.errorbar(cp_values, avg_inference_times, yerr=std_inference_times, color=color_time,\n",
    "             marker='o', capsize=5, label='Inference Time')\n",
    "ax1.tick_params(axis='y', labelcolor=color_time)\n",
    "\n",
    "# Create a twin of the original axis that shares the x-axis.\n",
    "ax2 = ax1.twinx()\n",
    "color_mem = 'tab:red'\n",
    "ax2.set_ylabel('Peak Memory Usage (MB)', color=color_mem)\n",
    "ax2.errorbar(cp_values, avg_peak_memory, yerr=std_peak_memory, color=color_mem,\n",
    "             marker='s', capsize=5, label='Peak Memory')\n",
    "ax2.tick_params(axis='y', labelcolor=color_mem)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"Inference Time & Peak Memory vs. Number of Calo Particles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a101434",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_labels = Aggloremative(all_predictions, threshold=0.175)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
